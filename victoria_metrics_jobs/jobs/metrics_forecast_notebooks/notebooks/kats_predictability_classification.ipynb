{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TSFEL-based Time Series Predictability Classification Notebook\n",
        "\n",
        "This notebook allows you to:\n",
        "1. Query time series from Victoria Metrics using a PromQL selector\n",
        "2. Extract time series features using TSFEL library (Time Series Feature Extraction Library)\n",
        "3. Detect weekly and monthly seasonality patterns using autocorrelation\n",
        "4. Classify each series by predictability based on TSFEL features (no cross-validation)\n",
        "5. Visualize results including historical data grouped by categories\n",
        "\n",
        "**Classification Categories:**\n",
        "- **Predictable**: Series suitable for forecasting (includes those with clear seasonality patterns)\n",
        "- **Low Predictability**: Weak patterns, limited autocorrelation\n",
        "- **Not Suitable**: Cannot be forecasted (insufficient data, data quality issues, etc.)\n",
        "\n",
        "**Key Differences from Cross-Validation Approach:**\n",
        "- Uses TSFEL library for feature extraction (statistical, temporal, spectral domains)\n",
        "- Classification based on statistical features, not model performance\n",
        "- No Prophet/ARIMA cross-validation - faster analysis\n",
        "- Focus on weekly and monthly seasonality detection\n",
        "- Outlier and changepoint detection use simple statistical methods (IQR-based outliers, variance-based changepoints)\n",
        "\n",
        "**Use this notebook for:**\n",
        "- Understanding which time series are suitable for forecasting\n",
        "- Identifying seasonal patterns (weekly/monthly) in your metrics\n",
        "- Quick classification without running expensive cross-validation\n",
        "- Visual inspection of series characteristics by category\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add current directory to Python path\n",
        "current_dir = str(Path.cwd())\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "\n",
        "# Victoria Metrics connection - from environment variables\n",
        "VM_QUERY_URL = os.getenv('VM_QUERY_URL', 'http://victoria-metrics:8428')\n",
        "VM_TOKEN = os.getenv('VM_TOKEN', '')\n",
        "\n",
        "print(f\"VM Query URL: {VM_QUERY_URL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TSFEL imports for time series feature extraction (TSFEL 0.2.0)\n",
        "import tsfel\n",
        "\n",
        "# Helper modules\n",
        "from prometheus_api_client import PrometheusConnect\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Connect to Victoria Metrics and Query Data\n",
        "\n",
        "**Configure your selector and history days in the cell below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PromQL selector - EDIT THIS\n",
        "SELECTOR = '{job=\"extractor\"}'  # Your PromQL selector\n",
        "\n",
        "# History parameter - EDIT THIS\n",
        "HISTORY_DAYS = 365  # Days of history to fetch\n",
        "\n",
        "# TSFEL-specific parameters\n",
        "MIN_HISTORY_POINTS = 60  # Minimum for seasonality detection\n",
        "SEASONALITY_LAG_WEEKLY = 7\n",
        "SEASONALITY_LAG_MONTHLY = 30\n",
        "ACF_THRESHOLD = 0.3  # Threshold for seasonality detection\n",
        "MAX_SERIES_PER_PLOT = 50  # Maximum series per category plot\n",
        "\n",
        "# Connect to Victoria Metrics and query historical data\n",
        "headers = {\"Authorization\": f\"Bearer {VM_TOKEN}\"} if VM_TOKEN else {}\n",
        "prom = PrometheusConnect(url=VM_QUERY_URL, headers=headers, disable_ssl=True)\n",
        "print(f\"Connected to Victoria Metrics at {VM_QUERY_URL}\")\n",
        "\n",
        "print(f\"\\nQuerying: {SELECTOR}\")\n",
        "end_date = datetime.now(timezone.utc)\n",
        "start_date = end_date - timedelta(days=HISTORY_DAYS)\n",
        "query_result = prom.custom_query_range(\n",
        "    query=SELECTOR.replace(\"'\", '\"'),  # Ensure double quotes for PromQL\n",
        "    start_time=start_date,\n",
        "    end_time=end_date,\n",
        "    step=\"24h\"\n",
        ")\n",
        "\n",
        "print(f\"Query range: {start_date.date()} to {end_date.date()}\")\n",
        "print(f\"Query returned {len(query_result)} series\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parse and Prepare Time Series Data\n",
        "\n",
        "This parses ALL time series returned by the selector query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse all series from query result\n",
        "all_series = []\n",
        "for item in query_result:\n",
        "    metric = item.get('metric', {})\n",
        "    metric_name = metric.get('__name__')\n",
        "    if not metric_name:\n",
        "        continue\n",
        "    labels = {k: v for k, v in metric.items() if k != '__name__'}\n",
        "    values = item.get('values', [])\n",
        "    samples = [(datetime.fromtimestamp(float(ts), tz=timezone.utc), float(value)) for ts, value in values]\n",
        "    if samples:\n",
        "        all_series.append((samples, {'metric_name': metric_name, 'labels': labels}))\n",
        "\n",
        "if not all_series:\n",
        "    raise ValueError(\"No data found for selector\")\n",
        "\n",
        "print(f\"Found {len(all_series)} time series for selector: {SELECTOR}\")\n",
        "print(\"\\nSeries preview:\")\n",
        "for idx, (samples, series_info) in enumerate(all_series[:5]):\n",
        "    print(f\"  {idx+1}. {series_info['metric_name']} {series_info['labels']}\")\n",
        "if len(all_series) > 5:\n",
        "    print(f\"  ... and {len(all_series) - 5} more\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract TSFEL Features and Detect Seasonality\n",
        "\n",
        "This section uses TSFEL library to extract comprehensive time series features and detect weekly/monthly seasonality patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regularize_time_intervals(df, freq='D', method='ffill'):\n",
        "    \"\"\"Regularize time intervals in a time series DataFrame.\n",
        "    \n",
        "    Creates a regular date range and fills missing values using statistical methods.\n",
        "    This helps ensure consistent time intervals for feature extraction.\n",
        "    \n",
        "    Args:\n",
        "        df: pandas DataFrame with 'ds' (datetime) and 'y' (value) columns\n",
        "        freq: Frequency string for regular intervals (default 'D' for daily)\n",
        "        method: Fill method - 'ffill' (forward fill), 'bfill' (backward fill), \n",
        "                'interpolate' (linear interpolation), or 'mean' (fill with mean)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with regular time intervals\n",
        "    \"\"\"\n",
        "    if len(df) < 2:\n",
        "        return df\n",
        "    \n",
        "    # Ensure 'ds' is datetime\n",
        "    df = df.copy()\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    df = df.sort_values('ds').reset_index(drop=True)\n",
        "    \n",
        "    # Create regular date range from first to last date\n",
        "    start_date = df['ds'].min()\n",
        "    end_date = df['ds'].max()\n",
        "    regular_range = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
        "    \n",
        "    # Set 'ds' as index and reindex to regular range\n",
        "    df_indexed = df.set_index('ds')[['y']]\n",
        "    df_regular = df_indexed.reindex(regular_range)\n",
        "    \n",
        "    # Fill missing values based on method\n",
        "    if method == 'ffill':\n",
        "        # Forward fill (carry last known value forward)\n",
        "        df_regular['y'] = df_regular['y'].ffill()\n",
        "        # Backward fill any remaining NaNs at the start\n",
        "        df_regular['y'] = df_regular['y'].bfill()\n",
        "    elif method == 'bfill':\n",
        "        # Backward fill (carry next known value backward)\n",
        "        df_regular['y'] = df_regular['y'].bfill()\n",
        "        # Forward fill any remaining NaNs at the end\n",
        "        df_regular['y'] = df_regular['y'].ffill()\n",
        "    elif method == 'interpolate':\n",
        "        # Linear interpolation\n",
        "        df_regular['y'] = df_regular['y'].interpolate(method='linear')\n",
        "        # Fill any remaining NaNs at edges\n",
        "        df_regular['y'] = df_regular['y'].ffill().bfill()\n",
        "    elif method == 'mean':\n",
        "        # Fill with mean value\n",
        "        mean_val = df['y'].mean()\n",
        "        df_regular['y'] = df_regular['y'].fillna(mean_val)\n",
        "    else:\n",
        "        # Default: forward fill + backward fill\n",
        "        df_regular['y'] = df_regular['y'].ffill().bfill()\n",
        "    \n",
        "    # Reset index to get 'ds' column back\n",
        "    df_regular = df_regular.reset_index()\n",
        "    df_regular = df_regular.rename(columns={'index': 'ds'})\n",
        "    \n",
        "    # Remove any remaining NaN values (shouldn't happen, but safety check)\n",
        "    df_regular = df_regular.dropna()\n",
        "    \n",
        "    return df_regular\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_seasonality(ts_data, lag_weekly=7, lag_monthly=30, threshold=0.3):\n",
        "    \"\"\"Detect weekly and monthly seasonality using autocorrelation.\n",
        "    \n",
        "    Args:\n",
        "        ts_data: pandas DataFrame with 'time' and 'value' columns\n",
        "        lag_weekly: Lag for weekly seasonality (default 7)\n",
        "        lag_monthly: Lag for monthly seasonality (default 30)\n",
        "        threshold: Minimum autocorrelation threshold for seasonality detection\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with seasonality flags and autocorrelation values\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    if len(ts_data) < max(lag_weekly, lag_monthly) + 1:\n",
        "        features['has_weekly_seasonality'] = False\n",
        "        features['has_monthly_seasonality'] = False\n",
        "        features['weekly_autocorr'] = 0.0\n",
        "        features['monthly_autocorr'] = 0.0\n",
        "        return features\n",
        "    \n",
        "    values = ts_data['value'].values\n",
        "    \n",
        "    # Calculate autocorrelation at weekly lag\n",
        "    if len(values) > lag_weekly:\n",
        "        try:\n",
        "            weekly_autocorr = np.corrcoef(values[:-lag_weekly], values[lag_weekly:])[0, 1]\n",
        "            if np.isnan(weekly_autocorr):\n",
        "                weekly_autocorr = 0.0\n",
        "            features['weekly_autocorr'] = weekly_autocorr\n",
        "            features['has_weekly_seasonality'] = abs(weekly_autocorr) > threshold\n",
        "        except Exception:\n",
        "            features['weekly_autocorr'] = 0.0\n",
        "            features['has_weekly_seasonality'] = False\n",
        "    else:\n",
        "        features['weekly_autocorr'] = 0.0\n",
        "        features['has_weekly_seasonality'] = False\n",
        "    \n",
        "    # Calculate autocorrelation at monthly lag\n",
        "    if len(values) > lag_monthly:\n",
        "        try:\n",
        "            monthly_autocorr = np.corrcoef(values[:-lag_monthly], values[lag_monthly:])[0, 1]\n",
        "            if np.isnan(monthly_autocorr):\n",
        "                monthly_autocorr = 0.0\n",
        "            features['monthly_autocorr'] = monthly_autocorr\n",
        "            features['has_monthly_seasonality'] = abs(monthly_autocorr) > threshold\n",
        "        except Exception:\n",
        "            features['monthly_autocorr'] = 0.0\n",
        "            features['has_monthly_seasonality'] = False\n",
        "    else:\n",
        "        features['monthly_autocorr'] = 0.0\n",
        "        features['has_monthly_seasonality'] = False\n",
        "    \n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tsfel_features(df):\n",
        "    \"\"\"Extract time series features using TSFEL (Time Series Feature Extraction Library).\n",
        "    \n",
        "    Args:\n",
        "        df: pandas DataFrame with 'ds' (datetime) and 'y' (value) columns\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of TSFEL features\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    if len(df) < 2:\n",
        "        return features\n",
        "    \n",
        "        # Add basic statistics first (always available)\n",
        "    values = df['y'].values\n",
        "    features['mean'] = np.mean(values)\n",
        "    features['std'] = np.std(values)\n",
        "    features['min'] = np.min(values)\n",
        "    features['max'] = np.max(values)\n",
        "    features['range'] = features['max'] - features['min']\n",
        "    # Calculate CV safely (handle scalar values)\n",
        "    mean_val = features.get('mean', 0)\n",
        "    std_val = features.get('std', 0)\n",
        "    if isinstance(mean_val, (pd.Series, np.ndarray)):\n",
        "        mean_val = mean_val.iloc[0] if isinstance(mean_val, pd.Series) else mean_val[0]\n",
        "    if isinstance(std_val, (pd.Series, np.ndarray)):\n",
        "        std_val = std_val.iloc[0] if isinstance(std_val, pd.Series) else std_val[0]\n",
        "    features['cv'] = std_val / mean_val if mean_val != 0 else np.inf\n",
        "    features['data_points'] = len(df)  # Store data point count for classification\n",
        "    \n",
        "    try:\n",
        "        # Prepare time series data for TSFEL\n",
        "        # Regularize time intervals to daily frequency with forward fill\n",
        "        df_prepared = regularize_time_intervals(df, freq='D', method='ffill')\n",
        "        \n",
        "        if len(df_prepared) < 2:\n",
        "            return features\n",
        "        \n",
        "        # Clean data: remove infinite values, ensure proper types\n",
        "        signal = df_prepared['y'].values.copy()\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        \n",
        "        if len(signal) < 2 or np.var(signal) == 0:\n",
        "            return features\n",
        "        \n",
        "        # Get TSFEL feature configuration for statistical and temporal domains\n",
        "        cfg = tsfel.get_features_by_domain(['statistical', 'temporal'])\n",
        "        \n",
        "        # Extract features using TSFEL 0.2.0\n",
        "        # TSFEL expects a 1D array (signal) and sampling frequency\n",
        "        if len(signal) < 10:\n",
        "            raise ValueError(f\"Signal too short for TSFEL: {len(signal)} points\")\n",
        "        \n",
        "        feature_dict = tsfel.time_series_features_extractor(cfg, signal, fs=1.0)\n",
        "        \n",
        "        # Extract features using exact TSFEL 0.2.0 feature names (tested with Python 3.10.19)\n",
        "        # All features have \"0_\" prefix in TSFEL 0.2.0\n",
        "        \n",
        "        # Helper function to ensure scalar values (not Series)\n",
        "        def to_scalar(val):\n",
        "            \"\"\"Convert value to scalar if it's a Series or array.\"\"\"\n",
        "            if isinstance(val, pd.Series):\n",
        "                return val.iloc[0] if len(val) > 0 else np.nan\n",
        "            elif isinstance(val, np.ndarray):\n",
        "                return val[0] if len(val) > 0 else np.nan\n",
        "            elif isinstance(val, (list, tuple)):\n",
        "                return val[0] if len(val) > 0 else np.nan\n",
        "            return val\n",
        "        \n",
        "        # Statistical domain features (exact names from TSFEL 0.2.0)\n",
        "        features['mean'] = to_scalar(feature_dict.get('0_Mean', np.nan))\n",
        "        features['std'] = to_scalar(feature_dict.get('0_Standard deviation', np.nan))\n",
        "        features['var'] = to_scalar(feature_dict.get('0_Variance', np.nan))\n",
        "        features['entropy'] = to_scalar(feature_dict.get('0_Entropy', np.nan))\n",
        "        \n",
        "        # Calculate CV from mean and std (TSFEL 0.2.0 doesn't have CV feature in statistical/temporal domains)\n",
        "        mean_val = features.get('mean', np.nan)\n",
        "        std_val = features.get('std', np.nan)\n",
        "        # Ensure scalar values (not Series)\n",
        "        if isinstance(mean_val, (pd.Series, np.ndarray)):\n",
        "            mean_val = mean_val.iloc[0] if isinstance(mean_val, pd.Series) else mean_val[0] if len(mean_val) > 0 else np.nan\n",
        "        if isinstance(std_val, (pd.Series, np.ndarray)):\n",
        "            std_val = std_val.iloc[0] if isinstance(std_val, pd.Series) else std_val[0] if len(std_val) > 0 else np.nan\n",
        "        \n",
        "        if not np.isnan(mean_val) and mean_val != 0:\n",
        "            features['cv'] = abs(std_val) / abs(mean_val) if not np.isnan(std_val) else np.nan\n",
        "        else:\n",
        "            features['cv'] = np.nan\n",
        "        \n",
        "        # Temporal domain features\n",
        "        # TSFEL's '0_Autocorrelation' is NOT a correlation coefficient (not in [-1, 1] range)\n",
        "        # Calculate actual lag-1 autocorrelation coefficient ourselves\n",
        "        if len(signal) > 1:\n",
        "            try:\n",
        "                # Calculate lag-1 autocorrelation coefficient (should be in [-1, 1])\n",
        "                acf1_val = np.corrcoef(signal[:-1], signal[1:])[0, 1]\n",
        "                if np.isnan(acf1_val):\n",
        "                    acf1_val = 0.0\n",
        "                features['acf1'] = acf1_val\n",
        "            except Exception:\n",
        "                features['acf1'] = np.nan\n",
        "        else:\n",
        "            features['acf1'] = np.nan\n",
        "        \n",
        "        # acf5 and acf10 are not available in TSFEL 0.2.0 statistical/temporal domains\n",
        "        features['acf5'] = np.nan\n",
        "        features['acf10'] = np.nan\n",
        "        \n",
        "        # Additional TSFEL features useful for predictability classification\n",
        "        # Trend indicator: Slope of the signal\n",
        "        features['slope'] = to_scalar(feature_dict.get('0_Slope', np.nan))\n",
        "        # Use slope as trend_strength (absolute value to indicate strength)\n",
        "        slope_val = features['slope']\n",
        "        if isinstance(slope_val, (pd.Series, np.ndarray)):\n",
        "            slope_val = to_scalar(slope_val)\n",
        "        features['trend_strength'] = abs(slope_val) if not np.isnan(slope_val) else np.nan\n",
        "        \n",
        "        # Stability indicators: Mean absolute deviation (lower = more stable)\n",
        "        features['mean_absolute_deviation'] = to_scalar(feature_dict.get('0_Mean absolute deviation', np.nan))\n",
        "        # Calculate stability as inverse of normalized MAD\n",
        "        mad_val = features.get('mean_absolute_deviation', np.nan)\n",
        "        mean_val = features.get('mean', np.nan)\n",
        "        std_val = features.get('std', np.nan)\n",
        "        \n",
        "        # Ensure scalar values (not Series)\n",
        "        if isinstance(mad_val, (pd.Series, np.ndarray)):\n",
        "            mad_val = mad_val.iloc[0] if isinstance(mad_val, pd.Series) else mad_val[0] if len(mad_val) > 0 else np.nan\n",
        "        if isinstance(mean_val, (pd.Series, np.ndarray)):\n",
        "            mean_val = mean_val.iloc[0] if isinstance(mean_val, pd.Series) else mean_val[0] if len(mean_val) > 0 else np.nan\n",
        "        if isinstance(std_val, (pd.Series, np.ndarray)):\n",
        "            std_val = std_val.iloc[0] if isinstance(std_val, pd.Series) else std_val[0] if len(std_val) > 0 else np.nan\n",
        "        \n",
        "        if not np.isnan(mad_val) and not np.isnan(mean_val):\n",
        "            if abs(mean_val) > 0:\n",
        "                normalized_mad = mad_val / abs(mean_val)\n",
        "                features['stability'] = 1.0 / (1.0 + normalized_mad) if normalized_mad > 0 else 1.0\n",
        "            else:\n",
        "                # If mean is near zero, use MAD relative to std\n",
        "                if not np.isnan(std_val) and std_val > 0:\n",
        "                    normalized_mad = mad_val / std_val\n",
        "                    features['stability'] = 1.0 / (1.0 + normalized_mad) if normalized_mad > 0 else 1.0\n",
        "                else:\n",
        "                    features['stability'] = np.nan\n",
        "        else:\n",
        "            features['stability'] = np.nan\n",
        "        \n",
        "        # Distribution shape indicators (useful for predictability)\n",
        "        features['skewness'] = to_scalar(feature_dict.get('0_Skewness', np.nan))\n",
        "        features['kurtosis'] = to_scalar(feature_dict.get('0_Kurtosis', np.nan))\n",
        "        \n",
        "        # Variability indicators\n",
        "        features['interquartile_range'] = to_scalar(feature_dict.get('0_Interquartile range', np.nan))\n",
        "        features['zero_crossing_rate'] = to_scalar(feature_dict.get('0_Zero crossing rate', np.nan))\n",
        "        \n",
        "        # Structure indicators\n",
        "        features['positive_turning_points'] = to_scalar(feature_dict.get('0_Positive turning points', np.nan))\n",
        "        features['negative_turning_points'] = to_scalar(feature_dict.get('0_Negative turning points', np.nan))\n",
        "        features['neighbourhood_peaks'] = to_scalar(feature_dict.get('0_Neighbourhood peaks', np.nan))\n",
        "        \n",
        "        # Features not available in TSFEL 0.2.0 - removed (not setting to NaN)\n",
        "        # linearity, lumpiness, arch_lm are not in TSFEL statistical/temporal domains\n",
        "        \n",
        "        # Store all TSFEL features for reference\n",
        "        features['all_tsfel_features'] = feature_dict\n",
        "        \n",
        "    except Exception as tsfel_error:\n",
        "        # TSFEL feature extraction failed - set all TSFEL-specific features to NaN\n",
        "        features['acf1'] = np.nan\n",
        "        features['acf5'] = np.nan\n",
        "        features['acf10'] = np.nan\n",
        "        features['trend_strength'] = np.nan\n",
        "        features['stability'] = np.nan\n",
        "        features['entropy'] = np.nan\n",
        "        features['slope'] = np.nan\n",
        "        features['skewness'] = np.nan\n",
        "        features['kurtosis'] = np.nan\n",
        "        features['interquartile_range'] = np.nan\n",
        "        features['zero_crossing_rate'] = np.nan\n",
        "        features['positive_turning_points'] = np.nan\n",
        "        features['negative_turning_points'] = np.nan\n",
        "        features['neighbourhood_peaks'] = np.nan\n",
        "        features['mean_absolute_deviation'] = np.nan\n",
        "        features['var'] = features.get('std', np.nan) ** 2 if not np.isnan(features.get('std', np.nan)) else np.nan\n",
        "    \n",
        "    # Add seasonality detection (custom implementation, not library-specific)\n",
        "    try:\n",
        "        ts_df = df.copy()\n",
        "        ts_df = ts_df.rename(columns={'ds': 'time', 'y': 'value'})\n",
        "        ts_df = ts_df[['time', 'value']].dropna()\n",
        "        \n",
        "        if len(ts_df) >= max(SEASONALITY_LAG_WEEKLY, SEASONALITY_LAG_MONTHLY) + 1:\n",
        "            seasonality_features = detect_seasonality(\n",
        "                ts_df, \n",
        "                lag_weekly=SEASONALITY_LAG_WEEKLY,\n",
        "                lag_monthly=SEASONALITY_LAG_MONTHLY,\n",
        "                threshold=ACF_THRESHOLD\n",
        "            )\n",
        "            features.update(seasonality_features)\n",
        "        else:\n",
        "            features['has_weekly_seasonality'] = False\n",
        "            features['has_monthly_seasonality'] = False\n",
        "            features['weekly_autocorr'] = 0.0\n",
        "            features['monthly_autocorr'] = 0.0\n",
        "    except Exception as e:\n",
        "        features['has_weekly_seasonality'] = False\n",
        "        features['has_monthly_seasonality'] = False\n",
        "        features['weekly_autocorr'] = 0.0\n",
        "        features['monthly_autocorr'] = 0.0\n",
        "    \n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_changepoints_simple(df):\n",
        "    \"\"\"Detect outliers and changepoints using simple statistical methods.\n",
        "    \n",
        "    Args:\n",
        "        df: pandas DataFrame with 'ds' (datetime) and 'y' (value) columns\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with outlier and changepoint information\n",
        "    \"\"\"\n",
        "    detector_info = {}\n",
        "    \n",
        "    try:\n",
        "        # Prepare data\n",
        "        values = df['y'].values.copy()\n",
        "        values = values[~np.isnan(values)]\n",
        "        values = values[~np.isinf(values)]\n",
        "        \n",
        "        if len(values) < 10:  # Need minimum data for detectors\n",
        "            detector_info['outlier_count'] = 0\n",
        "            detector_info['outlier_ratio'] = 0.0\n",
        "            detector_info['changepoint_count'] = 0\n",
        "            return detector_info\n",
        "        \n",
        "        # Outlier detection using IQR method\n",
        "        try:\n",
        "            Q1 = np.percentile(values, 25)\n",
        "            Q3 = np.percentile(values, 75)\n",
        "            IQR = Q3 - Q1\n",
        "            \n",
        "            # Define outlier bounds\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            \n",
        "            # Count outliers\n",
        "            outliers = (values < lower_bound) | (values > upper_bound)\n",
        "            detector_info['outlier_count'] = np.sum(outliers)\n",
        "            detector_info['outlier_ratio'] = detector_info['outlier_count'] / len(values)\n",
        "        except Exception as e:\n",
        "            detector_info['outlier_count'] = 0\n",
        "            detector_info['outlier_ratio'] = 0.0\n",
        "        \n",
        "        # Changepoint detection using variance-based method\n",
        "        try:\n",
        "            # Calculate rolling variance with window size\n",
        "            window_size = min(20, len(values) // 4)  # Adaptive window size\n",
        "            if window_size < 5:\n",
        "                detector_info['changepoint_count'] = 0\n",
        "            else:\n",
        "                # Calculate rolling variance\n",
        "                rolling_var = pd.Series(values).rolling(window=window_size, center=True).var()\n",
        "                \n",
        "                # Calculate threshold: mean + 2*std of rolling variance\n",
        "                var_mean = rolling_var.mean()\n",
        "                var_std = rolling_var.std()\n",
        "                threshold = var_mean + 2 * var_std if not np.isnan(var_std) and var_std > 0 else var_mean * 2\n",
        "                \n",
        "                # Detect significant changes in variance\n",
        "                changepoints = (rolling_var > threshold) & (~np.isnan(rolling_var))\n",
        "                \n",
        "                # Count distinct changepoint regions (consecutive True values count as one)\n",
        "                changepoint_count = 0\n",
        "                in_changepoint = False\n",
        "                for is_cp in changepoints:\n",
        "                    if is_cp and not in_changepoint:\n",
        "                        changepoint_count += 1\n",
        "                        in_changepoint = True\n",
        "                    elif not is_cp:\n",
        "                        in_changepoint = False\n",
        "                \n",
        "                detector_info['changepoint_count'] = changepoint_count\n",
        "        except Exception as e:\n",
        "            detector_info['changepoint_count'] = 0\n",
        "        \n",
        "    except Exception as e:\n",
        "        detector_info['outlier_count'] = 0\n",
        "        detector_info['outlier_ratio'] = 0.0\n",
        "        detector_info['changepoint_count'] = 0\n",
        "    \n",
        "    return detector_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each series and extract TSFEL features\n",
        "evaluation_results = []\n",
        "\n",
        "print(f\"Extracting TSFEL features for {len(all_series)} time series...\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "for series_idx, (samples, series_info) in enumerate(all_series):\n",
        "    print(f\"\\nProcessing {series_idx + 1}/{len(all_series)}: {series_info['metric_name']}\")\n",
        "    \n",
        "    try:\n",
        "        # Prepare data\n",
        "        df = pd.DataFrame(samples, columns=['ds', 'y'])\n",
        "        df['ds'] = pd.to_datetime(df['ds'], utc=True).dt.tz_localize(None)\n",
        "        df = df.sort_values('ds').reset_index(drop=True)\n",
        "        \n",
        "        # Check minimum data points\n",
        "        if len(df) < MIN_HISTORY_POINTS:\n",
        "            print(f\"  ⚠️  Skipping: insufficient data ({len(df)} < {MIN_HISTORY_POINTS})\")\n",
        "            result = {\n",
        "                'series_info': series_info,\n",
        "                'df': df,\n",
        "                'features': {},\n",
        "                'detector_info': {},\n",
        "                'classification': 'Not Suitable',\n",
        "                'reason': 'Insufficient data'\n",
        "            }\n",
        "            evaluation_results.append(result)\n",
        "            continue\n",
        "        \n",
        "        # Check for invalid values\n",
        "        if df['y'].isna().all() or np.isinf(df['y']).any():\n",
        "            print(f\"  ⚠️  Skipping: invalid values\")\n",
        "            result = {\n",
        "                'series_info': series_info,\n",
        "                'df': df,\n",
        "                'features': {},\n",
        "                'detector_info': {},\n",
        "                'classification': 'Not Suitable',\n",
        "                'reason': 'Invalid values'\n",
        "            }\n",
        "            evaluation_results.append(result)\n",
        "            continue\n",
        "        \n",
        "        # Extract TSFEL features\n",
        "        print(f\"  Extracting TSFEL features...\")\n",
        "        try:\n",
        "            features = extract_tsfel_features(df)\n",
        "            # Check if we got at least basic features (mean, std, etc.)\n",
        "            if not features or 'mean' not in features:\n",
        "                print(f\"  ⚠️  Warning: TSFEL feature extraction returned no features, using fallback\")\n",
        "                # Create minimal features as fallback\n",
        "                values = df['y'].values\n",
        "                features = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'cv': np.std(values) / np.mean(values) if np.mean(values) != 0 else np.inf,\n",
        "                    'acf1': np.nan,\n",
        "                    'stability': np.nan,\n",
        "                    'trend_strength': np.nan,\n",
        "                }\n",
        "        except Exception as feat_error:\n",
        "            print(f\"  ⚠️  Warning: TSFEL feature extraction failed: {feat_error}\")\n",
        "            # Use fallback features\n",
        "            values = df['y'].values\n",
        "            features = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'cv': np.std(values) / np.mean(values) if np.mean(values) != 0 else np.inf,\n",
        "                'acf1': np.nan,\n",
        "                'stability': np.nan,\n",
        "                'trend_strength': np.nan,\n",
        "            }\n",
        "        \n",
        "        # Detect outliers and changepoints\n",
        "        print(f\"  Detecting outliers and changepoints...\")\n",
        "        try:\n",
        "            detector_info = detect_outliers_changepoints_simple(df)\n",
        "        except Exception as det_error:\n",
        "            print(f\"  ⚠️  Warning: Detector failed: {det_error}\")\n",
        "            detector_info = {'outlier_count': 0, 'outlier_ratio': 0.0, 'changepoint_count': 0}\n",
        "        \n",
        "        result = {\n",
        "            'series_info': series_info,\n",
        "            'df': df,\n",
        "            'features': features,\n",
        "            'detector_info': detector_info,\n",
        "        }\n",
        "        \n",
        "        evaluation_results.append(result)\n",
        "        print(f\"  ✓ Completed\")\n",
        "        \n",
        "    except Exception as exc:\n",
        "        print(f\"  ✗ Failed: {exc}\")\n",
        "        result = {\n",
        "            'series_info': series_info,\n",
        "            'df': pd.DataFrame(),\n",
        "            'features': {},\n",
        "            'detector_info': {},\n",
        "            'classification': 'Not Suitable',\n",
        "            'reason': f'Error: {str(exc)}'\n",
        "        }\n",
        "        evaluation_results.append(result)\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Feature extraction complete: {len(evaluation_results)} series processed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changepoint Detection Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder - changepoint analysis moved to after classification section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Classify Series by Predictability\n",
        "\n",
        "Classify each series into one of three categories based on TSFEL features, seasonality detection, and data quality.\n",
        "\n",
        "**Classification Criteria:**\n",
        "- **Predictable**: Strong seasonality (weekly/monthly), good autocorrelation, stable variance, low outlier ratio\n",
        "- **Low Predictability**: Weak or no seasonality, moderate autocorrelation, some data quality issues\n",
        "- **Not Suitable**: Insufficient data, high outlier ratio, poor data quality, or extraction errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_with_tsfel(features, detector_info):\n",
        "    \"\"\"Classify time series predictability based on TSFEL features and detectors.\n",
        "    \n",
        "    Args:\n",
        "        features: Dictionary of TSFEL features including seasonality flags\n",
        "        detector_info: Dictionary with outlier and changepoint information\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (category, reason)\n",
        "    \"\"\"\n",
        "    # Handle None or empty features\n",
        "    if not features:\n",
        "        return 'Not Suitable', 'No features extracted'\n",
        "    \n",
        "    if not detector_info:\n",
        "        detector_info = {'outlier_ratio': 0.0, 'changepoint_count': 0}\n",
        "    \n",
        "    # Check for data quality issues\n",
        "    outlier_ratio = detector_info.get('outlier_ratio', 0.0)\n",
        "    changepoint_count = detector_info.get('changepoint_count', 0)\n",
        "    \n",
        "    # High outlier ratio indicates poor data quality\n",
        "    if outlier_ratio > 0.2:  # More than 20% outliers\n",
        "        return 'Not Suitable', f'High outlier ratio: {outlier_ratio:.2%}'\n",
        "    \n",
        "    # Too many changepoints indicates instability\n",
        "    if changepoint_count > 5:\n",
        "        return 'Not Suitable', f'Too many changepoints: {changepoint_count}'\n",
        "    \n",
        "    # Extract seasonality information\n",
        "    has_weekly = features.get('has_weekly_seasonality', False)\n",
        "    has_monthly = features.get('has_monthly_seasonality', False)\n",
        "    has_seasonality = has_weekly or has_monthly\n",
        "    \n",
        "    # Extract key TSFEL features\n",
        "    acf1 = features.get('acf1', np.nan)\n",
        "    trend_strength = features.get('trend_strength', np.nan)\n",
        "    stability = features.get('stability', np.nan)\n",
        "    cv = features.get('cv', np.nan)\n",
        "    skewness = features.get('skewness', np.nan)\n",
        "    kurtosis = features.get('kurtosis', np.nan)\n",
        "    zero_crossing_rate = features.get('zero_crossing_rate', np.nan)\n",
        "    interquartile_range = features.get('interquartile_range', np.nan)\n",
        "    mean_val = features.get('mean', np.nan)\n",
        "    std_val = features.get('std', np.nan)\n",
        "    \n",
        "    # Assess predictability indicators using new TSFEL features\n",
        "    # High zero crossing rate indicates noise/oscillations (less predictable)\n",
        "    high_noise = False\n",
        "    if not np.isnan(zero_crossing_rate):\n",
        "        # TSFEL's zero crossing rate is a count, normalize by data length if available\n",
        "        data_points = features.get('data_points', np.nan)\n",
        "        if not np.isnan(data_points) and data_points > 0:\n",
        "            zcr_normalized = zero_crossing_rate / data_points\n",
        "            # Normalized ZCR > 0.3 indicates high oscillation (30% of points cross zero)\n",
        "            high_noise = zcr_normalized > 0.3\n",
        "        else:\n",
        "            # Fallback: use absolute threshold (for daily data, ZCR > 30 indicates high oscillation)\n",
        "            high_noise = zero_crossing_rate > 30\n",
        "    \n",
        "    # Extreme skewness indicates non-normal distribution (harder to predict)\n",
        "    extreme_skew = False\n",
        "    if not np.isnan(skewness):\n",
        "        extreme_skew = abs(skewness) > 2.0\n",
        "    \n",
        "    # High kurtosis indicates heavy tails/outliers (less predictable)\n",
        "    heavy_tails = False\n",
        "    if not np.isnan(kurtosis):\n",
        "        heavy_tails = kurtosis > 5.0  # Normal distribution has kurtosis ~3\n",
        "    \n",
        "    # High variability (IQR relative to mean) indicates instability\n",
        "    high_variability = False\n",
        "    if not np.isnan(interquartile_range) and not np.isnan(mean_val) and abs(mean_val) > 0:\n",
        "        iqr_coefficient = interquartile_range / abs(mean_val)\n",
        "        high_variability = iqr_coefficient > 1.0  # IQR > mean indicates high variability\n",
        "    \n",
        "    # Count negative predictability indicators\n",
        "    negative_indicators = sum([high_noise, extreme_skew, heavy_tails, high_variability])\n",
        "    \n",
        "    # Count seasonality patterns\n",
        "    seasonality_count = sum([has_weekly, has_monthly])\n",
        "    \n",
        "    # Classification logic\n",
        "    # Predictable: Strong seasonality OR good autocorrelation with stable variance\n",
        "    # AND not too many negative indicators\n",
        "    if has_seasonality and seasonality_count >= 1:\n",
        "        # Series with detected seasonality are more predictable\n",
        "        # But check for negative indicators that might reduce predictability\n",
        "        if negative_indicators <= 1:  # Allow at most one negative indicator\n",
        "            if not np.isnan(acf1) and acf1 > 0.3:\n",
        "                category = 'Predictable'\n",
        "                seasonality_types = []\n",
        "                if has_weekly:\n",
        "                    seasonality_types.append('weekly')\n",
        "                if has_monthly:\n",
        "                    seasonality_types.append('monthly')\n",
        "                reason = f'Seasonality detected: {\", \".join(seasonality_types)}'\n",
        "                if not np.isnan(acf1):\n",
        "                    reason += f', ACF1={acf1:.2f}'\n",
        "                if negative_indicators > 0:\n",
        "                    reason += f' (some noise indicators)'\n",
        "                return category, reason\n",
        "    \n",
        "    # Check autocorrelation and stability for predictability\n",
        "    if not np.isnan(acf1) and acf1 > 0.5:\n",
        "        # Strong autocorrelation indicates predictability\n",
        "        if not np.isnan(stability) and stability > 0.7:\n",
        "            # Check negative indicators\n",
        "            if negative_indicators <= 1:\n",
        "                category = 'Predictable'\n",
        "                reason = f'Strong autocorrelation (ACF1={acf1:.2f}) and stability ({stability:.2f})'\n",
        "                if has_seasonality:\n",
        "                    seasonality_types = []\n",
        "                    if has_weekly:\n",
        "                        seasonality_types.append('weekly')\n",
        "                    if has_monthly:\n",
        "                        seasonality_types.append('monthly')\n",
        "                    reason += f', seasonality: {\", \".join(seasonality_types)}'\n",
        "                if negative_indicators > 0:\n",
        "                    reason += f' (some noise indicators)'\n",
        "                return category, reason\n",
        "            else:\n",
        "                # Strong autocorrelation but too many negative indicators\n",
        "                category = 'Low Predictability'\n",
        "                reason = f'Strong autocorrelation (ACF1={acf1:.2f}) but high noise/variability'\n",
        "                return category, reason\n",
        "    \n",
        "    # Moderate autocorrelation with some stability\n",
        "    if not np.isnan(acf1) and acf1 > 0.2:\n",
        "        if not np.isnan(stability) and stability > 0.5:\n",
        "            category = 'Low Predictability'\n",
        "            reason = f'Moderate autocorrelation (ACF1={acf1:.2f}) and stability ({stability:.2f})'\n",
        "            if has_seasonality:\n",
        "                seasonality_types = []\n",
        "                if has_weekly:\n",
        "                    seasonality_types.append('weekly')\n",
        "                if has_monthly:\n",
        "                    seasonality_types.append('monthly')\n",
        "                reason += f', weak seasonality: {\", \".join(seasonality_types)}'\n",
        "            if negative_indicators > 0:\n",
        "                reason += f', noise indicators: {negative_indicators}'\n",
        "            return category, reason\n",
        "    \n",
        "    # Low autocorrelation or poor stability\n",
        "    if not np.isnan(acf1) and acf1 < 0.2:\n",
        "        category = 'Low Predictability'\n",
        "        reason = f'Low autocorrelation (ACF1={acf1:.2f})'\n",
        "        if not np.isnan(stability):\n",
        "            reason += f', stability={stability:.2f}'\n",
        "        if negative_indicators > 0:\n",
        "            reason += f', noise indicators: {negative_indicators}'\n",
        "        return category, reason\n",
        "    \n",
        "    # Fallback: use basic statistics\n",
        "    if has_seasonality:\n",
        "        category = 'Low Predictability'\n",
        "        seasonality_types = []\n",
        "        if has_weekly:\n",
        "            seasonality_types.append('weekly')\n",
        "        if has_monthly:\n",
        "            seasonality_types.append('monthly')\n",
        "        reason = f'Seasonality detected but weak patterns: {\", \".join(seasonality_types)}'\n",
        "        return category, reason\n",
        "    \n",
        "    # Default to low predictability if we have some features\n",
        "    if not np.isnan(acf1) or not np.isnan(trend_strength):\n",
        "        return 'Low Predictability', 'Weak patterns detected'\n",
        "    \n",
        "    # Final fallback\n",
        "    return 'Not Suitable', 'Insufficient features for classification'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze changepoint detection results\n",
        "print(\"=\"*60)\n",
        "print(\"CHANGEPOINT DETECTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count series with changepoints\n",
        "total_series = len(evaluation_results)\n",
        "series_with_changepoints = 0\n",
        "\n",
        "for result in evaluation_results:\n",
        "    detector_info = result.get('detector_info', {})\n",
        "    changepoint_count = detector_info.get('changepoint_count', 0)\n",
        "    \n",
        "    if changepoint_count > 0:\n",
        "        series_with_changepoints += 1\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Total series: {total_series}\")\n",
        "print(f\"  Series with changepoints: {series_with_changepoints} ({series_with_changepoints/total_series*100:.1f}%)\")\n",
        "\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classify each series\n",
        "for result in evaluation_results:\n",
        "    # Skip if already classified (e.g., due to errors)\n",
        "    if 'classification' in result and result.get('classification') == 'Not Suitable':\n",
        "        continue\n",
        "    \n",
        "    features = result.get('features', {})\n",
        "    detector_info = result.get('detector_info', {})\n",
        "    \n",
        "    classification, reason = classify_with_tsfel(features, detector_info)\n",
        "    result['classification'] = classification\n",
        "    result['reason'] = reason\n",
        "\n",
        "print(\"Classification complete!\")\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Total series: {len(evaluation_results)}\")\n",
        "\n",
        "# Count classifications\n",
        "classification_counts = {}\n",
        "for result in evaluation_results:\n",
        "    cls = result.get('classification', 'Unknown')\n",
        "    classification_counts[cls] = classification_counts.get(cls, 0) + 1\n",
        "\n",
        "print(f\"\\nClassifications:\")\n",
        "for cls, count in sorted(classification_counts.items()):\n",
        "    print(f\"  {cls}: {count} ({count/len(evaluation_results)*100:.1f}%)\")\n",
        "\n",
        "# Seasonality detection summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SEASONALITY DETECTION SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "series_with_seasonality = 0\n",
        "weekly_count = 0\n",
        "monthly_count = 0\n",
        "seasonality_by_category = {\n",
        "    'Predictable': {'with_seasonality': 0, 'total': 0},\n",
        "    'Low Predictability': {'with_seasonality': 0, 'total': 0},\n",
        "    'Not Suitable': {'with_seasonality': 0, 'total': 0}\n",
        "}\n",
        "\n",
        "for result in evaluation_results:\n",
        "    features = result.get('features', {})\n",
        "    has_weekly = features.get('has_weekly_seasonality', False)\n",
        "    has_monthly = features.get('has_monthly_seasonality', False)\n",
        "    has_seasonality = has_weekly or has_monthly\n",
        "    \n",
        "    if has_seasonality:\n",
        "        series_with_seasonality += 1\n",
        "    if has_weekly:\n",
        "        weekly_count += 1\n",
        "    if has_monthly:\n",
        "        monthly_count += 1\n",
        "    \n",
        "    classification = result.get('classification', 'Unknown')\n",
        "    if classification in seasonality_by_category:\n",
        "        seasonality_by_category[classification]['total'] += 1\n",
        "        if has_seasonality:\n",
        "            seasonality_by_category[classification]['with_seasonality'] += 1\n",
        "\n",
        "print(f\"\\nTotal series with detected seasonality: {series_with_seasonality}/{len(evaluation_results)}\")\n",
        "print(f\"  Weekly seasonality: {weekly_count} series\")\n",
        "print(f\"  Monthly seasonality: {monthly_count} series\")\n",
        "\n",
        "print(f\"\\nSeasonality by category:\")\n",
        "for category, stats in seasonality_by_category.items():\n",
        "    if stats['total'] > 0:\n",
        "        pct = (stats['with_seasonality'] / stats['total']) * 100\n",
        "        print(f\"  {category}: {stats['with_seasonality']}/{stats['total']} ({pct:.1f}%) have seasonality\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Results and Summary\n",
        "\n",
        "Create summary tables and visualizations of the classification results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame with TSFEL features\n",
        "summary_data = []\n",
        "for result in evaluation_results:\n",
        "    series_info = result['series_info']\n",
        "    features = result.get('features', {})\n",
        "    detector_info = result.get('detector_info', {})\n",
        "    \n",
        "    # Extract seasonality information\n",
        "    has_weekly = features.get('has_weekly_seasonality', False)\n",
        "    has_monthly = features.get('has_monthly_seasonality', False)\n",
        "    \n",
        "    # Create seasonality summary string\n",
        "    seasonality_list = []\n",
        "    if has_weekly:\n",
        "        seasonality_list.append('weekly')\n",
        "    if has_monthly:\n",
        "        seasonality_list.append('monthly')\n",
        "    seasonality_detected = ', '.join(seasonality_list) if seasonality_list else 'None'\n",
        "    \n",
        "    summary_data.append({\n",
        "        'metric_name': series_info['metric_name'],\n",
        "        'labels': str(series_info.get('labels', {})),\n",
        "        'data_points': len(result.get('df', pd.DataFrame())),\n",
        "        'mean': features.get('mean', np.nan),\n",
        "        'std': features.get('std', np.nan),\n",
        "        'cv': features.get('cv', np.nan),\n",
        "        'acf1': features.get('acf1', np.nan),\n",
        "        'trend_strength': features.get('trend_strength', np.nan),\n",
        "        'stability': features.get('stability', np.nan),\n",
        "        'entropy': features.get('entropy', np.nan),\n",
        "        'skewness': features.get('skewness', np.nan),\n",
        "        'kurtosis': features.get('kurtosis', np.nan),\n",
        "        'zero_crossing_rate': features.get('zero_crossing_rate', np.nan),\n",
        "        'interquartile_range': features.get('interquartile_range', np.nan),\n",
        "        'has_weekly_seasonality': has_weekly,\n",
        "        'has_monthly_seasonality': has_monthly,\n",
        "        'seasonality_detected': seasonality_detected,\n",
        "        'weekly_autocorr': features.get('weekly_autocorr', np.nan),\n",
        "        'monthly_autocorr': features.get('monthly_autocorr', np.nan),\n",
        "        'outlier_ratio': detector_info.get('outlier_ratio', np.nan),\n",
        "        'changepoint_count': detector_info.get('changepoint_count', np.nan),\n",
        "        'classification': result.get('classification', 'Unknown'),\n",
        "        'reason': result.get('reason', ''),\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Display summary table\n",
        "print(\"Classification Summary Table:\")\n",
        "print(f\"Total series: {len(summary_df)}\")\n",
        "print(\"\\nUse the interactive dataset viewer below to explore, sort, and filter the data.\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "# Display the dataframe\n",
        "from IPython.display import display\n",
        "display(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distribution of classifications as pie chart\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "# Color scheme for categories\n",
        "colors = {\n",
        "    'Predictable': '#2ecc71',      # Green\n",
        "    'Low Predictability': '#f39c12',      # Orange\n",
        "    'Not Suitable': '#e74c3c'            # Red\n",
        "}\n",
        "\n",
        "# Classifications\n",
        "classification_counts = summary_df['classification'].value_counts()\n",
        "plot_colors = [colors.get(cat, '#95a5a6') for cat in classification_counts.index]\n",
        "ax.pie(classification_counts.values, labels=classification_counts.index, autopct='%1.1f%%',\n",
        "       colors=plot_colors, startangle=90, textprops={'fontsize': 12})\n",
        "ax.set_title('TSFEL-based Predictability Classifications', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: ACF1 vs Stability (colored by classification)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "# Filter valid data\n",
        "valid_data = summary_df[\n",
        "    (summary_df['acf1'].notna()) & \n",
        "    (summary_df['stability'].notna())\n",
        "]\n",
        "\n",
        "if len(valid_data) > 0:\n",
        "    # Color by classification\n",
        "    color_map = {\n",
        "        'Predictable': '#2ecc71',\n",
        "        'Low Predictability': '#f39c12',\n",
        "        'Not Suitable': '#e74c3c'\n",
        "    }\n",
        "    \n",
        "    for category in valid_data['classification'].unique():\n",
        "        category_data = valid_data[valid_data['classification'] == category]\n",
        "        ax.scatter(\n",
        "            category_data['stability'],\n",
        "            category_data['acf1'],\n",
        "            label=category,\n",
        "            color=color_map.get(category, '#95a5a6'),\n",
        "            s=100,\n",
        "            alpha=0.6\n",
        "        )\n",
        "    \n",
        "    ax.set_xlabel('Stability', fontsize=12)\n",
        "    ax.set_ylabel('ACF1 (Autocorrelation at lag 1)', fontsize=12)\n",
        "    ax.set_title('ACF1 vs Stability (colored by Classification)', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    \n",
        "    # Add quadrant labels\n",
        "    ax.text(0.95, 0.95, 'High ACF1 + High Stability\\n= More Predictable', \n",
        "            transform=ax.transAxes, ha='right', va='top', \n",
        "            fontsize=10, style='italic', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Plot Historical Data Grouped by Categories\n",
        "\n",
        "Visualize historical time series data grouped by their predictability classifications. This allows you to visually inspect the characteristics of series within each category.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_series_by_category(evaluation_results, max_series_per_plot=20):\n",
        "    \"\"\"Plot time series grouped by classification category.\n",
        "    \n",
        "    Args:\n",
        "        evaluation_results: List of evaluation results\n",
        "        max_series_per_plot: Maximum number of series to show per plot\n",
        "    \"\"\"\n",
        "    \n",
        "    # Group series by classification\n",
        "    categories = {\n",
        "        'Predictable': [],\n",
        "        'Low Predictability': [],\n",
        "        'Not Suitable': []\n",
        "    }\n",
        "    \n",
        "    for result in evaluation_results:\n",
        "        classification = result.get('classification', 'Not Suitable')\n",
        "        if classification in categories:\n",
        "            categories[classification].append(result)\n",
        "    \n",
        "    # Plot each category\n",
        "    for category_name, series_list in categories.items():\n",
        "        if len(series_list) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Limit number of series per plot\n",
        "        series_to_plot = series_list[:max_series_per_plot]\n",
        "        n_series = len(series_to_plot)\n",
        "        \n",
        "        if n_series == 0:\n",
        "            continue\n",
        "        \n",
        "        # Calculate grid dimensions\n",
        "        n_cols = min(4, n_series)\n",
        "        n_rows = (n_series + n_cols - 1) // n_cols\n",
        "        \n",
        "        # Increase figure height to accommodate statistics text below plots\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6 * n_rows))\n",
        "        \n",
        "        # Convert axes to a flat list of Axes objects\n",
        "        if n_rows == 1 and n_cols == 1:\n",
        "            axes = [axes]\n",
        "        elif isinstance(axes, np.ndarray):\n",
        "            axes = axes.flatten().tolist()\n",
        "        elif not isinstance(axes, list):\n",
        "            axes = [axes]\n",
        "        \n",
        "        # Set suptitle\n",
        "        fig.suptitle(f'{category_name} ({len(series_list)} total, showing {n_series})', \n",
        "                     fontsize=16, fontweight='bold', y=0.995)\n",
        "        \n",
        "        for idx, result in enumerate(series_to_plot):\n",
        "            ax = axes[idx]\n",
        "            df = result.get('df', pd.DataFrame())\n",
        "            \n",
        "            if len(df) == 0:\n",
        "                ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title('No data', fontsize=10)\n",
        "                continue\n",
        "            \n",
        "            # Plot time series\n",
        "            ax.plot(df['ds'], df['y'], 'b-', linewidth=1.5, alpha=0.7)\n",
        "            \n",
        "            # Title: Only metric name\n",
        "            series_info = result['series_info']\n",
        "            title = f\"{series_info['metric_name']}\"\n",
        "            ax.set_title(title, fontsize=10, fontweight='bold')\n",
        "            ax.set_xlabel('Date', fontsize=8)\n",
        "            ax.set_ylabel('Value', fontsize=8)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            ax.tick_params(labelsize=7)\n",
        "            \n",
        "            # Add decision summary text below the plot showing how features led to classification\n",
        "            features = result.get('features', {})\n",
        "            classification = result.get('classification', 'Unknown')\n",
        "            reason = result.get('reason', '')\n",
        "            detector_info = result.get('detector_info', {})\n",
        "            \n",
        "            if features:\n",
        "                decision_lines = []\n",
        "                \n",
        "                # Extract key features for decision display\n",
        "                acf1 = features.get('acf1', np.nan)\n",
        "                stability = features.get('stability', np.nan)\n",
        "                has_weekly = features.get('has_weekly_seasonality', False)\n",
        "                has_monthly = features.get('has_monthly_seasonality', False)\n",
        "                skewness = features.get('skewness', np.nan)\n",
        "                kurtosis = features.get('kurtosis', np.nan)\n",
        "                zero_crossing_rate = features.get('zero_crossing_rate', np.nan)\n",
        "                interquartile_range = features.get('interquartile_range', np.nan)\n",
        "                mean_val = features.get('mean', np.nan)\n",
        "                data_points = features.get('data_points', np.nan)\n",
        "                outlier_ratio = detector_info.get('outlier_ratio', 0.0)\n",
        "                changepoint_count = detector_info.get('changepoint_count', 0)\n",
        "                \n",
        "                # Classification decision factors\n",
        "                decision_lines.append(f\"Classification: {classification}\")\n",
        "                \n",
        "                # Primary decision factors\n",
        "                primary_factors = []\n",
        "                if not np.isnan(acf1):\n",
        "                    if acf1 > 0.5:\n",
        "                        primary_factors.append(f\"ACF1={acf1:.2f} (strong)\")\n",
        "                    elif acf1 > 0.2:\n",
        "                        primary_factors.append(f\"ACF1={acf1:.2f} (moderate)\")\n",
        "                    else:\n",
        "                        primary_factors.append(f\"ACF1={acf1:.2f} (low)\")\n",
        "                \n",
        "                if not np.isnan(stability):\n",
        "                    if stability > 0.7:\n",
        "                        primary_factors.append(f\"Stability={stability:.2f} (high)\")\n",
        "                    elif stability > 0.5:\n",
        "                        primary_factors.append(f\"Stability={stability:.2f} (moderate)\")\n",
        "                    else:\n",
        "                        primary_factors.append(f\"Stability={stability:.2f} (low)\")\n",
        "                \n",
        "                if has_weekly or has_monthly:\n",
        "                    seasonality_list = []\n",
        "                    if has_weekly:\n",
        "                        seasonality_list.append('weekly')\n",
        "                    if has_monthly:\n",
        "                        seasonality_list.append('monthly')\n",
        "                    primary_factors.append(f\"Seasonality: {', '.join(seasonality_list)}\")\n",
        "                \n",
        "                if primary_factors:\n",
        "                    decision_lines.append(\"Primary: \" + \" | \".join(primary_factors))\n",
        "                \n",
        "                # Negative indicators (noise/variability)\n",
        "                negative_indicators = []\n",
        "                \n",
        "                # Zero crossing rate\n",
        "                if not np.isnan(zero_crossing_rate) and not np.isnan(data_points) and data_points > 0:\n",
        "                    zcr_normalized = zero_crossing_rate / data_points\n",
        "                    if zcr_normalized > 0.3:\n",
        "                        negative_indicators.append(f\"High noise (ZCR={zcr_normalized:.1%})\")\n",
        "                    elif zcr_normalized > 0.1:\n",
        "                        negative_indicators.append(f\"Moderate noise (ZCR={zcr_normalized:.1%})\")\n",
        "                \n",
        "                # Skewness\n",
        "                if not np.isnan(skewness):\n",
        "                    if abs(skewness) > 2.0:\n",
        "                        negative_indicators.append(f\"Extreme skew ({skewness:.2f})\")\n",
        "                    elif abs(skewness) > 1.0:\n",
        "                        negative_indicators.append(f\"Moderate skew ({skewness:.2f})\")\n",
        "                \n",
        "                # Kurtosis\n",
        "                if not np.isnan(kurtosis):\n",
        "                    if kurtosis > 5.0:\n",
        "                        negative_indicators.append(f\"Heavy tails (kurt={kurtosis:.2f})\")\n",
        "                    elif kurtosis > 4.0:\n",
        "                        negative_indicators.append(f\"Moderate tails (kurt={kurtosis:.2f})\")\n",
        "                \n",
        "                # IQR variability\n",
        "                if not np.isnan(interquartile_range) and not np.isnan(mean_val) and abs(mean_val) > 0:\n",
        "                    iqr_coefficient = interquartile_range / abs(mean_val)\n",
        "                    if iqr_coefficient > 1.0:\n",
        "                        negative_indicators.append(f\"High variability (IQR/mean={iqr_coefficient:.2f})\")\n",
        "                    elif iqr_coefficient > 0.5:\n",
        "                        negative_indicators.append(f\"Moderate variability (IQR/mean={iqr_coefficient:.2f})\")\n",
        "                \n",
        "                # Data quality issues\n",
        "                if outlier_ratio > 0.2:\n",
        "                    negative_indicators.append(f\"High outliers ({outlier_ratio:.1%})\")\n",
        "                elif outlier_ratio > 0.1:\n",
        "                    negative_indicators.append(f\"Moderate outliers ({outlier_ratio:.1%})\")\n",
        "                \n",
        "                if changepoint_count > 5:\n",
        "                    negative_indicators.append(f\"Many changepoints ({changepoint_count})\")\n",
        "                elif changepoint_count > 2:\n",
        "                    negative_indicators.append(f\"Some changepoints ({changepoint_count})\")\n",
        "                \n",
        "                if negative_indicators:\n",
        "                    decision_lines.append(\"Concerns: \" + \" | \".join(negative_indicators[:3]))  # Limit to 3 concerns\n",
        "                \n",
        "                # Format decision text\n",
        "                if len(decision_lines) > 0:\n",
        "                    decision_text = \"\\n\".join(decision_lines)\n",
        "                    \n",
        "                    # Add text below the plot\n",
        "                    ax.text(0.5, -0.20, decision_text, transform=ax.transAxes,\n",
        "                           ha='center', va='top', fontsize=7, \n",
        "                           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7))\n",
        "        \n",
        "        # Hide unused subplots\n",
        "        for idx in range(n_series, len(axes)):\n",
        "            axes[idx].set_visible(False)\n",
        "        \n",
        "        # Adjust layout to make room for decision text below plots\n",
        "        # Increase bottom margin to accommodate decision summary\n",
        "        plt.tight_layout(rect=[0, 0.08, 1, 0.96])\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary statistics for this category\n",
        "        if len(series_list) > 0:\n",
        "            print(f\"\\n{category_name} Summary Statistics:\")\n",
        "            \n",
        "            # Collect TSFEL features\n",
        "            acf1_values = []\n",
        "            stability_values = []\n",
        "            trend_strength_values = []\n",
        "            weekly_count = 0\n",
        "            monthly_count = 0\n",
        "            outlier_ratios = []\n",
        "            \n",
        "            for result in series_list:\n",
        "                features = result.get('features', {})\n",
        "                \n",
        "                acf1 = features.get('acf1', np.nan)\n",
        "                if not np.isnan(acf1):\n",
        "                    acf1_values.append(acf1)\n",
        "                \n",
        "                stability = features.get('stability', np.nan)\n",
        "                if not np.isnan(stability):\n",
        "                    stability_values.append(stability)\n",
        "                \n",
        "                trend_strength = features.get('trend_strength', np.nan)\n",
        "                if not np.isnan(trend_strength):\n",
        "                    trend_strength_values.append(trend_strength)\n",
        "                \n",
        "                if features.get('has_weekly_seasonality', False):\n",
        "                    weekly_count += 1\n",
        "                if features.get('has_monthly_seasonality', False):\n",
        "                    monthly_count += 1\n",
        "                \n",
        "                detector_info = result.get('detector_info', {})\n",
        "                outlier_ratio = detector_info.get('outlier_ratio', np.nan)\n",
        "                if not np.isnan(outlier_ratio):\n",
        "                    outlier_ratios.append(outlier_ratio)\n",
        "            \n",
        "            print(f\"  Total series: {len(series_list)}\")\n",
        "            \n",
        "            if acf1_values:\n",
        "                print(f\"  Average ACF1: {np.mean(acf1_values):.3f}\")\n",
        "                print(f\"  Median ACF1: {np.median(acf1_values):.3f}\")\n",
        "            \n",
        "            if stability_values:\n",
        "                print(f\"  Average Stability: {np.mean(stability_values):.3f}\")\n",
        "                print(f\"  Median Stability: {np.median(stability_values):.3f}\")\n",
        "            \n",
        "            if trend_strength_values:\n",
        "                print(f\"  Average Trend Strength: {np.mean(trend_strength_values):.3f}\")\n",
        "                print(f\"  Median Trend Strength: {np.median(trend_strength_values):.3f}\")\n",
        "            \n",
        "            # Seasonality statistics\n",
        "            print(f\"  Seasonality Detection:\")\n",
        "            print(f\"    Weekly: {weekly_count}/{len(series_list)} series ({weekly_count/len(series_list)*100:.1f}%)\")\n",
        "            print(f\"    Monthly: {monthly_count}/{len(series_list)} series ({monthly_count/len(series_list)*100:.1f}%)\")\n",
        "            \n",
        "            if outlier_ratios:\n",
        "                print(f\"  Average Outlier Ratio: {np.mean(outlier_ratios):.3f}\")\n",
        "                print(f\"  Median Outlier Ratio: {np.median(outlier_ratios):.3f}\")\n",
        "            \n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot series grouped by classification\n",
        "print(\"=\"*60)\n",
        "print(\"TSFEL CLASSIFICATIONS - Historical Data by Category\")\n",
        "print(\"=\"*60)\n",
        "plot_series_by_category(evaluation_results, max_series_per_plot=MAX_SERIES_PER_PLOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Recommendations\n",
        "\n",
        "Final summary of the analysis with recommendations for model selection and comparison with cross-validation approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"FINAL SUMMARY AND RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nTotal Series Analyzed: {len(evaluation_results)}\")\n",
        "print(f\"Query Selector: {SELECTOR}\")\n",
        "print(f\"History Period: {HISTORY_DAYS} days\")\n",
        "\n",
        "# Classification summary\n",
        "predictable = sum(1 for r in evaluation_results if r.get('classification') == 'Predictable')\n",
        "low = sum(1 for r in evaluation_results if r.get('classification') == 'Low Predictability')\n",
        "not_suitable = sum(1 for r in evaluation_results if r.get('classification') == 'Not Suitable')\n",
        "\n",
        "print(f\"\\nTSFEL-based Classification Results:\")\n",
        "print(f\"  Predictable: {predictable} ({predictable/len(evaluation_results)*100:.1f}%)\")\n",
        "print(f\"  Low Predictability: {low} ({low/len(evaluation_results)*100:.1f}%)\")\n",
        "print(f\"  Not Suitable: {not_suitable} ({not_suitable/len(evaluation_results)*100:.1f}%)\")\n",
        "\n",
        "# Seasonality summary\n",
        "series_with_seasonality = 0\n",
        "weekly_count = 0\n",
        "monthly_count = 0\n",
        "for result in evaluation_results:\n",
        "    features = result.get('features', {})\n",
        "    if features.get('has_weekly_seasonality', False):\n",
        "        weekly_count += 1\n",
        "        series_with_seasonality += 1\n",
        "    if features.get('has_monthly_seasonality', False):\n",
        "        monthly_count += 1\n",
        "        if not features.get('has_weekly_seasonality', False):\n",
        "            series_with_seasonality += 1\n",
        "\n",
        "print(f\"\\nSeasonality Detection:\")\n",
        "print(f\"  Series with weekly seasonality: {weekly_count} ({weekly_count/len(evaluation_results)*100:.1f}%)\")\n",
        "print(f\"  Series with monthly seasonality: {monthly_count} ({monthly_count/len(evaluation_results)*100:.1f}%)\")\n",
        "print(f\"  Total series with any seasonality: {series_with_seasonality} ({series_with_seasonality/len(evaluation_results)*100:.1f}%)\")\n",
        "\n",
        "# Feature statistics\n",
        "acf1_values = []\n",
        "stability_values = []\n",
        "for result in evaluation_results:\n",
        "    features = result.get('features', {})\n",
        "    acf1 = features.get('acf1', np.nan)\n",
        "    stability = features.get('stability', np.nan)\n",
        "    if not np.isnan(acf1):\n",
        "        acf1_values.append(acf1)\n",
        "    if not np.isnan(stability):\n",
        "        stability_values.append(stability)\n",
        "\n",
        "if acf1_values:\n",
        "    print(f\"\\nTSFEL Feature Statistics:\")\n",
        "    print(f\"  Average ACF1: {np.mean(acf1_values):.3f}\")\n",
        "    print(f\"  Median ACF1: {np.median(acf1_values):.3f}\")\n",
        "if stability_values:\n",
        "    print(f\"  Average Stability: {np.mean(stability_values):.3f}\")\n",
        "    print(f\"  Median Stability: {np.median(stability_values):.3f}\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\nRecommendations:\")\n",
        "print(f\"  - Use Prophet or ARIMA for series classified as 'Predictable'\")\n",
        "print(f\"  - Series with detected weekly/monthly seasonality are good candidates for Prophet\")\n",
        "print(f\"  - Review 'Low Predictability' series - may need feature engineering or different models\")\n",
        "print(f\"  - 'Not Suitable' series may require data cleaning or alternative approaches\")\n",
        "\n",
        "print(f\"\\nComparison with Cross-Validation Approach:\")\n",
        "print(f\"  - This TSFEL-based approach is faster (no model training required)\")\n",
        "print(f\"  - Classification is based on statistical features, not model performance\")\n",
        "print(f\"  - Focus on seasonality detection (weekly/monthly) for forecasting suitability\")\n",
        "print(f\"  - Use cross-validation approach (predictability_classification.ipynb) for model-specific evaluation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
