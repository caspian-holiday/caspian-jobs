{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet Forecast Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Query historical metrics from Victoria Metrics\n",
    "2. Use darts Prophet model for forecasting\n",
    "3. Generate forecasts and save to database\n",
    "\n",
    "**Configuration:** \n",
    "- Connection settings (VM URL, DB credentials) are provided via parameters or environment variables\n",
    "- Query selector and history days are configured in the query section (section 3)\n",
    "- Model parameters are configured in the model training section (section 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell (tagged for papermill injection)\n",
    "# These will be injected by papermill when executed by the job\n",
    "# When running locally, use environment variables instead\n",
    "# Note: selector, history_days, forecast parameters, and model parameters\n",
    "# are hardcoded in the notebook cells where they are used (NOT passed as parameters)\n",
    "# Database configuration is loaded from YAML config file using VM_JOBS_ENVIRONMENT and VM_JOBS_DB_PASSWORD\n",
    "vm_query_url = ''\n",
    "vm_token = ''\n",
    "vm_jobs_environment = ''\n",
    "dry_run = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**Connection settings:** Set via papermill parameters or environment variables.\n",
    "**Forecasting parameters:** Will be defined in the model training cell (see section 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Connection settings: from papermill parameters or environment variables\n",
    "# Database configuration is loaded automatically from YAML config using VM_JOBS_ENVIRONMENT and VM_JOBS_DB_PASSWORD\n",
    "import os\n",
    "\n",
    "# Victoria Metrics connection - from parameters/env vars (prefer parameters)\n",
    "# Parameters are now individual variables (not a dict) due to papermill requirements\n",
    "VM_QUERY_URL = vm_query_url if vm_query_url else os.getenv('VM_QUERY_URL', 'http://victoria-metrics:8428')\n",
    "VM_TOKEN = vm_token if vm_token else os.getenv('VM_TOKEN', '')\n",
    "\n",
    "# Environment - from parameters/env vars (prefer parameters)\n",
    "VM_JOBS_ENVIRONMENT = vm_jobs_environment if vm_jobs_environment else os.getenv('VM_JOBS_ENVIRONMENT', '')\n",
    "# Config file path - from environment variable\n",
    "VM_JOBS_CONFIG_PATH = os.getenv('VM_JOBS_CONFIG_PATH', '')\n",
    "\n",
    "# Dry run mode: train and plot, but don't save to database\n",
    "# Handle both bool and string types from papermill\n",
    "if isinstance(dry_run, bool):\n",
    "    DRY_RUN = dry_run\n",
    "elif isinstance(dry_run, str):\n",
    "    DRY_RUN = dry_run.lower() in ('true', '1', 'yes')\n",
    "else:\n",
    "    DRY_RUN = os.getenv('DRY_RUN', 'false').lower() in ('true', '1', 'yes')\n",
    "\n",
    "print(f\"VM Query URL: {VM_QUERY_URL}\")\n",
    "print(f\"VM_JOBS_ENVIRONMENT: {VM_JOBS_ENVIRONMENT or 'NOT SET (will use env var)'}\")\n",
    "print(f\"Dry Run Mode: {DRY_RUN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to Python path\n",
    "current_dir = str(Path.cwd())\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Darts imports\n",
    "from darts import TimeSeries\n",
    "from darts.models import Prophet as DartsProphet\n",
    "\n",
    "# Helper modules\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from database_helpers import (\n",
    "    create_database_connection,\n",
    "    create_forecast_run_record,\n",
    "    save_forecasts_to_database\n",
    ")\n",
    "\n",
    "print(\"Imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style for dry run mode (if enabled)\n",
    "if DRY_RUN:\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.rcParams['figure.figsize'] = (15, 8)\n",
    "    print(\"Plotting enabled for dry run mode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connect to Victoria Metrics and Query Data\n",
    "\n",
    "**Configure your selector and history days in the cell below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromQL selector - HARDCODE THIS\n",
    "SELECTOR = '{job=\"extractor\"}'  # EDIT THIS: Your PromQL selector\n",
    "\n",
    "# History parameter - HARDCODE THIS\n",
    "HISTORY_DAYS = 365  # Days of history to fetch\n",
    "\n",
    "# Connect to Victoria Metrics and query historical data\n",
    "# Connect to Victoria Metrics and query historical data\n",
    "headers = {\"Authorization\": f\"Bearer {VM_TOKEN}\"} if VM_TOKEN else {}\n",
    "prom = PrometheusConnect(url=VM_QUERY_URL, headers=headers, disable_ssl=True)\n",
    "print(f\"Connected to Victoria Metrics at {VM_QUERY_URL}\")\n",
    "\n",
    "\n",
    "print(f\"\\nQuerying: {SELECTOR}\")\n",
    "end_date = datetime.now(timezone.utc)\n",
    "start_date = end_date - timedelta(days=HISTORY_DAYS)\n",
    "query_result = prom.custom_query_range(\n",
    "    query=SELECTOR.replace(\"'\", '\"'),  # Ensure double quotes for PromQL\n",
    "    start_time=start_date,\n",
    "    end_time=end_date,\n",
    "    step=\"24h\"\n",
    ")\n",
    "\n",
    "print(f\"Query range: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Query returned {len(query_result)} series\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse and Prepare Data\n",
    "\n",
    "This parses ALL time series returned by the selector query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all series from query result\n",
    "all_series = []\n",
    "for item in query_result:\n",
    "    metric = item.get('metric', {})\n",
    "    metric_name = metric.get('__name__')\n",
    "    if not metric_name:\n",
    "        continue\n",
    "    labels = {k: v for k, v in metric.items() if k != '__name__'}\n",
    "    values = item.get('values', [])\n",
    "    samples = [(datetime.fromtimestamp(float(ts), tz=timezone.utc), float(value)) for ts, value in values]\n",
    "    if samples:\n",
    "        all_series.append((samples, {'metric_name': metric_name, 'labels': labels}))\n",
    "\n",
    "if not all_series:\n",
    "    raise ValueError(\"No data found for selector\")\n",
    "\n",
    "print(f\"Found {len(all_series)} time series for selector: {SELECTOR}\")\n",
    "print(\"\\nSeries preview:\")\n",
    "for idx, (samples, series_info) in enumerate(all_series[:5]):\n",
    "    print(f\"  {idx+1}. {series_info['metric_name']} {series_info['labels']}\")\n",
    "if len(all_series) > 5:\n",
    "    print(f\"  ... and {len(all_series) - 5} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model and Generate Forecast\n",
    "\n",
    "**Note:** This processes ALL time series returned by the selector, training a separate model for each series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters - HARDCODE THESE\n",
    "FORECAST_HORIZON_DAYS = 20  # Business days to forecast ahead\n",
    "MIN_HISTORY_POINTS = 30  # Minimum data points required\n",
    "PROPHET_PARAMS = {\n",
    "    'yearly_seasonality': False,\n",
    "    'weekly_seasonality': False,\n",
    "    'daily_seasonality': False,\n",
    "    'seasonality_mode': 'additive',\n",
    "    'changepoint_prior_scale': 0.5,\n",
    "}\n",
    "PROPHET_FIT_PARAMS = {}  # Optional fit parameters\n",
    "\n",
    "# Floor parameter: set to 0 to prevent negative forecasts\n",
    "# Alternatively, set to a small positive value (e.g., 0.01) if your metrics should never be zero\n",
    "FORECAST_FLOOR = 0  # Minimum value for forecasts (set to None to disable)\n",
    "\n",
    "print(f\"Forecast horizon: {FORECAST_HORIZON_DAYS} business days\")\n",
    "print(f\"Prophet params: {PROPHET_PARAMS}\\n\")\n",
    "\n",
    "# Process each series: train model and generate forecast\n",
    "forecasts_by_series = []\n",
    "\n",
    "for series_idx, (samples, series_info) in enumerate(all_series):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {series_idx + 1}/{len(all_series)}: {series_info['metric_name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare business day data (query result already has one value per 24h)\n",
    "        df_training = pd.DataFrame(samples, columns=['ds', 'y'])\n",
    "        df_training['ds'] = pd.to_datetime(df_training['ds'], utc=True).dt.tz_localize(None)\n",
    "        \n",
    "        if df_training.empty:\n",
    "            print(f\"  ⚠️  Skipping: no data\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if len(df_training) < MIN_HISTORY_POINTS:\n",
    "            print(f\"  ⚠️  Skipping: insufficient data ({len(df_training)} < {MIN_HISTORY_POINTS})\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Training data: {len(df_training)} points, {df_training['ds'].min().date()} to {df_training['ds'].max().date()}\")\n",
    "        \n",
    "        # Add floor/cap columns if floor is specified (required for Prophet to enforce floor)\n",
    "        # Prophet requires logistic growth mode and floor/cap columns in the data\n",
    "        prophet_params = PROPHET_PARAMS.copy()\n",
    "        if FORECAST_FLOOR is not None:\n",
    "            # Set growth to logistic (required for floor/cap to work)\n",
    "            prophet_params['growth'] = 'logistic'\n",
    "            # Add floor column to training data\n",
    "            df_training['floor'] = FORECAST_FLOOR\n",
    "            # Add cap (set to a value above max to avoid constraining upper bound)\n",
    "            df_training['cap'] = df_training['y'].max() * 1.5 if df_training['y'].max() > FORECAST_FLOOR else FORECAST_FLOOR + 1\n",
    "        \n",
    "        # Convert to darts TimeSeries (including floor/cap columns if present)\n",
    "        series = TimeSeries.from_dataframe(df_training.set_index('ds'))\n",
    "        \n",
    "        model = DartsProphet(**prophet_params)\n",
    "        model.fit(series)\n",
    "        \n",
    "        # Generate forecast with uncertainty intervals (use num_samples for probabilistic forecast)\n",
    "        # num_samples=100 generates 100 sample paths to estimate uncertainty\n",
    "        # If floor is set via logistic growth, Prophet will automatically respect it\n",
    "        forecast = model.predict(n=FORECAST_HORIZON_DAYS, num_samples=100)\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'ds': forecast.time_index,\n",
    "            'yhat': forecast.values().flatten()\n",
    "        })\n",
    "        \n",
    "        # Extract 95% prediction intervals (2.5% and 97.5% quantiles)\n",
    "        if hasattr(forecast, 'quantile_timeseries'):\n",
    "            try:\n",
    "                lower = forecast.quantile_timeseries(0.025)  # 2.5% quantile\n",
    "                upper = forecast.quantile_timeseries(0.975)  # 97.5% quantile\n",
    "                forecast_df['yhat_lower'] = lower.values().flatten()\n",
    "                forecast_df['yhat_upper'] = upper.values().flatten()\n",
    "            except Exception:\n",
    "                # If quantiles are not available, skip intervals\n",
    "                pass\n",
    "        \n",
    "        print(f\"  ✓ Forecast: {len(forecast_df)} predictions\")\n",
    "        \n",
    "        # Store forecast\n",
    "        if DRY_RUN:\n",
    "            forecasts_by_series.append((series_info, forecast_df, df_training))\n",
    "        else:\n",
    "            forecasts_by_series.append((series_info, forecast_df))\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(f\"  ✗ Failed: {exc}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Successfully forecasted {len(forecasts_by_series)}/{len(all_series)} series\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results (Dry Run Mode)\n",
    "\n",
    "**Note:** In dry run mode, plots are generated for each series showing historical data and forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecasts for each series (dry run mode)\n",
    "if DRY_RUN:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DRY RUN MODE: Generating plots (no database operations)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for plot_idx, forecast_item in enumerate(forecasts_by_series):\n",
    "        # Unpack: (series_info, forecast_df, df_training) in dry run mode\n",
    "        series_info, forecast_df, df_training = forecast_item\n",
    "        \n",
    "        plt.figure(figsize=(18, 8))\n",
    "        \n",
    "        # Plot historical data\n",
    "        plt.plot(df_training['ds'], df_training['y'], \n",
    "                'ko-', label='Historical Data', linewidth=2, markersize=3, alpha=0.6)\n",
    "        \n",
    "        # Plot forecast trend\n",
    "        plt.plot(forecast_df['ds'], forecast_df['yhat'], \n",
    "                'b--', label='Forecast (trend)', linewidth=2.5)\n",
    "        \n",
    "        # Plot uncertainty intervals (if available)\n",
    "        if 'yhat_lower' in forecast_df.columns and 'yhat_upper' in forecast_df.columns:\n",
    "            plt.fill_between(forecast_df['ds'], \n",
    "                           forecast_df['yhat_lower'], \n",
    "                           forecast_df['yhat_upper'],\n",
    "                           alpha=0.2, color='blue', label='Uncertainty Interval')\n",
    "        \n",
    "        # Vertical line showing where forecast starts\n",
    "        last_history_date = df_training['ds'].max()\n",
    "        plt.axvline(x=last_history_date, color='red', linestyle=':', \n",
    "                   linewidth=2, label='Forecast Start', alpha=0.7)\n",
    "        \n",
    "        # Title and labels\n",
    "        title = f\"Prophet Forecast: {series_info['metric_name']}\"\n",
    "        if series_info['labels']:\n",
    "            title += f\" {series_info['labels']}\"\n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Value', fontsize=12)\n",
    "        plt.legend(loc='best', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"  ✓ Plotted forecast for {series_info['metric_name']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dry run complete: {len(forecasts_by_series)} series forecasted and plotted\")\n",
    "    print(\"No data was saved to database.\")\n",
    "else:\n",
    "    # Create database connection using helper function\n",
    "    # This automatically loads database config from YAML using VM_JOBS_ENVIRONMENT and VM_JOBS_DB_PASSWORD\n",
    "    engine, conn = create_database_connection(environment=VM_JOBS_ENVIRONMENT if VM_JOBS_ENVIRONMENT else None, config_path=VM_JOBS_CONFIG_PATH if VM_JOBS_CONFIG_PATH else None)\n",
    "    \n",
    "    print(\"Database connection established\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Forecasts to Database\n",
    "\n",
    "**Note:** This step is skipped in dry run mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save forecasts to database for each series (only if not dry run)\n",
    "if not DRY_RUN:\n",
    "    # Create run record once (shared across all series for this selector)\n",
    "    # Only include intervals if they exist in the forecast\n",
    "    forecast_types = [{\"name\": \"trend\", \"field\": \"yhat\"}]\n",
    "    # Check if any forecast has intervals to determine if we should include them\n",
    "    has_intervals = any(\n",
    "        'yhat_lower' in item[1].columns and 'yhat_upper' in item[1].columns\n",
    "        for item in forecasts_by_series\n",
    "    )\n",
    "    if has_intervals:\n",
    "        forecast_types.extend([\n",
    "            {\"name\": \"lower\", \"field\": \"yhat_lower\"},\n",
    "            {\"name\": \"upper\", \"field\": \"yhat_upper\"},\n",
    "        ])\n",
    "    \n",
    "    run_id = create_forecast_run_record(\n",
    "        conn=conn,\n",
    "        job_id=\"metrics_forecast_notebooks\",\n",
    "        selection_value=SELECTOR,\n",
    "        model_type=\"prophet\",\n",
    "        model_config=PROPHET_PARAMS,\n",
    "        model_fit_config=PROPHET_FIT_PARAMS,\n",
    "        history_days=HISTORY_DAYS,\n",
    "        forecast_horizon_days=FORECAST_HORIZON_DAYS,\n",
    "        min_history_points=MIN_HISTORY_POINTS,\n",
    "        config_source=\"notebook\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Created forecast run record: run_id={run_id}\")\n",
    "    print(f\"\\nSaving forecasts for {len(forecasts_by_series)} series...\")\n",
    "    \n",
    "    total_rows_inserted = 0\n",
    "    for forecast_item in forecasts_by_series:\n",
    "        # Unpack: (series_info, forecast_df) in normal mode\n",
    "        series_info, forecast_df = forecast_item[0], forecast_item[1]\n",
    "        try:\n",
    "            rows_inserted, _, _ = save_forecasts_to_database(\n",
    "                conn=conn,\n",
    "                metric_name=series_info['metric_name'],\n",
    "                labels=series_info['labels'],\n",
    "                forecast_df=forecast_df,\n",
    "                forecast_types=forecast_types,\n",
    "                run_id=run_id,  # Link all forecasts to the same parameter record\n",
    "            )\n",
    "            total_rows_inserted += rows_inserted\n",
    "            print(f\"  ✓ {series_info['metric_name']}: {rows_inserted} rows saved\")\n",
    "        except Exception as exc:\n",
    "            print(f\"  ✗ {series_info['metric_name']}: Failed to save - {exc}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {total_rows_inserted} forecast rows saved to database\")\n",
    "else:\n",
    "    print(\"Dry run mode: Skipping database save operations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection (only if not dry run)\n",
    "if not DRY_RUN:\n",
    "    conn.close()\n",
    "    engine.dispose()\n",
    "    print(\"Database connection closed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
