{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Predictability Classification Notebook\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Query time series from Victoria Metrics using a PromQL selector\n",
    "2. Evaluate predictability characteristics for each time series\n",
    "3. Classify each series by predictability for Prophet and ARIMA forecasting models\n",
    "4. Visualize results including historical data grouped by categories\n",
    "\n",
    "**Classification Categories:**\n",
    "- **Predictable**: Series suitable for forecasting (includes those with clear seasonality patterns)\n",
    "- **Low Predictability**: Weak patterns, high error\n",
    "- **Not Suitable**: Cannot be forecasted (insufficient data, model failures, etc.)\n",
    "\n",
    "**Use this notebook for:**\n",
    "- Understanding which time series are suitable for forecasting\n",
    "- Comparing Prophet vs ARIMA suitability\n",
    "- Identifying patterns in predictable vs unpredictable series\n",
    "- Making informed decisions about which models to use for different metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to Python path\n",
    "current_dir = str(Path.cwd())\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Victoria Metrics connection - from environment variables\n",
    "VM_QUERY_URL = os.getenv('VM_QUERY_URL', 'http://victoria-metrics:8428')\n",
    "VM_TOKEN = os.getenv('VM_TOKEN', '')\n",
    "\n",
    "print(f\"VM Query URL: {VM_QUERY_URL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical tests\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from scipy import stats\n",
    "\n",
    "# Darts imports for time series and models\n",
    "from darts import TimeSeries\n",
    "from darts.models import Prophet as DartsProphet, ARIMA as DartsARIMA\n",
    "from darts.metrics import mape, rmse, mae\n",
    "\n",
    "# Helper modules\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "print(\"Imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Victoria Metrics and Query Data\n",
    "\n",
    "**Configure your selector and history days in the cell below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromQL selector - EDIT THIS\n",
    "SELECTOR = '{job=\"extractor\"}'  # Your PromQL selector\n",
    "\n",
    "# History parameter - EDIT THIS\n",
    "HISTORY_DAYS = 365  # Days of history to fetch\n",
    "\n",
    "# Cross-validation parameters (optimized for ~7 months of data)\n",
    "CV_FOLDS = 5  # Number of cross-validation folds\n",
    "CV_HORIZON = 14  # Forecast horizon for CV (days) - 2 weeks\n",
    "# Minimum data points: need enough for k-fold (cv_horizon * (cv_folds + 1) + minimum training size)\n",
    "MIN_HISTORY_POINTS = max(60, CV_HORIZON * (CV_FOLDS + 1))  # At least 60 days for seasonality + k-fold requirements\n",
    "MAX_SERIES_PER_PLOT = 50  # Maximum series per category plot\n",
    "\n",
    "# Connect to Victoria Metrics and query historical data\n",
    "headers = {\"Authorization\": f\"Bearer {VM_TOKEN}\"} if VM_TOKEN else {}\n",
    "prom = PrometheusConnect(url=VM_QUERY_URL, headers=headers, disable_ssl=True)\n",
    "print(f\"Connected to Victoria Metrics at {VM_QUERY_URL}\")\n",
    "\n",
    "print(f\"\\nQuerying: {SELECTOR}\")\n",
    "end_date = datetime.now(timezone.utc)\n",
    "start_date = end_date - timedelta(days=HISTORY_DAYS)\n",
    "query_result = prom.custom_query_range(\n",
    "    query=SELECTOR.replace(\"'\", '\"'),  # Ensure double quotes for PromQL\n",
    "    start_time=start_date,\n",
    "    end_time=end_date,\n",
    "    step=\"24h\"\n",
    ")\n",
    "\n",
    "print(f\"Query range: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Query returned {len(query_result)} series\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse and Prepare Time Series Data\n",
    "\n",
    "This parses ALL time series returned by the selector query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all series from query result\n",
    "all_series = []\n",
    "for item in query_result:\n",
    "    metric = item.get('metric', {})\n",
    "    metric_name = metric.get('__name__')\n",
    "    if not metric_name:\n",
    "        continue\n",
    "    labels = {k: v for k, v in metric.items() if k != '__name__'}\n",
    "    values = item.get('values', [])\n",
    "    samples = [(datetime.fromtimestamp(float(ts), tz=timezone.utc), float(value)) for ts, value in values]\n",
    "    if samples:\n",
    "        all_series.append((samples, {'metric_name': metric_name, 'labels': labels}))\n",
    "\n",
    "if not all_series:\n",
    "    raise ValueError(\"No data found for selector\")\n",
    "\n",
    "print(f\"Found {len(all_series)} time series for selector: {SELECTOR}\")\n",
    "print(\"\\nSeries preview:\")\n",
    "for idx, (samples, series_info) in enumerate(all_series[:5]):\n",
    "    print(f\"  {idx+1}. {series_info['metric_name']} {series_info['labels']}\")\n",
    "if len(all_series) > 5:\n",
    "    print(f\"  ... and {len(all_series) - 5} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Predictability Features\n",
    "\n",
    "This section computes statistical features and model performance metrics for each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistical_features(df):\n",
    "    \"\"\"Compute statistical features for a time series.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(df) < 2:\n",
    "        return features\n",
    "    \n",
    "    values = df['y'].values\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['mean'] = np.mean(values)\n",
    "    features['std'] = np.std(values)\n",
    "    features['cv'] = features['std'] / features['mean'] if features['mean'] != 0 else np.inf\n",
    "    features['min'] = np.min(values)\n",
    "    features['max'] = np.max(values)\n",
    "    features['range'] = features['max'] - features['min']\n",
    "    \n",
    "    # Stationarity tests\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            adf_result = adfuller(values, autolag='AIC')\n",
    "        features['adf_pvalue'] = adf_result[1]\n",
    "        features['adf_statistic'] = adf_result[0]\n",
    "        features['is_stationary_adf'] = adf_result[1] < 0.05\n",
    "    except Exception:\n",
    "        features['adf_pvalue'] = np.nan\n",
    "        features['adf_statistic'] = np.nan\n",
    "        features['is_stationary_adf'] = False\n",
    "    \n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress KPSS warnings about p-values being smaller than returned\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "            kpss_result = kpss(values, regression='ct', nlags='auto')\n",
    "        features['kpss_pvalue'] = kpss_result[1]\n",
    "        features['kpss_statistic'] = kpss_result[0]\n",
    "        # Handle case where p-value might be very small (outside lookup table range)\n",
    "        if kpss_result[1] is not None and not np.isnan(kpss_result[1]):\n",
    "            features['is_stationary_kpss'] = kpss_result[1] > 0.05\n",
    "        else:\n",
    "            # If p-value is None or NaN, use statistic to infer (very small p-value = non-stationary)\n",
    "            features['is_stationary_kpss'] = False\n",
    "    except Exception:\n",
    "        features['kpss_pvalue'] = np.nan\n",
    "        features['kpss_statistic'] = np.nan\n",
    "        features['is_stationary_kpss'] = False\n",
    "    \n",
    "    # Trend strength (linear regression R²)\n",
    "    try:\n",
    "        x = np.arange(len(values))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)\n",
    "        features['trend_r2'] = r_value ** 2\n",
    "        features['trend_slope'] = slope\n",
    "        features['trend_pvalue'] = p_value\n",
    "    except Exception:\n",
    "        features['trend_r2'] = np.nan\n",
    "        features['trend_slope'] = np.nan\n",
    "        features['trend_pvalue'] = np.nan\n",
    "    \n",
    "    # Autocorrelation at lag 1\n",
    "    try:\n",
    "        if len(values) > 1:\n",
    "            autocorr = np.corrcoef(values[:-1], values[1:])[0, 1]\n",
    "            features['autocorr_lag1'] = autocorr if not np.isnan(autocorr) else 0\n",
    "        else:\n",
    "            features['autocorr_lag1'] = 0\n",
    "    except Exception:\n",
    "        features['autocorr_lag1'] = 0\n",
    "    \n",
    "    # Variance stability (coefficient of variation)\n",
    "    features['variance_stability'] = 1.0 / (1.0 + features['cv']) if features['cv'] > 0 else 0\n",
    "    \n",
    "    # Seasonality detection\n",
    "    # Check for weekly seasonality (7-day pattern)\n",
    "    if len(values) >= 14:  # Need at least 2 weeks\n",
    "        try:\n",
    "            # Calculate autocorrelation at lag 7 (weekly)\n",
    "            if len(values) > 7:\n",
    "                weekly_autocorr = np.corrcoef(values[:-7], values[7:])[0, 1] if len(values) > 7 else 0\n",
    "                features['weekly_autocorr'] = weekly_autocorr if not np.isnan(weekly_autocorr) else 0\n",
    "                features['has_weekly_seasonality'] = abs(weekly_autocorr) > 0.3  # Threshold for weekly pattern\n",
    "            else:\n",
    "                features['weekly_autocorr'] = 0\n",
    "                features['has_weekly_seasonality'] = False\n",
    "        except Exception:\n",
    "            features['weekly_autocorr'] = 0\n",
    "            features['has_weekly_seasonality'] = False\n",
    "    else:\n",
    "        features['weekly_autocorr'] = 0\n",
    "        features['has_weekly_seasonality'] = False\n",
    "    \n",
    "    # Check for monthly seasonality (30-day pattern)\n",
    "    if len(values) >= 60:  # Need at least 2 months\n",
    "        try:\n",
    "            lag = min(30, len(values) - 1)\n",
    "            if len(values) > lag:\n",
    "                monthly_autocorr = np.corrcoef(values[:-lag], values[lag:])[0, 1] if len(values) > lag else 0\n",
    "                features['monthly_autocorr'] = monthly_autocorr if not np.isnan(monthly_autocorr) else 0\n",
    "                features['has_monthly_seasonality'] = abs(monthly_autocorr) > 0.3  # Threshold for monthly pattern\n",
    "            else:\n",
    "                features['monthly_autocorr'] = 0\n",
    "                features['has_monthly_seasonality'] = False\n",
    "        except Exception:\n",
    "            features['monthly_autocorr'] = 0\n",
    "            features['has_monthly_seasonality'] = False\n",
    "    else:\n",
    "        features['monthly_autocorr'] = 0\n",
    "        features['has_monthly_seasonality'] = False\n",
    "    \n",
    "    # Check for yearly seasonality (365-day pattern) - only if we have enough data\n",
    "    if len(values) >= 730:  # Need at least 2 years\n",
    "        try:\n",
    "            lag = min(365, len(values) - 1)\n",
    "            if len(values) > lag:\n",
    "                yearly_autocorr = np.corrcoef(values[:-lag], values[lag:])[0, 1] if len(values) > lag else 0\n",
    "                features['yearly_autocorr'] = yearly_autocorr if not np.isnan(yearly_autocorr) else 0\n",
    "                features['has_yearly_seasonality'] = abs(yearly_autocorr) > 0.3  # Threshold for yearly pattern\n",
    "            else:\n",
    "                features['yearly_autocorr'] = 0\n",
    "                features['has_yearly_seasonality'] = False\n",
    "        except Exception:\n",
    "            features['yearly_autocorr'] = 0\n",
    "            features['has_yearly_seasonality'] = False\n",
    "    else:\n",
    "        features['yearly_autocorr'] = 0\n",
    "        features['has_yearly_seasonality'] = False\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prophet_model(series, cv_folds=5, cv_horizon=7, stat_features=None):\n",
    "    \"\"\"Evaluate Prophet model using k-fold walk-forward cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        series: TimeSeries object\n",
    "        cv_folds: Number of cross-validation folds\n",
    "        cv_horizon: Forecast horizon for CV\n",
    "        stat_features: Dictionary of statistical features including seasonality detection\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        if series is None:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'success': False, 'error': 'Series is None'}\n",
    "        \n",
    "        # Minimum data check: need enough for k-fold with minimum training size\n",
    "        min_training_size = max(60, cv_horizon * 2)  # At least 60 days or 2x horizon for training\n",
    "        min_total_size = min_training_size + cv_horizon * cv_folds\n",
    "        \n",
    "        if len(series) < min_total_size:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'success': False, \n",
    "                   'error': f'Insufficient data: {len(series)} < {min_total_size} (min for {cv_folds} folds)'}\n",
    "        \n",
    "        # Detect seasonality from features if provided\n",
    "        has_weekly = False\n",
    "        has_monthly = False\n",
    "        has_yearly = False\n",
    "        \n",
    "        if stat_features:\n",
    "            has_weekly = stat_features.get('has_weekly_seasonality', False)\n",
    "            has_monthly = stat_features.get('has_monthly_seasonality', False)\n",
    "            has_yearly = stat_features.get('has_yearly_seasonality', False)\n",
    "        \n",
    "        # K-fold walk-forward validation\n",
    "        # Use expanding windows: each fold uses more training data\n",
    "        series_len = len(series)\n",
    "        fold_mape = []\n",
    "        fold_rmse = []\n",
    "        fold_mae = []\n",
    "        \n",
    "        # Calculate fold positions - space them evenly across available data\n",
    "        # Start from minimum training size, end before the last cv_horizon\n",
    "        available_range = series_len - min_training_size - cv_horizon\n",
    "        if available_range <= 0:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'success': False, \n",
    "                   'error': 'Insufficient data for k-fold validation'}\n",
    "        \n",
    "        # Space folds evenly across available range\n",
    "        if cv_folds == 1:\n",
    "            split_points = [min_training_size]\n",
    "        else:\n",
    "            step = available_range / (cv_folds - 1) if cv_folds > 1 else available_range\n",
    "            split_points = [int(min_training_size + i * step) for i in range(cv_folds)]\n",
    "        \n",
    "        for fold_idx, train_end in enumerate(split_points):\n",
    "            try:\n",
    "                # Ensure we have enough room for test set\n",
    "                if train_end + cv_horizon > series_len:\n",
    "                    continue\n",
    "                \n",
    "                train_series = series[:train_end]\n",
    "                test_series = series[train_end:train_end + cv_horizon]\n",
    "                \n",
    "                # Build add_seasonalities list for custom seasonalities\n",
    "                # Monthly must be added via add_seasonalities (Prophet doesn't have built-in monthly)\n",
    "                add_seasonalities = []\n",
    "                \n",
    "                # Add monthly seasonality if detected\n",
    "                if has_monthly:\n",
    "                    add_seasonalities.append({\n",
    "                        'name': 'monthly',\n",
    "                        'seasonal_periods': 30.5,\n",
    "                        'fourier_order': 5\n",
    "                    })\n",
    "                \n",
    "                # If weekly is detected, add it as custom seasonality (disable built-in)\n",
    "                # This matches the pattern used in recommended parameters\n",
    "                if has_weekly:\n",
    "                    add_seasonalities.append({\n",
    "                        'name': 'weekly',\n",
    "                        'seasonal_periods': 7.0,\n",
    "                        'fourier_order': 3\n",
    "                    })\n",
    "                    # When using custom weekly seasonality, disable built-in weekly\n",
    "                    weekly_seasonality_param = False\n",
    "                else:\n",
    "                    weekly_seasonality_param = False\n",
    "                \n",
    "                # Prophet parameters - enable seasonality if detected\n",
    "                prophet_params = {\n",
    "                    'yearly_seasonality': has_yearly,\n",
    "                    'weekly_seasonality': weekly_seasonality_param,\n",
    "                    'daily_seasonality': False,\n",
    "                    'seasonality_mode': 'additive',\n",
    "                    'changepoint_prior_scale': 0.5,\n",
    "                }\n",
    "                \n",
    "                # Add custom seasonalities if any are detected\n",
    "                if add_seasonalities:\n",
    "                    prophet_params['add_seasonalities'] = add_seasonalities\n",
    "                \n",
    "                model = DartsProphet(**prophet_params)\n",
    "                \n",
    "                # Fit and predict\n",
    "                model.fit(train_series)\n",
    "                forecast = model.predict(len(test_series))\n",
    "                \n",
    "                # Calculate metrics for this fold\n",
    "                fold_mape.append(mape(test_series, forecast))\n",
    "                fold_rmse.append(rmse(test_series, forecast))\n",
    "                fold_mae.append(mae(test_series, forecast))\n",
    "                \n",
    "            except Exception as fold_error:\n",
    "                # Skip this fold if it fails\n",
    "                continue\n",
    "        \n",
    "        # Check if we got any successful folds\n",
    "        if len(fold_mape) == 0:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'success': False, \n",
    "                   'error': 'All folds failed'}\n",
    "        \n",
    "        # Aggregate metrics across folds (mean and std)\n",
    "        metrics = {\n",
    "            'mape': np.mean(fold_mape),\n",
    "            'rmse': np.mean(fold_rmse),\n",
    "            'mae': np.mean(fold_mae),\n",
    "            'mape_std': np.std(fold_mape),\n",
    "            'rmse_std': np.std(fold_rmse),\n",
    "            'mae_std': np.std(fold_mae),\n",
    "            'folds_successful': len(fold_mape),\n",
    "            'folds_total': cv_folds,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        metrics = {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'success': False, 'error': str(e)}\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(series, cv_folds=5, cv_horizon=7):\n",
    "    \"\"\"Evaluate ARIMA model using k-fold walk-forward cross-validation.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        if series is None:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'aic': np.inf, 'bic': np.inf, 'success': False, 'error': 'Series is None'}\n",
    "        \n",
    "        # Minimum data check: need enough for k-fold with minimum training size\n",
    "        min_training_size = max(60, cv_horizon * 2)  # At least 60 days or 2x horizon for training\n",
    "        min_total_size = min_training_size + cv_horizon * cv_folds\n",
    "        \n",
    "        if len(series) < min_total_size:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'aic': np.inf, 'bic': np.inf, 'success': False,\n",
    "                   'error': f'Insufficient data: {len(series)} < {min_total_size} (min for {cv_folds} folds)'}\n",
    "        \n",
    "        # K-fold walk-forward validation\n",
    "        series_len = len(series)\n",
    "        fold_mape = []\n",
    "        fold_rmse = []\n",
    "        fold_mae = []\n",
    "        fold_aic = []\n",
    "        fold_bic = []\n",
    "        \n",
    "        # Calculate fold positions - space them evenly across available data\n",
    "        available_range = series_len - min_training_size - cv_horizon\n",
    "        if available_range <= 0:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'aic': np.inf, 'bic': np.inf, 'success': False,\n",
    "                   'error': 'Insufficient data for k-fold validation'}\n",
    "        \n",
    "        # Space folds evenly across available range\n",
    "        if cv_folds == 1:\n",
    "            split_points = [min_training_size]\n",
    "        else:\n",
    "            step = available_range / (cv_folds - 1) if cv_folds > 1 else available_range\n",
    "            split_points = [int(min_training_size + i * step) for i in range(cv_folds)]\n",
    "        \n",
    "        for fold_idx, train_end in enumerate(split_points):\n",
    "            try:\n",
    "                # Ensure we have enough room for test set\n",
    "                if train_end + cv_horizon > series_len:\n",
    "                    continue\n",
    "                \n",
    "                train_series = series[:train_end]\n",
    "                test_series = series[train_end:train_end + cv_horizon]\n",
    "                \n",
    "                # Default ARIMA(1,1,1) parameters\n",
    "                model = DartsARIMA(p=1, d=1, q=1)\n",
    "                model.fit(train_series)\n",
    "                forecast = model.predict(len(test_series))\n",
    "                \n",
    "                # Calculate metrics for this fold\n",
    "                fold_mape.append(mape(test_series, forecast))\n",
    "                fold_rmse.append(rmse(test_series, forecast))\n",
    "                fold_mae.append(mae(test_series, forecast))\n",
    "                \n",
    "                # Get AIC/BIC from model if available (use last fold's values)\n",
    "                try:\n",
    "                    if hasattr(model, 'model') and model.model is not None:\n",
    "                        if hasattr(model.model, 'aic'):\n",
    "                            fold_aic.append(model.model.aic)\n",
    "                        if hasattr(model.model, 'bic'):\n",
    "                            fold_bic.append(model.model.bic)\n",
    "                except (AttributeError, TypeError, ValueError):\n",
    "                    pass\n",
    "                \n",
    "            except Exception as fold_error:\n",
    "                # Skip this fold if it fails\n",
    "                continue\n",
    "        \n",
    "        # Check if we got any successful folds\n",
    "        if len(fold_mape) == 0:\n",
    "            return {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'aic': np.inf, 'bic': np.inf, 'success': False,\n",
    "                   'error': 'All folds failed'}\n",
    "        \n",
    "        # Aggregate metrics across folds\n",
    "        aic_val = np.mean(fold_aic) if len(fold_aic) > 0 else np.nan\n",
    "        bic_val = np.mean(fold_bic) if len(fold_bic) > 0 else np.nan\n",
    "        \n",
    "        metrics = {\n",
    "            'mape': np.mean(fold_mape),\n",
    "            'rmse': np.mean(fold_rmse),\n",
    "            'mae': np.mean(fold_mae),\n",
    "            'mape_std': np.std(fold_mape),\n",
    "            'rmse_std': np.std(fold_rmse),\n",
    "            'mae_std': np.std(fold_mae),\n",
    "            'aic': aic_val,\n",
    "            'bic': bic_val,\n",
    "            'folds_successful': len(fold_mape),\n",
    "            'folds_total': cv_folds,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        metrics = {'mape': np.inf, 'rmse': np.inf, 'mae': np.inf, 'aic': np.inf, 'bic': np.inf, 'success': False, 'error': str(e)}\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each series and evaluate predictability\n",
    "evaluation_results = []\n",
    "\n",
    "print(f\"Evaluating {len(all_series)} time series...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for series_idx, (samples, series_info) in enumerate(all_series):\n",
    "    print(f\"\\nProcessing {series_idx + 1}/{len(all_series)}: {series_info['metric_name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        df = pd.DataFrame(samples, columns=['ds', 'y'])\n",
    "        df['ds'] = pd.to_datetime(df['ds'], utc=True).dt.tz_localize(None)\n",
    "        df = df.sort_values('ds').reset_index(drop=True)\n",
    "        \n",
    "        # Check minimum data points\n",
    "        if len(df) < MIN_HISTORY_POINTS:\n",
    "            print(f\"  ⚠️  Skipping: insufficient data ({len(df)} < {MIN_HISTORY_POINTS})\")\n",
    "            result = {\n",
    "                'series_info': series_info,\n",
    "                'df': df,\n",
    "                'statistical_features': {},\n",
    "                'prophet_metrics': {'success': False},\n",
    "                'arima_metrics': {'success': False},\n",
    "                'classification': 'Not Suitable',\n",
    "                'reason': 'Insufficient data'\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "            continue\n",
    "        \n",
    "        # Check for invalid values\n",
    "        if df['y'].isna().all() or np.isinf(df['y']).any():\n",
    "            print(f\"  ⚠️  Skipping: invalid values\")\n",
    "            result = {\n",
    "                'series_info': series_info,\n",
    "                'df': df,\n",
    "                'statistical_features': {},\n",
    "                'prophet_metrics': {'success': False},\n",
    "                'arima_metrics': {'success': False},\n",
    "                'classification': 'Not Suitable',\n",
    "                'reason': 'Invalid values'\n",
    "            }\n",
    "            evaluation_results.append(result)\n",
    "            continue\n",
    "        \n",
    "        # Compute statistical features\n",
    "        print(f\"  Computing statistical features...\")\n",
    "        stat_features = compute_statistical_features(df)\n",
    "        \n",
    "        # Convert to darts TimeSeries with daily frequency, handling missing dates\n",
    "        # Create complete date range with daily frequency\n",
    "        date_range = pd.date_range(start=df['ds'].min(), end=df['ds'].max(), freq='D')\n",
    "        \n",
    "        # Set index and reindex to fill missing dates\n",
    "        df_indexed = df.set_index('ds')[['y']]\n",
    "        df_complete = df_indexed.reindex(date_range)\n",
    "        \n",
    "        # Forward fill missing values, then backward fill any remaining\n",
    "        df_complete['y'] = df_complete['y'].ffill().bfill()\n",
    "        \n",
    "        # Convert to darts TimeSeries\n",
    "        series = TimeSeries.from_dataframe(df_complete)\n",
    "        \n",
    "        # Evaluate Prophet model (with seasonality if detected)\n",
    "        print(f\"  Evaluating Prophet model...\")\n",
    "        prophet_metrics = evaluate_prophet_model(series, cv_folds=CV_FOLDS, cv_horizon=CV_HORIZON, stat_features=stat_features)\n",
    "        \n",
    "        # Evaluate ARIMA model\n",
    "        print(f\"  Evaluating ARIMA model...\")\n",
    "        arima_metrics = evaluate_arima_model(series, cv_folds=CV_FOLDS, cv_horizon=CV_HORIZON)\n",
    "        \n",
    "        result = {\n",
    "            'series_info': series_info,\n",
    "            'df': df,\n",
    "            'statistical_features': stat_features,\n",
    "            'prophet_metrics': prophet_metrics,\n",
    "            'arima_metrics': arima_metrics,\n",
    "        }\n",
    "        \n",
    "        evaluation_results.append(result)\n",
    "        print(f\"  ✓ Completed\")\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(f\"  ✗ Failed: {exc}\")\n",
    "        result = {\n",
    "            'series_info': series_info,\n",
    "            'df': pd.DataFrame(),\n",
    "            'statistical_features': {},\n",
    "            'prophet_metrics': {'success': False, 'error': str(exc)},\n",
    "            'arima_metrics': {'success': False, 'error': str(exc)},\n",
    "            'classification': 'Not Suitable',\n",
    "            'reason': f'Error: {str(exc)}'\n",
    "        }\n",
    "        evaluation_results.append(result)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Evaluation complete: {len(evaluation_results)} series processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classify Series by Predictability\n",
    "\n",
    "Classify each series into one of three categories based on model performance, statistical features, and seasonality detection.\n",
    "\n",
    "**Seasonality Boost**: Series with detected seasonal patterns (weekly, monthly, yearly) are considered more predictable. The classification thresholds are relaxed when seasonality is detected:\n",
    "- **1 seasonality pattern**: Thresholds relaxed by ~3% (MAPE) and ~5% (RMSE)\n",
    "- **2+ seasonality patterns**: Thresholds relaxed by ~5% (MAPE) and ~10% (RMSE)\n",
    "\n",
    "This means a series with clear seasonal patterns can be classified as \"Predictable\" even if its error metrics are slightly higher than the standard thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_predictability(metrics, stat_features, mean_value):\n",
    "    \"\"\"Classify time series predictability based on metrics, features, and seasonality detection.\n",
    "    \n",
    "    Seasonality detection can boost the classification - series with clear seasonal patterns\n",
    "    are more predictable and may be upgraded to a higher category.\n",
    "    \"\"\"\n",
    "    # Handle None values\n",
    "    if metrics is None:\n",
    "        return 'Not Suitable', 'Metrics are None'\n",
    "    \n",
    "    if stat_features is None:\n",
    "        stat_features = {}\n",
    "    \n",
    "    if not metrics.get('success', False):\n",
    "        return 'Not Suitable', 'Model training failed'\n",
    "    \n",
    "    mape_val = metrics.get('mape', np.inf)\n",
    "    rmse_val = metrics.get('rmse', np.inf)\n",
    "    \n",
    "    # Calculate relative RMSE (as percentage of mean)\n",
    "    if mean_value is None or mean_value <= 0:\n",
    "        rmse_pct = np.inf\n",
    "    else:\n",
    "        rmse_pct = (rmse_val / mean_value) * 100\n",
    "    \n",
    "    # Check for seasonality patterns (boosts predictability)\n",
    "    has_weekly = stat_features.get('has_weekly_seasonality', False) if isinstance(stat_features, dict) else False\n",
    "    has_monthly = stat_features.get('has_monthly_seasonality', False) if isinstance(stat_features, dict) else False\n",
    "    has_yearly = stat_features.get('has_yearly_seasonality', False) if isinstance(stat_features, dict) else False\n",
    "    has_seasonality = has_weekly or has_monthly or has_yearly\n",
    "    \n",
    "    # Count number of seasonality patterns detected\n",
    "    seasonality_count = sum([has_weekly, has_monthly, has_yearly])\n",
    "    \n",
    "    # Seasonality boost: adjust thresholds if seasonality is detected\n",
    "    # Strong seasonality (2+ patterns) provides more boost\n",
    "    if seasonality_count >= 2:\n",
    "        # Strong seasonality: relax thresholds by 5% for MAPE and 10% for RMSE\n",
    "        mape_threshold_high = 15  # instead of 10\n",
    "        mape_threshold_mod = 35   # instead of 30\n",
    "        rmse_threshold_high = 30  # instead of 20\n",
    "        rmse_threshold_mod = 60   # instead of 50\n",
    "        seasonality_boost = \"strong\"\n",
    "    elif seasonality_count == 1:\n",
    "        # Moderate seasonality: relax thresholds by 3% for MAPE and 5% for RMSE\n",
    "        mape_threshold_high = 13  # instead of 10\n",
    "        mape_threshold_mod = 33   # instead of 30\n",
    "        rmse_threshold_high = 25  # instead of 20\n",
    "        rmse_threshold_mod = 55   # instead of 50\n",
    "        seasonality_boost = \"moderate\"\n",
    "    else:\n",
    "        # No seasonality: use standard thresholds\n",
    "        mape_threshold_high = 10\n",
    "        mape_threshold_mod = 30\n",
    "        rmse_threshold_high = 20\n",
    "        rmse_threshold_mod = 50\n",
    "        seasonality_boost = \"none\"\n",
    "    \n",
    "    # Classification logic with seasonality-adjusted thresholds\n",
    "    # Combined \"Predictable\" category (formerly Highly + Moderately Predictable)\n",
    "    # If series meets moderate threshold, it's considered Predictable\n",
    "    if mape_val < mape_threshold_mod and rmse_pct < rmse_threshold_mod:\n",
    "        category = 'Predictable'\n",
    "        reason = f'MAPE={mape_val:.2f}%, RMSE={rmse_pct:.2f}%'\n",
    "        if has_seasonality:\n",
    "            seasonality_info = []\n",
    "            if has_weekly:\n",
    "                seasonality_info.append('weekly')\n",
    "            if has_monthly:\n",
    "                seasonality_info.append('monthly')\n",
    "            if has_yearly:\n",
    "                seasonality_info.append('yearly')\n",
    "            reason += f' (seasonality: {\", \".join(seasonality_info)})'\n",
    "        return category, reason\n",
    "    \n",
    "    # Special case: If seasonality is detected and error isn't extremely high, \n",
    "    # classify as \"Predictable\" (seasonality makes series more predictable)\n",
    "    # This check happens before Low Predictability to give seasonality a boost\n",
    "    if has_seasonality and seasonality_count >= 1:\n",
    "        # For series with seasonality, use more lenient thresholds\n",
    "        # If MAPE < 50% and RMSE < 80%, consider it predictable\n",
    "        if mape_val < 50.0 and rmse_pct < 80.0:\n",
    "            category = 'Predictable'\n",
    "            seasonality_info = []\n",
    "            if has_weekly:\n",
    "                seasonality_info.append('weekly')\n",
    "            if has_monthly:\n",
    "                seasonality_info.append('monthly')\n",
    "            if has_yearly:\n",
    "                seasonality_info.append('yearly')\n",
    "            reason = f'MAPE={mape_val:.2f}%, RMSE={rmse_pct:.2f}% (seasonality detected: {\", \".join(seasonality_info)})'\n",
    "            return category, reason\n",
    "    \n",
    "    # If we get here, check for Low Predictability\n",
    "    if mape_val < np.inf and rmse_pct < np.inf:\n",
    "        category = 'Low Predictability'\n",
    "        reason = f'MAPE={mape_val:.2f}%, RMSE={rmse_pct:.2f}%'\n",
    "        if has_seasonality:\n",
    "            # Even with seasonality, if metrics are extremely poor, it's still low predictability\n",
    "            seasonality_info = []\n",
    "            if has_weekly:\n",
    "                seasonality_info.append('weekly')\n",
    "            if has_monthly:\n",
    "                seasonality_info.append('monthly')\n",
    "            if has_yearly:\n",
    "                seasonality_info.append('yearly')\n",
    "            reason += f' (has seasonality: {\", \".join(seasonality_info)} but very high error)'\n",
    "        return category, reason\n",
    "    \n",
    "    # Final fallback - should not normally reach here, but ensures we always return\n",
    "    return 'Not Suitable', 'Metrics indicate unsuitability'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each series for both Prophet and ARIMA\n",
    "for result in evaluation_results:\n",
    "    stat_features = result.get('statistical_features', {})\n",
    "    mean_value = stat_features.get('mean', 0)\n",
    "    \n",
    "    # Classify for Prophet\n",
    "    prophet_metrics = result.get('prophet_metrics', {})\n",
    "    prophet_class, prophet_reason = classify_predictability(prophet_metrics, stat_features, mean_value)\n",
    "    result['prophet_classification'] = prophet_class\n",
    "    result['prophet_reason'] = prophet_reason\n",
    "    \n",
    "    # Classify for ARIMA\n",
    "    arima_metrics = result.get('arima_metrics', {})\n",
    "    arima_class, arima_reason = classify_predictability(arima_metrics, stat_features, mean_value)\n",
    "    result['arima_classification'] = arima_class\n",
    "    result['arima_reason'] = arima_reason\n",
    "\n",
    "print(\"Classification complete!\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total series: {len(evaluation_results)}\")\n",
    "\n",
    "# Count classifications\n",
    "prophet_counts = {}\n",
    "arima_counts = {}\n",
    "for result in evaluation_results:\n",
    "    p_class = result.get('prophet_classification', 'Unknown')\n",
    "    a_class = result.get('arima_classification', 'Unknown')\n",
    "    prophet_counts[p_class] = prophet_counts.get(p_class, 0) + 1\n",
    "    arima_counts[a_class] = arima_counts.get(a_class, 0) + 1\n",
    "\n",
    "print(f\"\\nProphet Classifications:\")\n",
    "for cls, count in sorted(prophet_counts.items()):\n",
    "    print(f\"  {cls}: {count}\")\n",
    "\n",
    "print(f\"\\nARIMA Classifications:\")\n",
    "for cls, count in sorted(arima_counts.items()):\n",
    "    print(f\"  {cls}: {count}\")\n",
    "\n",
    "# Verify seasonality detection is being used in classification\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SEASONALITY DETECTION VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Count series with seasonality patterns\n",
    "series_with_seasonality = 0\n",
    "seasonality_by_category = {\n",
    "    'Predictable': {'with_seasonality': 0, 'total': 0},\n",
    "    'Low Predictability': {'with_seasonality': 0, 'total': 0},\n",
    "    'Not Suitable': {'with_seasonality': 0, 'total': 0}\n",
    "}\n",
    "\n",
    "for result in evaluation_results:\n",
    "    stat_features = result.get('statistical_features', {})\n",
    "    has_weekly = stat_features.get('has_weekly_seasonality', False)\n",
    "    has_monthly = stat_features.get('has_monthly_seasonality', False)\n",
    "    has_yearly = stat_features.get('has_yearly_seasonality', False)\n",
    "    has_seasonality = has_weekly or has_monthly or has_yearly\n",
    "    \n",
    "    if has_seasonality:\n",
    "        series_with_seasonality += 1\n",
    "    \n",
    "    # Check Prophet classification\n",
    "    prophet_class = result.get('prophet_classification', 'Unknown')\n",
    "    if prophet_class in seasonality_by_category:\n",
    "        seasonality_by_category[prophet_class]['total'] += 1\n",
    "        if has_seasonality:\n",
    "            seasonality_by_category[prophet_class]['with_seasonality'] += 1\n",
    "\n",
    "print(f\"\\nTotal series with detected seasonality: {series_with_seasonality}/{len(evaluation_results)}\")\n",
    "print(f\"\\nSeasonality impact on Prophet classifications:\")\n",
    "for category, stats in seasonality_by_category.items():\n",
    "    if stats['total'] > 0:\n",
    "        pct = (stats['with_seasonality'] / stats['total']) * 100\n",
    "        print(f\"  {category}: {stats['with_seasonality']}/{stats['total']} ({pct:.1f}%) have seasonality\")\n",
    "\n",
    "# Show examples of series where seasonality boosted classification\n",
    "print(f\"\\nExamples where seasonality was detected:\")\n",
    "example_count = 0\n",
    "for result in evaluation_results:\n",
    "    if example_count >= 5:\n",
    "        break\n",
    "    stat_features = result.get('statistical_features', {})\n",
    "    has_weekly = stat_features.get('has_weekly_seasonality', False)\n",
    "    has_monthly = stat_features.get('has_monthly_seasonality', False)\n",
    "    has_yearly = stat_features.get('has_yearly_seasonality', False)\n",
    "    \n",
    "    if has_weekly or has_monthly or has_yearly:\n",
    "        series_info = result.get('series_info', {})\n",
    "        prophet_class = result.get('prophet_classification', 'Unknown')\n",
    "        prophet_reason = result.get('prophet_reason', '')\n",
    "        \n",
    "        seasonality_types = []\n",
    "        if has_weekly:\n",
    "            seasonality_types.append('weekly')\n",
    "        if has_monthly:\n",
    "            seasonality_types.append('monthly')\n",
    "        if has_yearly:\n",
    "            seasonality_types.append('yearly')\n",
    "        \n",
    "        print(f\"  - {series_info.get('metric_name', 'Unknown')}: {prophet_class}\")\n",
    "        print(f\"    Seasonality: {', '.join(seasonality_types)}\")\n",
    "        print(f\"    Reason: {prophet_reason}\")\n",
    "        example_count += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results and Summary\n",
    "\n",
    "Create summary tables and visualizations of the classification results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame with seasonality information\n",
    "summary_data = []\n",
    "for result in evaluation_results:\n",
    "    series_info = result['series_info']\n",
    "    stat_features = result.get('statistical_features', {})\n",
    "    prophet_metrics = result.get('prophet_metrics', {})\n",
    "    arima_metrics = result.get('arima_metrics', {})\n",
    "    \n",
    "    # Extract seasonality information\n",
    "    has_weekly = stat_features.get('has_weekly_seasonality', False)\n",
    "    has_monthly = stat_features.get('has_monthly_seasonality', False)\n",
    "    has_yearly = stat_features.get('has_yearly_seasonality', False)\n",
    "    \n",
    "    # Create seasonality summary string\n",
    "    seasonality_list = []\n",
    "    if has_weekly:\n",
    "        seasonality_list.append('weekly')\n",
    "    if has_monthly:\n",
    "        seasonality_list.append('monthly')\n",
    "    if has_yearly:\n",
    "        seasonality_list.append('yearly')\n",
    "    seasonality_detected = ', '.join(seasonality_list) if seasonality_list else 'None'\n",
    "    \n",
    "    summary_data.append({\n",
    "        'metric_name': series_info['metric_name'],\n",
    "        'labels': str(series_info.get('labels', {})),\n",
    "        'data_points': len(result.get('df', pd.DataFrame())),\n",
    "        'mean': stat_features.get('mean', np.nan),\n",
    "        'std': stat_features.get('std', np.nan),\n",
    "        'cv': stat_features.get('cv', np.nan),\n",
    "        'is_stationary_adf': stat_features.get('is_stationary_adf', False),\n",
    "        'trend_r2': stat_features.get('trend_r2', np.nan),\n",
    "        'has_weekly_seasonality': has_weekly,\n",
    "        'has_monthly_seasonality': has_monthly,\n",
    "        'has_yearly_seasonality': has_yearly,\n",
    "        'seasonality_detected': seasonality_detected,\n",
    "        'prophet_mape': prophet_metrics.get('mape', np.nan),\n",
    "        'prophet_mape_std': prophet_metrics.get('mape_std', np.nan),\n",
    "        'prophet_rmse': prophet_metrics.get('rmse', np.nan),\n",
    "        'prophet_rmse_std': prophet_metrics.get('rmse_std', np.nan),\n",
    "        'prophet_folds': prophet_metrics.get('folds_successful', 0),\n",
    "        'prophet_classification': result.get('prophet_classification', 'Unknown'),\n",
    "        'arima_mape': arima_metrics.get('mape', np.nan),\n",
    "        'arima_mape_std': arima_metrics.get('mape_std', np.nan),\n",
    "        'arima_rmse': arima_metrics.get('rmse', np.nan),\n",
    "        'arima_rmse_std': arima_metrics.get('rmse_std', np.nan),\n",
    "        'arima_folds': arima_metrics.get('folds_successful', 0),\n",
    "        'arima_classification': result.get('arima_classification', 'Unknown'),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display summary table in dataset viewer (not as text)\n",
    "print(\"Classification Summary Table:\")\n",
    "print(f\"Total series: {len(summary_df)}\")\n",
    "print(\"\\nUse the interactive dataset viewer below to explore, sort, and filter the data.\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Display the dataframe - this will show in Jupyter's dataset viewer\n",
    "from IPython.display import display\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of classifications as pie charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Color scheme for categories\n",
    "colors = {\n",
    "    'Predictable': '#2ecc71',      # Green\n",
    "    'Low Predictability': '#f39c12',      # Orange\n",
    "    'Not Suitable': '#e74c3c'            # Red\n",
    "}\n",
    "\n",
    "# Prophet classifications\n",
    "prophet_counts = summary_df['prophet_classification'].value_counts()\n",
    "prophet_colors = [colors.get(cat, '#95a5a6') for cat in prophet_counts.index]\n",
    "axes[0].pie(prophet_counts.values, labels=prophet_counts.index, autopct='%1.1f%%',\n",
    "            colors=prophet_colors, startangle=90, textprops={'fontsize': 11})\n",
    "axes[0].set_title('Prophet Classifications', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# ARIMA classifications\n",
    "arima_counts = summary_df['arima_classification'].value_counts()\n",
    "arima_colors = [colors.get(cat, '#95a5a6') for cat in arima_counts.index]\n",
    "axes[1].pie(arima_counts.values, labels=arima_counts.index, autopct='%1.1f%%',\n",
    "            colors=arima_colors, startangle=90, textprops={'fontsize': 11})\n",
    "axes[1].set_title('ARIMA Classifications', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: MAPE vs Stationarity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Prophet\n",
    "prophet_valid = summary_df[summary_df['prophet_mape'] < np.inf]\n",
    "if len(prophet_valid) > 0:\n",
    "    scatter = axes[0].scatter(\n",
    "        prophet_valid['is_stationary_adf'].astype(int),\n",
    "        prophet_valid['prophet_mape'],\n",
    "        c=prophet_valid['trend_r2'],\n",
    "        s=100,\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[0].set_xlabel('Stationary (ADF test: 0=No, 1=Yes)', fontsize=12)\n",
    "    axes[0].set_ylabel('Prophet MAPE (%)', fontsize=12)\n",
    "    axes[0].set_title('Prophet MAPE vs Stationarity (colored by Trend R²)', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0], label='Trend R²')\n",
    "\n",
    "# ARIMA\n",
    "arima_valid = summary_df[summary_df['arima_mape'] < np.inf]\n",
    "if len(arima_valid) > 0:\n",
    "    scatter = axes[1].scatter(\n",
    "        arima_valid['is_stationary_adf'].astype(int),\n",
    "        arima_valid['arima_mape'],\n",
    "        c=arima_valid['trend_r2'],\n",
    "        s=100,\n",
    "        alpha=0.6,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[1].set_xlabel('Stationary (ADF test: 0=No, 1=Yes)', fontsize=12)\n",
    "    axes[1].set_ylabel('ARIMA MAPE (%)', fontsize=12)\n",
    "    axes[1].set_title('ARIMA MAPE vs Stationarity (colored by Trend R²)', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[1], label='Trend R²')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison: Prophet vs ARIMA suitability\n",
    "comparison_data = []\n",
    "for result in evaluation_results:\n",
    "    comparison_data.append({\n",
    "        'metric_name': result['series_info']['metric_name'],\n",
    "        'prophet_class': result.get('prophet_classification', 'Unknown'),\n",
    "        'arima_class': result.get('arima_classification', 'Unknown'),\n",
    "        'prophet_mape': result.get('prophet_metrics', {}).get('mape', np.nan),\n",
    "        'arima_mape': result.get('arima_metrics', {}).get('mape', np.nan),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Filter valid data\n",
    "valid_comparison = comparison_df[\n",
    "    (comparison_df['prophet_mape'] < np.inf) & \n",
    "    (comparison_df['arima_mape'] < np.inf)\n",
    "]\n",
    "\n",
    "if len(valid_comparison) > 0:\n",
    "    ax.scatter(valid_comparison['prophet_mape'], valid_comparison['arima_mape'], \n",
    "               alpha=0.6, s=100)\n",
    "    \n",
    "    # Add diagonal line (y=x)\n",
    "    max_val = max(valid_comparison['prophet_mape'].max(), valid_comparison['arima_mape'].max())\n",
    "    ax.plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='Equal performance')\n",
    "    \n",
    "    ax.set_xlabel('Prophet MAPE (%)', fontsize=12)\n",
    "    ax.set_ylabel('ARIMA MAPE (%)', fontsize=12)\n",
    "    ax.set_title('Model Comparison: Prophet vs ARIMA MAPE', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    ax.text(0.95, 0.05, 'Prophet Better', transform=ax.transAxes, \n",
    "            ha='right', va='bottom', fontsize=10, style='italic')\n",
    "    ax.text(0.05, 0.95, 'ARIMA Better', transform=ax.transAxes, \n",
    "            ha='left', va='top', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Historical Data Grouped by Categories\n",
    "\n",
    "Visualize historical time series data grouped by their predictability classifications. This allows you to visually inspect the characteristics of series within each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prophet parameter recommendations for each category\n",
    "# This will be used when plotting to show recommended parameters alongside summary statistics\n",
    "# Note: This tries to use get_recommended_prophet_params if available (Section 9), \n",
    "# but has a complete inline fallback if the function isn't defined yet\n",
    "\n",
    "def generate_prophet_params_for_plotting(evaluation_results):\n",
    "    \"\"\"Generate Prophet parameter recommendations for each category.\n",
    "    \n",
    "    Returns a dictionary mapping category names to their recommended parameters.\n",
    "    \"\"\"\n",
    "    categories = ['Predictable', 'Low Predictability', 'Not Suitable']\n",
    "    prophet_params_dict = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        # Get all series in this category\n",
    "        category_series = [r for r in evaluation_results \n",
    "                          if r.get('prophet_classification') == category]\n",
    "        \n",
    "        if len(category_series) == 0:\n",
    "            # No series in this category, use defaults (inline version)\n",
    "            base_params = {\n",
    "                'yearly_seasonality': False,\n",
    "                'weekly_seasonality': False,\n",
    "                'daily_seasonality': False,\n",
    "                'seasonality_mode': 'additive',\n",
    "            }\n",
    "            if category == 'Predictable':\n",
    "                base_params.update({\n",
    "                    'changepoint_prior_scale': 0.5,\n",
    "                    'seasonality_prior_scale': 10.0,\n",
    "                    'holidays_prior_scale': 10.0,\n",
    "                    'mcmc_samples': 0,\n",
    "                    'interval_width': 0.80,\n",
    "                    'uncertainty_samples': 1000,\n",
    "                })\n",
    "            elif category == 'Low Predictability':\n",
    "                base_params.update({\n",
    "                    'changepoint_prior_scale': 0.05,\n",
    "                    'seasonality_prior_scale': 0.01,\n",
    "                    'holidays_prior_scale': 10.0,\n",
    "                    'mcmc_samples': 0,\n",
    "                    'interval_width': 0.95,\n",
    "                    'uncertainty_samples': 1000,\n",
    "                })\n",
    "            else:  # Not Suitable\n",
    "                base_params.update({\n",
    "                    'changepoint_prior_scale': 0.1,\n",
    "                    'seasonality_prior_scale': 0.01,\n",
    "                    'holidays_prior_scale': 10.0,\n",
    "                    'mcmc_samples': 0,\n",
    "                    'interval_width': 0.95,\n",
    "                    'uncertainty_samples': 1000,\n",
    "                })\n",
    "            prophet_params_dict[category] = base_params\n",
    "            continue\n",
    "        \n",
    "        # Analyze seasonality patterns in this category\n",
    "        weekly_count = sum(1 for r in category_series \n",
    "                          if r.get('statistical_features', {}).get('has_weekly_seasonality', False))\n",
    "        monthly_count = sum(1 for r in category_series \n",
    "                           if r.get('statistical_features', {}).get('has_monthly_seasonality', False))\n",
    "        yearly_count = sum(1 for r in category_series \n",
    "                          if r.get('statistical_features', {}).get('has_yearly_seasonality', False))\n",
    "        \n",
    "        # Determine if we should enable seasonality (if >=20% of series show it, or at least 1 series)\n",
    "        weekly_threshold = max(1, int(len(category_series) * 0.2))\n",
    "        monthly_threshold = max(1, int(len(category_series) * 0.2))\n",
    "        yearly_threshold = max(1, int(len(category_series) * 0.2))\n",
    "        \n",
    "        enable_weekly = weekly_count >= weekly_threshold\n",
    "        enable_monthly = monthly_count >= monthly_threshold\n",
    "        enable_yearly = yearly_count >= yearly_threshold\n",
    "        \n",
    "        # Verification: Ensure monthly is enabled if ANY series in category has it\n",
    "        # (This is a safeguard - the threshold should already handle this, but double-check)\n",
    "        if monthly_count > 0 and not enable_monthly:\n",
    "            # If we have monthly seasonality but threshold wasn't met, enable it anyway\n",
    "            # (This shouldn't happen with threshold=max(1, ...), but just in case)\n",
    "            enable_monthly = True\n",
    "        \n",
    "        # Get base parameters for this category (try to use function if available, otherwise inline)\n",
    "        try:\n",
    "            base_params = get_recommended_prophet_params(category).copy()\n",
    "            # Remove any existing add_seasonalities from the function result - we'll add our own\n",
    "            if 'add_seasonalities' in base_params:\n",
    "                del base_params['add_seasonalities']\n",
    "        except NameError:\n",
    "            # Function not defined yet, use inline version\n",
    "            base_params = {\n",
    "                'yearly_seasonality': False,  # Will be set later based on detection\n",
    "                'weekly_seasonality': False,  # Will be set later based on detection\n",
    "                'daily_seasonality': False,\n",
    "                'seasonality_mode': 'additive',\n",
    "            }\n",
    "            if category == 'Predictable':\n",
    "                base_params.update({\n",
    "                    'changepoint_prior_scale': 0.5,\n",
    "                    'seasonality_prior_scale': 10.0,\n",
    "                    'holidays_prior_scale': 10.0,\n",
    "                    'mcmc_samples': 0,\n",
    "                    'interval_width': 0.80,\n",
    "                    'uncertainty_samples': 1000,\n",
    "                })\n",
    "            elif category == 'Low Predictability':\n",
    "                base_params.update({\n",
    "                    'changepoint_prior_scale': 0.05,\n",
    "                    'seasonality_prior_scale': 0.01,\n",
    "                    'holidays_prior_scale': 10.0,\n",
    "                    'mcmc_samples': 0,\n",
    "                    'interval_width': 0.95,\n",
    "                    'uncertainty_samples': 1000,\n",
    "                })\n",
    "            else:  # Not Suitable\n",
    "                base_params.update({\n",
    "                    'changepoint_prior_scale': 0.1,\n",
    "                    'seasonality_prior_scale': 0.01,\n",
    "                    'holidays_prior_scale': 10.0,\n",
    "                    'mcmc_samples': 0,\n",
    "                    'interval_width': 0.95,\n",
    "                    'uncertainty_samples': 1000,\n",
    "                })\n",
    "        \n",
    "        # Add custom seasonalities - ALWAYS build this list from detected patterns\n",
    "        # IMPORTANT: Add monthly FIRST, then weekly, to ensure proper ordering\n",
    "        add_seasonalities = []\n",
    "        \n",
    "        # Add monthly seasonality if detected (ALWAYS add when detected)\n",
    "        # Monthly must be added via add_seasonalities (Prophet doesn't have built-in monthly)\n",
    "        if enable_monthly:\n",
    "            add_seasonalities.append({\n",
    "                'name': 'monthly',\n",
    "                'seasonal_periods': 30.5,\n",
    "                'fourier_order': 5\n",
    "            })\n",
    "        \n",
    "        # If weekly is detected, add it as custom seasonality\n",
    "        if enable_weekly:\n",
    "            add_seasonalities.append({\n",
    "                'name': 'weekly',\n",
    "                'seasonal_periods': 7.0,\n",
    "                'fourier_order': 3\n",
    "            })\n",
    "            # When using custom weekly seasonality, disable built-in weekly\n",
    "            base_params['weekly_seasonality'] = False\n",
    "        else:\n",
    "            # No weekly detected, use built-in if needed (but we're not enabling it here)\n",
    "            base_params['weekly_seasonality'] = False\n",
    "        \n",
    "        # Set yearly seasonality (use built-in, not custom)\n",
    "        base_params['yearly_seasonality'] = enable_yearly\n",
    "        \n",
    "        # ALWAYS add seasonalities to params dictionary if we have any\n",
    "        # This ensures monthly (and weekly if detected) are always included\n",
    "        # Even if only one is detected, the list should contain it\n",
    "        if add_seasonalities:\n",
    "            base_params['add_seasonalities'] = add_seasonalities\n",
    "        # If no custom seasonalities, ensure add_seasonalities is not in the dict\n",
    "        # (to avoid confusion, but this shouldn't happen if monthly/weekly are detected)\n",
    "        \n",
    "        prophet_params_dict[category] = base_params\n",
    "    \n",
    "    return prophet_params_dict\n",
    "\n",
    "# Generate parameters for plotting\n",
    "prophet_params_for_plotting = generate_prophet_params_for_plotting(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series_by_category(evaluation_results, model_type='prophet', max_series_per_plot=20, prophet_params=None):\n",
    "    \"\"\"Plot time series grouped by classification category.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: List of evaluation results\n",
    "        model_type: 'prophet' or 'arima'\n",
    "        max_series_per_plot: Maximum number of series to show per plot\n",
    "        prophet_params: Optional dict mapping category names to recommended Prophet parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group series by classification\n",
    "    categories = {\n",
    "        'Predictable': [],\n",
    "        'Low Predictability': [],\n",
    "        'Not Suitable': []\n",
    "    }\n",
    "    \n",
    "    for result in evaluation_results:\n",
    "        classification = result.get(f'{model_type}_classification', 'Not Suitable')\n",
    "        if classification in categories:\n",
    "            categories[classification].append(result)\n",
    "    \n",
    "    # Plot each category\n",
    "    for category_name, series_list in categories.items():\n",
    "        if len(series_list) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Limit number of series per plot\n",
    "        series_to_plot = series_list[:max_series_per_plot]\n",
    "        n_series = len(series_to_plot)\n",
    "        \n",
    "        if n_series == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        n_cols = min(4, n_series)\n",
    "        n_rows = (n_series + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "        \n",
    "        # Convert axes to a flat list of Axes objects\n",
    "        # plt.subplots can return: single Axes, 1D array, 2D array, or list\n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = [axes]\n",
    "        elif isinstance(axes, np.ndarray):\n",
    "            axes = axes.flatten().tolist()\n",
    "        elif not isinstance(axes, list):\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Set suptitle with padding to avoid overlap with subplot titles\n",
    "        fig.suptitle(f'{model_type.upper()} - {category_name} ({len(series_list)} total, showing {n_series})', \n",
    "                     fontsize=16, fontweight='bold', y=1)\n",
    "        \n",
    "        for idx, result in enumerate(series_to_plot):\n",
    "            ax = axes[idx]\n",
    "            df = result.get('df', pd.DataFrame())\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('No data', fontsize=10)\n",
    "                continue\n",
    "            \n",
    "            # Plot time series\n",
    "            ax.plot(df['ds'], df['y'], 'b-', linewidth=1.5, alpha=0.7)\n",
    "            \n",
    "            # Title with key info\n",
    "            series_info = result['series_info']\n",
    "            title = f\"{series_info['metric_name']}\"\n",
    "            if series_info.get('labels'):\n",
    "                # Show first few labels\n",
    "                label_str = str(series_info['labels'])[:50]\n",
    "                if len(str(series_info['labels'])) > 50:\n",
    "                    label_str += '...'\n",
    "                title += f\"\\n{label_str}\"\n",
    "            \n",
    "            # Add metrics to title\n",
    "            metrics = result.get(f'{model_type}_metrics', {})\n",
    "            if metrics.get('success', False):\n",
    "                mape_val = metrics.get('mape', np.nan)\n",
    "                rmse_val = metrics.get('rmse', np.nan)\n",
    "                title += f\"\\nMAPE: {mape_val:.2f}%, RMSE: {rmse_val:.2f}\"\n",
    "            \n",
    "            ax.set_title(title, fontsize=9)\n",
    "            ax.set_xlabel('Date', fontsize=8)\n",
    "            ax.set_ylabel('Value', fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.tick_params(labelsize=7)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(n_series, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        # Adjust layout to prevent suptitle from overlapping with subplot titles\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics for this category\n",
    "        if len(series_list) > 0:\n",
    "            print(f\"\\n{category_name} Summary Statistics:\")\n",
    "            mape_values = []\n",
    "            rmse_values = []\n",
    "            weekly_count = 0\n",
    "            monthly_count = 0\n",
    "            yearly_count = 0\n",
    "            \n",
    "            for result in series_list:\n",
    "                metrics = result.get(f'{model_type}_metrics', {})\n",
    "                if metrics.get('success', False):\n",
    "                    mape_val = metrics.get('mape', np.nan)\n",
    "                    rmse_val = metrics.get('rmse', np.nan)\n",
    "                    if mape_val < np.inf:\n",
    "                        mape_values.append(mape_val)\n",
    "                    if rmse_val < np.inf:\n",
    "                        rmse_values.append(rmse_val)\n",
    "                \n",
    "                # Count seasonality patterns\n",
    "                stat_features = result.get('statistical_features', {})\n",
    "                if stat_features.get('has_weekly_seasonality', False):\n",
    "                    weekly_count += 1\n",
    "                if stat_features.get('has_monthly_seasonality', False):\n",
    "                    monthly_count += 1\n",
    "                if stat_features.get('has_yearly_seasonality', False):\n",
    "                    yearly_count += 1\n",
    "            \n",
    "            if mape_values:\n",
    "                print(f\"  Average MAPE: {np.mean(mape_values):.2f}%\")\n",
    "                print(f\"  Median MAPE: {np.median(mape_values):.2f}%\")\n",
    "            if rmse_values:\n",
    "                print(f\"  Average RMSE: {np.mean(rmse_values):.2f}\")\n",
    "                print(f\"  Median RMSE: {np.median(rmse_values):.2f}\")\n",
    "            print(f\"  Total series: {len(series_list)}\")\n",
    "            \n",
    "            # Seasonality statistics\n",
    "            print(f\"  Seasonality Detection:\")\n",
    "            print(f\"    Weekly: {weekly_count}/{len(series_list)} series ({weekly_count/len(series_list)*100:.1f}%)\")\n",
    "            print(f\"    Monthly: {monthly_count}/{len(series_list)} series ({monthly_count/len(series_list)*100:.1f}%)\")\n",
    "            print(f\"    Yearly: {yearly_count}/{len(series_list)} series ({yearly_count/len(series_list)*100:.1f}%)\")\n",
    "            \n",
    "            # Print recommended Prophet parameters if available\n",
    "            if model_type == 'prophet' and prophet_params is not None and category_name in prophet_params:\n",
    "                params = prophet_params[category_name]\n",
    "                if params and isinstance(params, dict):\n",
    "                    print(f\"\\n  Recommended Prophet Parameters:\")\n",
    "                    print(f\"    {{\")\n",
    "                    # Sort keys but handle add_seasonalities specially to ensure all are printed\n",
    "                    sorted_keys = sorted([k for k in params.keys() if k != 'add_seasonalities'])\n",
    "                    # Print add_seasonalities first if it exists (to make it more visible)\n",
    "                    if 'add_seasonalities' in params and params['add_seasonalities']:\n",
    "                        print(f\"      'add_seasonalities': [\")\n",
    "                        # Ensure we iterate through ALL seasonalities in the list\n",
    "                        for seasonality in params['add_seasonalities']:\n",
    "                            if isinstance(seasonality, dict) and 'name' in seasonality:\n",
    "                                print(f\"          {{\")\n",
    "                                print(f\"              'name': '{seasonality['name']}',\")\n",
    "                                print(f\"              'seasonal_periods': {seasonality['seasonal_periods']},\")\n",
    "                                print(f\"              'fourier_order': {seasonality['fourier_order']}\")\n",
    "                                print(f\"          }},\")\n",
    "                        print(f\"      ],\")\n",
    "                    # Print other parameters\n",
    "                    for key in sorted_keys:\n",
    "                        value = params[key]\n",
    "                        if isinstance(value, bool):\n",
    "                            print(f\"      '{key}': {value},\")\n",
    "                        elif isinstance(value, str):\n",
    "                            print(f\"      '{key}': '{value}',\")\n",
    "                        else:\n",
    "                            print(f\"      '{key}': {value},\")\n",
    "                    print(f\"    }}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prophet classifications (with recommended parameters shown in summary)\n",
    "print(\"=\"*60)\n",
    "print(\"PROPHET CLASSIFICATIONS - Historical Data by Category\")\n",
    "print(\"=\"*60)\n",
    "plot_series_by_category(evaluation_results, model_type='prophet', max_series_per_plot=MAX_SERIES_PER_PLOT, \n",
    "                        prophet_params=prophet_params_for_plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARIMA classifications\n",
    "print(\"=\"*60)\n",
    "print(\"ARIMA CLASSIFICATIONS - Historical Data by Category\")\n",
    "print(\"=\"*60)\n",
    "plot_series_by_category(evaluation_results, model_type='arima', max_series_per_plot=MAX_SERIES_PER_PLOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prophet Parameter Helper Function\n",
    "\n",
    "Helper function for generating Prophet parameters (used internally by plotting function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_prophet_params(category, stat_features=None):\n",
    "    \"\"\"Get recommended Prophet parameters based on category and detected seasonality.\n",
    "    \n",
    "    Args:\n",
    "        category: Predictability category ('Predictable', 'Low Predictability', etc.)\n",
    "        stat_features: Dictionary of statistical features including seasonality detection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detect seasonality from features if provided\n",
    "    has_weekly = False\n",
    "    has_monthly = False\n",
    "    has_yearly = False\n",
    "    \n",
    "    if stat_features:\n",
    "        has_weekly = stat_features.get('has_weekly_seasonality', False)\n",
    "        has_monthly = stat_features.get('has_monthly_seasonality', False)\n",
    "        has_yearly = stat_features.get('has_yearly_seasonality', False)\n",
    "    \n",
    "    # Base parameters - enable seasonality based on detection\n",
    "    base_params = {\n",
    "        'yearly_seasonality': has_yearly,  # Enable if detected\n",
    "        'weekly_seasonality': has_weekly,  # Enable if detected\n",
    "        'daily_seasonality': False,  # Usually not needed for daily aggregated data\n",
    "        'seasonality_mode': 'additive',\n",
    "    }\n",
    "    \n",
    "    # Category-specific recommendations\n",
    "    if category == 'Predictable':\n",
    "        # Predictable series: balanced flexibility (combines former Highly + Moderately Predictable)\n",
    "        # Use moderate flexibility to handle both strong and moderate patterns\n",
    "        return {\n",
    "            **base_params,\n",
    "            'changepoint_prior_scale': 0.5,  # Moderate flexibility\n",
    "            'seasonality_prior_scale': 10.0,  # Strong seasonality if present\n",
    "            'holidays_prior_scale': 10.0,\n",
    "            'mcmc_samples': 0,  # No MCMC (faster, deterministic)\n",
    "            'interval_width': 0.80,\n",
    "            'uncertainty_samples': 1000,\n",
    "        }\n",
    "    \n",
    "    elif category == 'Low Predictability':\n",
    "        # Low predictability: more flexible model to capture complex patterns\n",
    "        # Lower changepoint_prior_scale = more changepoints = more flexible\n",
    "        return {\n",
    "            **base_params,\n",
    "            'changepoint_prior_scale': 0.05,  # More flexible, more changepoints\n",
    "            'seasonality_prior_scale': 0.01,  # Allow seasonality to adapt more\n",
    "            'holidays_prior_scale': 10.0,\n",
    "            'mcmc_samples': 0,\n",
    "            'interval_width': 0.95,  # Wider intervals (less confident)\n",
    "            'uncertainty_samples': 1000,\n",
    "        }\n",
    "    \n",
    "    else:  # Not Suitable\n",
    "        # For not suitable series, provide conservative defaults\n",
    "        # but note that forecasting may not be recommended\n",
    "        return {\n",
    "            **base_params,\n",
    "            'changepoint_prior_scale': 0.1,  # Very flexible\n",
    "            'seasonality_prior_scale': 0.01,\n",
    "            'holidays_prior_scale': 10.0,\n",
    "            'mcmc_samples': 0,\n",
    "            'interval_width': 0.95,\n",
    "            'uncertainty_samples': 1000,\n",
    "        }\n",
    "\n",
    "# Helper function for generating Prophet parameters\n",
    "# Note: Parameters are displayed when plotting by category (Section 8)\n",
    "# This function is kept for reference and potential future use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Final summary of the analysis with recommendations for model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal Series Analyzed: {len(evaluation_results)}\")\n",
    "print(f\"Query Selector: {SELECTOR}\")\n",
    "print(f\"History Period: {HISTORY_DAYS} days\")\n",
    "\n",
    "# Prophet summary\n",
    "prophet_predictable = sum(1 for r in evaluation_results if r.get('prophet_classification') == 'Predictable')\n",
    "prophet_low = sum(1 for r in evaluation_results if r.get('prophet_classification') == 'Low Predictability')\n",
    "prophet_not = sum(1 for r in evaluation_results if r.get('prophet_classification') == 'Not Suitable')\n",
    "\n",
    "print(f\"\\nProphet Model Suitability:\")\n",
    "print(f\"  Predictable: {prophet_predictable} ({prophet_predictable/len(evaluation_results)*100:.1f}%)\")\n",
    "print(f\"  Low Predictability: {prophet_low} ({prophet_low/len(evaluation_results)*100:.1f}%)\")\n",
    "print(f\"  Not Suitable: {prophet_not} ({prophet_not/len(evaluation_results)*100:.1f}%)\")\n",
    "\n",
    "# ARIMA summary\n",
    "arima_predictable = sum(1 for r in evaluation_results if r.get('arima_classification') == 'Predictable')\n",
    "arima_low = sum(1 for r in evaluation_results if r.get('arima_classification') == 'Low Predictability')\n",
    "arima_not = sum(1 for r in evaluation_results if r.get('arima_classification') == 'Not Suitable')\n",
    "\n",
    "print(f\"\\nARIMA Model Suitability:\")\n",
    "print(f\"  Predictable: {arima_predictable} ({arima_predictable/len(evaluation_results)*100:.1f}%)\")\n",
    "print(f\"  Low Predictability: {arima_low} ({arima_low/len(evaluation_results)*100:.1f}%)\")\n",
    "print(f\"  Not Suitable: {arima_not} ({arima_not/len(evaluation_results)*100:.1f}%)\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"  - Use Prophet for series classified as 'Predictable'\")\n",
    "print(f\"  - Use ARIMA for series where ARIMA shows better performance (lower MAPE)\")\n",
    "print(f\"  - Consider alternative approaches for 'Not Suitable' series\")\n",
    "print(f\"  - Review 'Low Predictability' series - may need feature engineering or different models\")\n",
    "\n",
    "# Find series where one model is clearly better\n",
    "better_prophet = 0\n",
    "better_arima = 0\n",
    "equal = 0\n",
    "\n",
    "for result in evaluation_results:\n",
    "    prophet_metrics = result.get('prophet_metrics', {})\n",
    "    arima_metrics = result.get('arima_metrics', {})\n",
    "    \n",
    "    if prophet_metrics.get('success', False) and arima_metrics.get('success', False):\n",
    "        prophet_mape = prophet_metrics.get('mape', np.inf)\n",
    "        arima_mape = arima_metrics.get('mape', np.inf)\n",
    "        \n",
    "        if prophet_mape < np.inf and arima_mape < np.inf:\n",
    "            diff = abs(prophet_mape - arima_mape)\n",
    "            if diff < 1.0:  # Within 1% - consider equal\n",
    "                equal += 1\n",
    "            elif prophet_mape < arima_mape:\n",
    "                better_prophet += 1\n",
    "            else:\n",
    "                better_arima += 1\n",
    "\n",
    "print(f\"\\nModel Comparison (where both models succeeded):\")\n",
    "print(f\"  Prophet performs better: {better_prophet}\")\n",
    "print(f\"  ARIMA performs better: {better_arima}\")\n",
    "print(f\"  Similar performance: {equal}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
