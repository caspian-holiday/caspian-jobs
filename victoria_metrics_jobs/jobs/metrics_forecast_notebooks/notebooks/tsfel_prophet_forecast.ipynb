{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TSFEL-Prophet Forecast Notebook\n",
        "\n",
        "This notebook combines TSFEL feature extraction with adaptive Prophet forecasting:\n",
        "\n",
        "1. Query time series from Victoria Metrics using a PromQL selector\n",
        "2. Iterate over each time series (memory-efficient processing)\n",
        "3. Extract TSFEL features and classify predictability\n",
        "4. Generate Prophet forecasts with adaptive parameters based on TSFEL features\n",
        "5. Save forecasts to database\n",
        "\n",
        "**Classification-Based Forecasting:**\n",
        "- **Predictable**: Prophet with seasonality enabled based on detected weekly/monthly patterns\n",
        "- **Low Predictability**: Prophet with no seasonality (trend-only regression)\n",
        "- **Not Suitable**: Skipped (insufficient data or poor quality)\n",
        "\n",
        "**Key Features:**\n",
        "- Memory-efficient: processes one series at a time\n",
        "- Adaptive parameters: Prophet settings tuned based on TSFEL features\n",
        "- Automatic seasonality detection: weekly and monthly patterns\n",
        "- Database integration: saves forecasts with parameter tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parameters cell (tagged for papermill injection)\n",
        "# These will be injected by papermill when executed by the job\n",
        "# When running locally, use environment variables instead\n",
        "# Note: selector, history_days, forecast parameters, and model parameters\n",
        "# are hardcoded in the notebook cells where they are used (NOT passed as parameters)\n",
        "# Database configuration is loaded from YAML config file using VM_JOBS_ENVIRONMENT and VM_JOBS_DB_PASSWORD\n",
        "# output_results_path: if set, notebook writes a JSON file with timeseries_processed and timeseries_failed for the job to read\n",
        "# vm_jobs_config_path: path to YAML config; passed by job when run as service, used by create_database_connection(..., config_path=...)\n",
        "vm_query_url = ''\n",
        "vm_token = ''\n",
        "vm_jobs_environment = ''\n",
        "vm_jobs_config_path = ''\n",
        "dry_run = True\n",
        "output_results_path = ''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "**Connection settings:** Set via papermill parameters or environment variables.\n",
        "**Forecasting parameters:** Will be defined in the model training section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Connection settings: from papermill parameters or environment variables\n",
        "# Database configuration is loaded automatically from YAML config using VM_JOBS_ENVIRONMENT and VM_JOBS_DB_PASSWORD\n",
        "import os\n",
        "\n",
        "# Victoria Metrics connection - from parameters/env vars (prefer parameters)\n",
        "# Parameters are now individual variables (not a dict) due to papermill requirements\n",
        "VM_QUERY_URL = vm_query_url if vm_query_url else os.getenv('VM_QUERY_URL', 'http://victoria-metrics:8428')\n",
        "VM_TOKEN = vm_token if vm_token else os.getenv('VM_TOKEN', '')\n",
        "\n",
        "# Environment - from parameters/env vars (prefer parameters)\n",
        "VM_JOBS_ENVIRONMENT = vm_jobs_environment if vm_jobs_environment else os.getenv('VM_JOBS_ENVIRONMENT', '')\n",
        "# Config file path - from papermill parameter (when run as service) or environment variable\n",
        "VM_JOBS_CONFIG_PATH = vm_jobs_config_path if vm_jobs_config_path else os.getenv('VM_JOBS_CONFIG_PATH', '')\n",
        "\n",
        "# Dry run mode: when True, don't save to database (plotting always happens)\n",
        "# Handle both bool and string types from papermill\n",
        "if isinstance(dry_run, bool):\n",
        "    DRY_RUN = dry_run\n",
        "elif isinstance(dry_run, str):\n",
        "    DRY_RUN = dry_run.lower() in ('true', '1', 'yes')\n",
        "else:\n",
        "    DRY_RUN = os.getenv('DRY_RUN', 'false').lower() in ('true', '1', 'yes')\n",
        "\n",
        "print(f\"VM Query URL: {VM_QUERY_URL}\")\n",
        "print(f\"VM_JOBS_ENVIRONMENT: {VM_JOBS_ENVIRONMENT or 'NOT SET (will use env var)'}\")\n",
        "print(f\"Dry Run Mode: {DRY_RUN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add current directory to Python path\n",
        "current_dir = str(Path.cwd())\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, timezone, date\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Plotting libraries (for plotting)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TSFEL imports for time series feature extraction\n",
        "import tsfel\n",
        "\n",
        "# Darts imports\n",
        "from darts import TimeSeries\n",
        "from darts.models import Prophet as DartsProphet\n",
        "\n",
        "# Helper modules\n",
        "from prometheus_api_client import PrometheusConnect\n",
        "from database_helpers import (\n",
        "    check_forecast_metadata_table_exists,\n",
        "    create_database_connection,\n",
        "    create_forecast_run_record,\n",
        "    save_forecast_metadata_for_metric_by_id,\n",
        "    save_forecasts_to_database,\n",
        "    update_forecast_run_record,\n",
        ")\n",
        "\n",
        "# Set plot style (plotting always enabled)\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "print(\"Plotting enabled\")\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Connect to Victoria Metrics and Query Data\n",
        "\n",
        "**Configure your selector and history days in the cell below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PromQL selector - EDIT THIS\n",
        "SELECTOR = '{job=\"extractor\"}'  # Your PromQL selector\n",
        "\n",
        "# History parameter - EDIT THIS\n",
        "HISTORY_DAYS = 365  # Days of history to fetch\n",
        "\n",
        "# Forecast parameters\n",
        "FORECAST_HORIZON_DAYS = 90  # Business days to forecast ahead\n",
        "MIN_HISTORY_POINTS = 60  # Minimum for TSFEL feature extraction and seasonality detection\n",
        "\n",
        "# TSFEL-specific parameters\n",
        "SEASONALITY_LAG_WEEKLY = 7\n",
        "SEASONALITY_LAG_MONTHLY = 30\n",
        "SEASONALITY_LAG_QUARTERLY = 90\n",
        "ACF_THRESHOLD = 0.3  # Threshold for seasonality detection\n",
        "\n",
        "# Floor parameter: set to 0 to prevent negative forecasts\n",
        "FORECAST_FLOOR = 0  # Minimum value for forecasts (set to None to disable)\n",
        "\n",
        "# Connect to Victoria Metrics and query historical data\n",
        "headers = {\"Authorization\": f\"Bearer {VM_TOKEN}\"} if VM_TOKEN else {}\n",
        "prom = PrometheusConnect(url=VM_QUERY_URL, headers=headers, disable_ssl=True)\n",
        "print(f\"Connected to Victoria Metrics at {VM_QUERY_URL}\")\n",
        "\n",
        "print(f\"\\nQuerying: {SELECTOR}\")\n",
        "end_date = datetime.now(timezone.utc)\n",
        "start_date = end_date - timedelta(days=HISTORY_DAYS)\n",
        "query_result = prom.custom_query_range(\n",
        "    query=SELECTOR.replace(\"'\", '\"'),  # Ensure double quotes for PromQL\n",
        "    start_time=start_date,\n",
        "    end_time=end_date,\n",
        "    step=\"24h\"\n",
        ")\n",
        "\n",
        "print(f\"Query range: {start_date.date()} to {end_date.date()}\")\n",
        "print(f\"Query returned {len(query_result)} series\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TSFEL Feature Extraction Functions\n",
        "\n",
        "This section contains functions for time series feature extraction using TSFEL library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regularize_time_intervals(df, freq='D', method='ffill'):\n",
        "    \"\"\"Regularize time intervals in a time series DataFrame.\n",
        "    \n",
        "    Creates a regular date range and fills missing values using statistical methods.\n",
        "    This helps ensure consistent time intervals for feature extraction.\n",
        "    \n",
        "    Args:\n",
        "        df: pandas DataFrame with 'ds' (datetime) and 'y' (value) columns\n",
        "        freq: Frequency string for regular intervals (default 'D' for daily)\n",
        "        method: Fill method - 'ffill' (forward fill), 'bfill' (backward fill), \n",
        "                'interpolate' (linear interpolation), or 'mean' (fill with mean)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with regular time intervals\n",
        "    \"\"\"\n",
        "    if len(df) < 2:\n",
        "        return df\n",
        "    \n",
        "    # Ensure 'ds' is datetime\n",
        "    df = df.copy()\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    df = df.sort_values('ds').reset_index(drop=True)\n",
        "    \n",
        "    # Create regular date range from first to last date\n",
        "    start_date = df['ds'].min()\n",
        "    end_date = df['ds'].max()\n",
        "    regular_range = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
        "    \n",
        "    # Set 'ds' as index and reindex to regular range\n",
        "    df_indexed = df.set_index('ds')[['y']]\n",
        "    df_regular = df_indexed.reindex(regular_range)\n",
        "    \n",
        "    # Fill missing values based on method\n",
        "    if method == 'ffill':\n",
        "        # Forward fill (carry last known value forward)\n",
        "        df_regular['y'] = df_regular['y'].ffill()\n",
        "        # Backward fill any remaining NaNs at the start\n",
        "        df_regular['y'] = df_regular['y'].bfill()\n",
        "    elif method == 'bfill':\n",
        "        # Backward fill (carry next known value backward)\n",
        "        df_regular['y'] = df_regular['y'].bfill()\n",
        "        # Forward fill any remaining NaNs at the end\n",
        "        df_regular['y'] = df_regular['y'].ffill()\n",
        "    elif method == 'interpolate':\n",
        "        # Linear interpolation\n",
        "        df_regular['y'] = df_regular['y'].interpolate(method='linear')\n",
        "        # Fill any remaining NaNs at edges\n",
        "        df_regular['y'] = df_regular['y'].ffill().bfill()\n",
        "    elif method == 'mean':\n",
        "        # Fill with mean value\n",
        "        mean_val = df['y'].mean()\n",
        "        df_regular['y'] = df_regular['y'].fillna(mean_val)\n",
        "    else:\n",
        "        # Default: forward fill + backward fill\n",
        "        df_regular['y'] = df_regular['y'].ffill().bfill()\n",
        "    \n",
        "    # Reset index to get 'ds' column back\n",
        "    df_regular = df_regular.reset_index()\n",
        "    df_regular = df_regular.rename(columns={'index': 'ds'})\n",
        "    \n",
        "    # Remove any remaining NaN values (shouldn't happen, but safety check)\n",
        "    df_regular = df_regular.dropna()\n",
        "    \n",
        "    return df_regular\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to convert values to scalars (not Series/arrays)\n",
        "def to_scalar(val):\n",
        "    \"\"\"Convert value to scalar if it's a Series or array.\"\"\"\n",
        "    if isinstance(val, pd.Series):\n",
        "        return val.iloc[0] if len(val) > 0 else np.nan\n",
        "    elif isinstance(val, np.ndarray):\n",
        "        return val[0] if len(val) > 0 else np.nan\n",
        "    elif isinstance(val, (list, tuple)):\n",
        "        return val[0] if len(val) > 0 else np.nan\n",
        "    return val\n",
        "\n",
        "\n",
        "def feature_cv(signal):\n",
        "    \"\"\"Coefficient of Variation: std / mean\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) < 2:\n",
        "            return np.nan\n",
        "        mean_val = np.mean(signal)\n",
        "        if mean_val == 0:\n",
        "            return np.inf\n",
        "        std_val = np.std(signal)\n",
        "        return abs(std_val) / abs(mean_val) if std_val != 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def feature_acf1(signal):\n",
        "    \"\"\"Lag-1 autocorrelation coefficient (ACF1)\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) < 2:\n",
        "            return np.nan\n",
        "        if np.var(signal) == 0:\n",
        "            return np.nan\n",
        "        acf1_val = np.corrcoef(signal[:-1], signal[1:])[0, 1]\n",
        "        return 0.0 if np.isnan(acf1_val) else acf1_val\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def feature_stability(signal):\n",
        "    \"\"\"Stability: inverse of normalized Mean Absolute Deviation (MAD)\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) < 2:\n",
        "            return np.nan\n",
        "        \n",
        "        mean_val = np.mean(signal)\n",
        "        std_val = np.std(signal)\n",
        "        mad_val = np.mean(np.abs(signal - mean_val))\n",
        "        \n",
        "        if np.isnan(mad_val) or np.isnan(mean_val):\n",
        "            return np.nan\n",
        "        \n",
        "        if abs(mean_val) > 0:\n",
        "            normalized_mad = mad_val / abs(mean_val)\n",
        "            return 1.0 / (1.0 + normalized_mad) if normalized_mad > 0 else 1.0\n",
        "        else:\n",
        "            # If mean is near zero, use MAD relative to std\n",
        "            if not np.isnan(std_val) and std_val > 0:\n",
        "                normalized_mad = mad_val / std_val\n",
        "                return 1.0 / (1.0 + normalized_mad) if normalized_mad > 0 else 1.0\n",
        "            else:\n",
        "                return np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def feature_weekly_autocorr(signal, lag=7, threshold=0.3):\n",
        "    \"\"\"Weekly seasonality autocorrelation at lag 7\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) <= lag:\n",
        "            return 0.0\n",
        "        autocorr = np.corrcoef(signal[:-lag], signal[lag:])[0, 1]\n",
        "        return 0.0 if np.isnan(autocorr) else autocorr\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def feature_monthly_autocorr(signal, lag=30, threshold=0.3):\n",
        "    \"\"\"Monthly seasonality autocorrelation at lag 30\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) <= lag:\n",
        "            return 0.0\n",
        "        autocorr = np.corrcoef(signal[:-lag], signal[lag:])[0, 1]\n",
        "        return 0.0 if np.isnan(autocorr) else autocorr\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def feature_quarterly_autocorr(signal, lag=90, threshold=0.3):\n",
        "    \"\"\"Quarterly seasonality autocorrelation at lag 90\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) <= lag:\n",
        "            return 0.0\n",
        "        autocorr = np.corrcoef(signal[:-lag], signal[lag:])[0, 1]\n",
        "        return 0.0 if np.isnan(autocorr) else autocorr\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def feature_outlier_ratio(signal):\n",
        "    \"\"\"IQR-based outlier ratio\"\"\"\n",
        "    try:\n",
        "        signal = np.array(signal)\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        if len(signal) < 10:\n",
        "            return 0.0\n",
        "        \n",
        "        Q1 = np.percentile(signal, 25)\n",
        "        Q3 = np.percentile(signal, 75)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = (signal < lower_bound) | (signal > upper_bound)\n",
        "        return np.sum(outliers) / len(signal)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def feature_changepoint_count_and_dates(df):\n",
        "    \"\"\"Variance-based changepoint detection that returns both count and dates.\n",
        "    \n",
        "    Args:\n",
        "        df: pandas DataFrame with 'ds' (datetime) and 'y' (value) columns\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (changepoint_count, changepoint_dates_list)\n",
        "        changepoint_count: Integer count of changepoints\n",
        "        changepoint_dates_list: List of date objects where changepoints occur\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if len(df) < 10:\n",
        "            return 0, []\n",
        "        \n",
        "        # Ensure 'ds' is datetime and sort\n",
        "        df = df.copy()\n",
        "        df['ds'] = pd.to_datetime(df['ds'])\n",
        "        df = df.sort_values('ds').reset_index(drop=True)\n",
        "        \n",
        "        # Filter out NaN and Inf values while keeping dates aligned\n",
        "        # Create a mask for valid values\n",
        "        valid_mask = ~(df['y'].isna() | np.isinf(df['y']))\n",
        "        df_clean = df[valid_mask].reset_index(drop=True)\n",
        "        \n",
        "        if len(df_clean) < 10:\n",
        "            return 0, []\n",
        "        \n",
        "        # Get aligned values and dates\n",
        "        values = df_clean['y'].values\n",
        "        dates = df_clean['ds'].values\n",
        "        \n",
        "        window_size = min(20, len(values) // 4)\n",
        "        if window_size < 5:\n",
        "            return 0, []\n",
        "        \n",
        "        # Calculate rolling variance with center=True\n",
        "        # This means the variance at index i is calculated from values[i-window_size//2 : i+window_size//2+1]\n",
        "        # When center=True, the first window_size//2 and last window_size//2 values will be NaN\n",
        "        rolling_var = pd.Series(values).rolling(window=window_size, center=True).var()\n",
        "        var_mean = rolling_var.mean()\n",
        "        var_std = rolling_var.std()\n",
        "        threshold = var_mean + 2 * var_std if not np.isnan(var_std) and var_std > 0 else var_mean * 2\n",
        "        \n",
        "        # Create mask for changepoints (high variance regions)\n",
        "        changepoints_mask = (rolling_var > threshold) & (~np.isnan(rolling_var))\n",
        "        \n",
        "        changepoint_dates = []\n",
        "        in_changepoint = False\n",
        "        \n",
        "        \n",
        "        for idx, is_cp in enumerate(changepoints_mask):\n",
        "            if is_cp and not in_changepoint:\n",
        "                # Find the actual changepoint by looking at value changes\n",
        "                # within the high variance region, not just the detection index\n",
        "                # Look ahead in the high variance region to find where values actually change\n",
        "                changepoint_idx = idx\n",
        "                \n",
        "                # Look ahead up to window_size positions to find the actual change\n",
        "                look_ahead = min(window_size, len(values) - idx - 1)\n",
        "                max_change_idx = idx\n",
        "                max_change = 0\n",
        "                \n",
        "                for j in range(1, look_ahead + 1):\n",
        "                    if idx + j >= len(values):\n",
        "                        break\n",
        "                    # Calculate change magnitude\n",
        "                    if idx + j - 1 >= 0:\n",
        "                        change = abs(values[idx + j] - values[idx + j - 1])\n",
        "                        if change > max_change:\n",
        "                            max_change = change\n",
        "                            max_change_idx = idx + j\n",
        "                \n",
        "                # Use the index where the largest change occurred\n",
        "                changepoint_idx = max_change_idx\n",
        "                \n",
        "                if changepoint_idx < len(dates):\n",
        "                    changepoint_date = pd.Timestamp(dates[changepoint_idx])\n",
        "                    changepoint_date_date = changepoint_date.date()\n",
        "                    \n",
        "                    # Just store the changepoint date (previous_date calculated later)\n",
        "                    changepoint_dates.append(changepoint_date_date)\n",
        "                in_changepoint = True\n",
        "            elif not is_cp:\n",
        "                in_changepoint = False\n",
        "        \n",
        "        return len(changepoint_dates), changepoint_dates\n",
        "    except Exception:\n",
        "        return 0, []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_personalized_features_config():\n",
        "    \"\"\"Create TSFEL personalized features configuration dictionary\"\"\"\n",
        "    personalized_features = {\n",
        "        'CV': {\n",
        "            'function': feature_cv,\n",
        "            'name': 'Coefficient of Variation',\n",
        "            'domain': 'statistical',\n",
        "            'parameters': {}\n",
        "        },\n",
        "        'ACF1': {\n",
        "            'function': feature_acf1,\n",
        "            'name': 'Lag-1 Autocorrelation',\n",
        "            'domain': 'temporal',\n",
        "            'parameters': {}\n",
        "        },\n",
        "        'Stability': {\n",
        "            'function': feature_stability,\n",
        "            'name': 'Stability (inverse normalized MAD)',\n",
        "            'domain': 'statistical',\n",
        "            'parameters': {}\n",
        "        },\n",
        "        'Weekly Autocorr': {\n",
        "            'function': lambda signal: feature_weekly_autocorr(signal, lag=SEASONALITY_LAG_WEEKLY, threshold=ACF_THRESHOLD),\n",
        "            'name': 'Weekly Seasonality Autocorrelation',\n",
        "            'domain': 'temporal',\n",
        "            'parameters': {'lag': SEASONALITY_LAG_WEEKLY, 'threshold': ACF_THRESHOLD}\n",
        "        },\n",
        "        'Monthly Autocorr': {\n",
        "            'function': lambda signal: feature_monthly_autocorr(signal, lag=SEASONALITY_LAG_MONTHLY, threshold=ACF_THRESHOLD),\n",
        "            'name': 'Monthly Seasonality Autocorrelation',\n",
        "            'domain': 'temporal',\n",
        "            'parameters': {'lag': SEASONALITY_LAG_MONTHLY, 'threshold': ACF_THRESHOLD}\n",
        "        },\n",
        "        'Quarterly Autocorr': {\n",
        "            'function': lambda signal: feature_quarterly_autocorr(signal, lag=SEASONALITY_LAG_QUARTERLY, threshold=ACF_THRESHOLD),\n",
        "            'name': 'Quarterly Seasonality Autocorrelation',\n",
        "            'domain': 'temporal',\n",
        "            'parameters': {'lag': SEASONALITY_LAG_QUARTERLY, 'threshold': ACF_THRESHOLD}\n",
        "        },\n",
        "        'Outlier Ratio': {\n",
        "            'function': feature_outlier_ratio,\n",
        "            'name': 'IQR-based Outlier Ratio',\n",
        "            'domain': 'statistical',\n",
        "            'parameters': {}\n",
        "        },\n",
        "        'Changepoint Count': {\n",
        "            'function': feature_changepoint_count_and_dates,\n",
        "            'name': 'Variance-based Changepoint Count and Dates',\n",
        "            'domain': 'temporal',\n",
        "            'parameters': {}\n",
        "        }\n",
        "    }\n",
        "    return personalized_features\n",
        "\n",
        "\n",
        "def extract_tsfel_features(df):\n",
        "    \"\"\"Extract time series features using TSFEL with personalized features.\n",
        "    \n",
        "    Args:\n",
        "        df: pandas DataFrame with 'ds' (datetime) and 'y' (value) columns\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of TSFEL features including personalized features\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    if len(df) < 2:\n",
        "        return features\n",
        "    \n",
        "    # Add basic statistics first (always available)\n",
        "    values = df['y'].values\n",
        "    features['mean'] = np.mean(values)\n",
        "    features['std'] = np.std(values)\n",
        "    features['min'] = np.min(values)\n",
        "    features['max'] = np.max(values)\n",
        "    features['range'] = features['max'] - features['min']\n",
        "    features['data_points'] = len(df)\n",
        "    \n",
        "    try:\n",
        "        # Prepare time series data for TSFEL\n",
        "        df_prepared = regularize_time_intervals(df, freq='D', method='ffill')\n",
        "        \n",
        "        if len(df_prepared) < 2:\n",
        "            # Calculate basic CV as fallback\n",
        "            mean_val = features.get('mean', 0)\n",
        "            std_val = features.get('std', 0)\n",
        "            features['cv'] = std_val / mean_val if mean_val != 0 else np.inf\n",
        "            return features\n",
        "        \n",
        "        # Clean data: remove infinite values, ensure proper types\n",
        "        signal = df_prepared['y'].values.copy()\n",
        "        signal = signal[~np.isnan(signal)]\n",
        "        signal = signal[~np.isinf(signal)]\n",
        "        \n",
        "        if len(signal) < 2 or np.var(signal) == 0:\n",
        "            mean_val = features.get('mean', 0)\n",
        "            std_val = features.get('std', 0)\n",
        "            features['cv'] = std_val / mean_val if mean_val != 0 else np.inf\n",
        "            return features\n",
        "        \n",
        "        # Get TSFEL feature configuration for statistical and temporal domains\n",
        "        cfg = tsfel.get_features_by_domain(['statistical', 'temporal'])\n",
        "        \n",
        "        # Extract features using TSFEL 0.2.0\n",
        "        if len(signal) < 10:\n",
        "            raise ValueError(f\"Signal too short for TSFEL: {len(signal)} points\")\n",
        "        \n",
        "        feature_dict = tsfel.time_series_features_extractor(cfg, signal, fs=1.0)\n",
        "        \n",
        "        # Extract TSFEL features using exact names from TSFEL 0.2.0\n",
        "        features['mean'] = to_scalar(feature_dict.get('0_Mean', features['mean']))\n",
        "        features['std'] = to_scalar(feature_dict.get('0_Standard deviation', features['std']))\n",
        "        features['var'] = to_scalar(feature_dict.get('0_Variance', np.nan))\n",
        "        features['entropy'] = to_scalar(feature_dict.get('0_Entropy', np.nan))\n",
        "        features['autocorr_persistence'] = to_scalar(feature_dict.get('0_Autocorrelation', np.nan))\n",
        "        \n",
        "        # Calculate slope and trend_strength\n",
        "        features['slope'] = to_scalar(feature_dict.get('0_Slope', np.nan))\n",
        "        slope_val = features['slope']\n",
        "        if isinstance(slope_val, (pd.Series, np.ndarray)):\n",
        "            slope_val = to_scalar(slope_val)\n",
        "        features['trend_strength'] = abs(slope_val) if not np.isnan(slope_val) else np.nan\n",
        "        \n",
        "        # Distribution shape indicators\n",
        "        features['skewness'] = to_scalar(feature_dict.get('0_Skewness', np.nan))\n",
        "        features['kurtosis'] = to_scalar(feature_dict.get('0_Kurtosis', np.nan))\n",
        "        \n",
        "        # Variability indicators\n",
        "        features['interquartile_range'] = to_scalar(feature_dict.get('0_Interquartile range', np.nan))\n",
        "        features['zero_crossing_rate'] = to_scalar(feature_dict.get('0_Zero crossing rate', np.nan))\n",
        "        \n",
        "        # Structure indicators\n",
        "        features['positive_turning_points'] = to_scalar(feature_dict.get('0_Positive turning points', np.nan))\n",
        "        features['negative_turning_points'] = to_scalar(feature_dict.get('0_Negative turning points', np.nan))\n",
        "        features['neighbourhood_peaks'] = to_scalar(feature_dict.get('0_Neighbourhood peaks', np.nan))\n",
        "        features['mean_absolute_deviation'] = to_scalar(feature_dict.get('0_Mean absolute deviation', np.nan))\n",
        "        \n",
        "        # Extract personalized features\n",
        "        personalized_cfg = create_personalized_features_config()\n",
        "        \n",
        "        # Calculate personalized features\n",
        "        features['cv'] = personalized_cfg['CV']['function'](signal)\n",
        "        features['acf1'] = personalized_cfg['ACF1']['function'](signal)\n",
        "        features['stability'] = personalized_cfg['Stability']['function'](signal)\n",
        "        \n",
        "        # Seasonality features\n",
        "        weekly_autocorr = personalized_cfg['Weekly Autocorr']['function'](signal)\n",
        "        monthly_autocorr = personalized_cfg['Monthly Autocorr']['function'](signal)\n",
        "        quarterly_autocorr = personalized_cfg['Quarterly Autocorr']['function'](signal)\n",
        "        \n",
        "        features['weekly_autocorr'] = weekly_autocorr\n",
        "        features['monthly_autocorr'] = monthly_autocorr\n",
        "        features['quarterly_autocorr'] = quarterly_autocorr\n",
        "        features['has_weekly_seasonality'] = abs(weekly_autocorr) > ACF_THRESHOLD\n",
        "        features['has_monthly_seasonality'] = abs(monthly_autocorr) > ACF_THRESHOLD\n",
        "        features['has_quarterly_seasonality'] = abs(quarterly_autocorr) > ACF_THRESHOLD\n",
        "        \n",
        "        # Outlier and changepoint features\n",
        "        features['outlier_ratio'] = personalized_cfg['Outlier Ratio']['function'](signal)\n",
        "        \n",
        "        # Changepoint detection returns both count and dates (takes dataframe, not signal)\n",
        "        changepoint_count, changepoint_dates = personalized_cfg['Changepoint Count']['function'](df_prepared)\n",
        "        features['changepoint_count'] = float(changepoint_count) if not np.isnan(changepoint_count) else 0.0\n",
        "        features['changepoint_dates'] = changepoint_dates  # List of datetime objects\n",
        "        \n",
        "    except Exception as tsfel_error:\n",
        "        # TSFEL feature extraction failed - set all TSFEL-specific features to NaN\n",
        "        features['acf1'] = np.nan\n",
        "        features['autocorr_persistence'] = np.nan\n",
        "        features['trend_strength'] = np.nan\n",
        "        features['stability'] = np.nan\n",
        "        features['entropy'] = np.nan\n",
        "        features['slope'] = np.nan\n",
        "        features['skewness'] = np.nan\n",
        "        features['kurtosis'] = np.nan\n",
        "        features['interquartile_range'] = np.nan\n",
        "        features['zero_crossing_rate'] = np.nan\n",
        "        features['positive_turning_points'] = np.nan\n",
        "        features['negative_turning_points'] = np.nan\n",
        "        features['neighbourhood_peaks'] = np.nan\n",
        "        features['mean_absolute_deviation'] = np.nan\n",
        "        features['var'] = features.get('std', np.nan) ** 2 if not np.isnan(features.get('std', np.nan)) else np.nan\n",
        "        \n",
        "        # Calculate basic CV as fallback\n",
        "        mean_val = features.get('mean', np.nan)\n",
        "        std_val = features.get('std', np.nan)\n",
        "        if not np.isnan(mean_val) and mean_val != 0:\n",
        "            features['cv'] = abs(std_val) / abs(mean_val) if not np.isnan(std_val) else np.nan\n",
        "        else:\n",
        "            features['cv'] = np.nan\n",
        "        \n",
        "        # Set seasonality and detector features to defaults\n",
        "        features['has_weekly_seasonality'] = False\n",
        "        features['has_monthly_seasonality'] = False\n",
        "        features['has_quarterly_seasonality'] = False\n",
        "        features['weekly_autocorr'] = 0.0\n",
        "        features['monthly_autocorr'] = 0.0\n",
        "        features['quarterly_autocorr'] = 0.0\n",
        "        features['outlier_ratio'] = 0.0\n",
        "        features['changepoint_count'] = 0.0\n",
        "        features['changepoint_dates'] = []\n",
        "    \n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Classification Function\n",
        "\n",
        "Classify time series by predictability based on TSFEL features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_with_tsfel(features, detector_info):\n",
        "    \"\"\"Classify time series predictability based on TSFEL features and detectors.\n",
        "    \n",
        "    Args:\n",
        "        features: Dictionary of TSFEL features including seasonality flags\n",
        "        detector_info: Dictionary with outlier and changepoint information\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (category, reason)\n",
        "    \"\"\"\n",
        "    # Handle None or empty features\n",
        "    if not features:\n",
        "        return 'Not Suitable', 'No features extracted'\n",
        "    \n",
        "    if not detector_info:\n",
        "        detector_info = {'outlier_ratio': 0.0, 'changepoint_count': 0}\n",
        "    \n",
        "    # Check for data quality issues\n",
        "    outlier_ratio = detector_info.get('outlier_ratio', 0.0)\n",
        "    changepoint_count = detector_info.get('changepoint_count', 0)\n",
        "    \n",
        "    # High outlier ratio indicates poor data quality\n",
        "    if outlier_ratio > 0.2:  # More than 20% outliers\n",
        "        return 'Not Suitable', f'High outlier ratio: {outlier_ratio:.2%}'\n",
        "    \n",
        "    # Too many changepoints indicates instability\n",
        "    if changepoint_count > 5:\n",
        "        return 'Not Suitable', f'Too many changepoints: {changepoint_count}'\n",
        "    \n",
        "    # Extract seasonality information\n",
        "    has_weekly = features.get('has_weekly_seasonality', False)\n",
        "    has_monthly = features.get('has_monthly_seasonality', False)\n",
        "    has_quarterly = features.get('has_quarterly_seasonality', False)\n",
        "    has_seasonality = has_weekly or has_monthly or has_quarterly\n",
        "    \n",
        "    # Extract key TSFEL features\n",
        "    acf1 = features.get('acf1', np.nan)\n",
        "    autocorr_persistence = features.get('autocorr_persistence', np.nan)\n",
        "    trend_strength = features.get('trend_strength', np.nan)\n",
        "    stability = features.get('stability', np.nan)\n",
        "    cv = features.get('cv', np.nan)\n",
        "    skewness = features.get('skewness', np.nan)\n",
        "    kurtosis = features.get('kurtosis', np.nan)\n",
        "    zero_crossing_rate = features.get('zero_crossing_rate', np.nan)\n",
        "    interquartile_range = features.get('interquartile_range', np.nan)\n",
        "    mean_val = features.get('mean', np.nan)\n",
        "    std_val = features.get('std', np.nan)\n",
        "    \n",
        "    # Assess predictability indicators\n",
        "    high_noise = False\n",
        "    if not np.isnan(zero_crossing_rate):\n",
        "        data_points = features.get('data_points', np.nan)\n",
        "        if not np.isnan(data_points) and data_points > 0:\n",
        "            zcr_normalized = zero_crossing_rate / data_points\n",
        "            high_noise = zcr_normalized > 0.3\n",
        "        else:\n",
        "            high_noise = zero_crossing_rate > 30\n",
        "    \n",
        "    extreme_skew = False\n",
        "    if not np.isnan(skewness):\n",
        "        extreme_skew = abs(skewness) > 2.0\n",
        "    \n",
        "    heavy_tails = False\n",
        "    if not np.isnan(kurtosis):\n",
        "        heavy_tails = kurtosis > 5.0\n",
        "    \n",
        "    high_variability = False\n",
        "    if not np.isnan(interquartile_range) and not np.isnan(mean_val) and abs(mean_val) > 0:\n",
        "        iqr_coefficient = interquartile_range / abs(mean_val)\n",
        "        high_variability = iqr_coefficient > 1.0\n",
        "    \n",
        "    negative_indicators = sum([high_noise, extreme_skew, heavy_tails, high_variability])\n",
        "    seasonality_count = sum([has_weekly, has_monthly, has_quarterly])\n",
        "    \n",
        "    # Classification logic\n",
        "    if has_seasonality and seasonality_count >= 1:\n",
        "        if negative_indicators <= 1:\n",
        "            if not np.isnan(acf1) and acf1 > 0.3:\n",
        "                category = 'Predictable'\n",
        "                seasonality_types = []\n",
        "                if has_weekly:\n",
        "                    seasonality_types.append('weekly')\n",
        "                if has_monthly:\n",
        "                    seasonality_types.append('monthly')\n",
        "                if has_quarterly:\n",
        "                    seasonality_types.append('quarterly')\n",
        "                reason = f'Seasonality detected: {\", \".join(seasonality_types)}'\n",
        "                if not np.isnan(acf1):\n",
        "                    reason += f', ACF1={acf1:.2f}'\n",
        "                return category, reason\n",
        "    \n",
        "    if not np.isnan(acf1) and acf1 > 0.5:\n",
        "        if not np.isnan(stability) and stability > 0.7:\n",
        "            if negative_indicators <= 1:\n",
        "                category = 'Predictable'\n",
        "                reason = f'Strong autocorrelation (ACF1={acf1:.2f}) and stability ({stability:.2f})'\n",
        "                return category, reason\n",
        "            else:\n",
        "                category = 'Low Predictability'\n",
        "                reason = f'Strong autocorrelation (ACF1={acf1:.2f}) but high noise/variability'\n",
        "                return category, reason\n",
        "    \n",
        "    if not np.isnan(acf1) and acf1 > 0.2:\n",
        "        if not np.isnan(stability) and stability > 0.5:\n",
        "            if not np.isnan(autocorr_persistence) and autocorr_persistence > 10 and negative_indicators <= 1:\n",
        "                category = 'Predictable'\n",
        "                reason = f'Moderate autocorrelation (ACF1={acf1:.2f}), high persistence (lag={autocorr_persistence:.0f}), and stability ({stability:.2f})'\n",
        "                return category, reason\n",
        "            else:\n",
        "                category = 'Low Predictability'\n",
        "                reason = f'Moderate autocorrelation (ACF1={acf1:.2f}) and stability ({stability:.2f})'\n",
        "                return category, reason\n",
        "    \n",
        "    if not np.isnan(acf1) and acf1 < 0.2:\n",
        "        category = 'Low Predictability'\n",
        "        reason = f'Low autocorrelation (ACF1={acf1:.2f})'\n",
        "        return category, reason\n",
        "    \n",
        "    if has_seasonality:\n",
        "        category = 'Low Predictability'\n",
        "        seasonality_types = []\n",
        "        if has_weekly:\n",
        "            seasonality_types.append('weekly')\n",
        "        if has_monthly:\n",
        "            seasonality_types.append('monthly')\n",
        "        if has_quarterly:\n",
        "            seasonality_types.append('quarterly')\n",
        "        reason = f'Seasonality detected but weak patterns: {\", \".join(seasonality_types)}'\n",
        "        return category, reason\n",
        "    \n",
        "    if not np.isnan(acf1) or not np.isnan(trend_strength):\n",
        "        return 'Low Predictability', 'Weak patterns detected'\n",
        "    \n",
        "    return 'Not Suitable', 'Insufficient features for classification'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_prophet_params(classification, features, df=None):\n",
        "    \"\"\"Generate Prophet parameters based on classification and TSFEL features.\n",
        "    \n",
        "    Args:\n",
        "        classification: 'Predictable', 'Low Predictability', or 'Not Suitable'\n",
        "        features: Dictionary of TSFEL features\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of Prophet parameters\n",
        "    \"\"\"\n",
        "    if classification == 'Not Suitable':\n",
        "        return None\n",
        "    \n",
        "    # Base parameters\n",
        "    prophet_params = {\n",
        "        'yearly_seasonality': False,\n",
        "        'weekly_seasonality': False,\n",
        "        'daily_seasonality': False,\n",
        "        'seasonality_mode': 'multiplicative',\n",
        "    }\n",
        "    \n",
        "    if classification == 'Predictable':\n",
        "        # Enable seasonality based on detected patterns\n",
        "        has_weekly = features.get('has_weekly_seasonality', False)\n",
        "        has_monthly = features.get('has_monthly_seasonality', False)\n",
        "        has_quarterly = features.get('has_quarterly_seasonality', False)\n",
        "        \n",
        "        # Build add_seasonalities list for custom seasonalities\n",
        "        # Monthly and quarterly must be added via add_seasonalities (Prophet doesn't have built-in monthly/quarterly)\n",
        "        add_seasonalities = []\n",
        "        \n",
        "        # Add monthly seasonality if detected\n",
        "        if has_monthly:\n",
        "            add_seasonalities.append({\n",
        "                'name': 'monthly',\n",
        "                'seasonal_periods': 30.5,\n",
        "                'fourier_order': 5\n",
        "            })\n",
        "        \n",
        "        # Add quarterly seasonality if detected\n",
        "        if has_quarterly:\n",
        "            add_seasonalities.append({\n",
        "                'name': 'quarterly',\n",
        "                'seasonal_periods': 91.25,\n",
        "                'fourier_order': 5\n",
        "            })\n",
        "        \n",
        "        # If weekly is detected, add it as custom seasonality (disable built-in)\n",
        "        # This matches the pattern used in recommended parameters\n",
        "        if has_weekly:\n",
        "            prophet_params['weekly_seasonality'] = True\n",
        "        else:\n",
        "            prophet_params['weekly_seasonality'] = False\n",
        "        \n",
        "        # Add custom seasonalities if any are detected\n",
        "        if add_seasonalities:\n",
        "            prophet_params['add_seasonalities'] = add_seasonalities\n",
        "        \n",
        "        # Add changepoint dates if detected\n",
        "        changepoint_dates = features.get('changepoint_dates', [])\n",
        "        if changepoint_dates and len(changepoint_dates) > 0:\n",
        "            # changepoint_dates is now a simple list of date objects\n",
        "            # Calculate previous_date for each changepoint when adding to Prophet\n",
        "            changepoint_dates_clean = []\n",
        "            \n",
        "            for cp_date in changepoint_dates:\n",
        "                # Handle both tuple format (legacy) and date format (new)\n",
        "                if isinstance(cp_date, tuple) and len(cp_date) == 2:\n",
        "                    # Legacy format: (changepoint_date, previous_date)\n",
        "                    changepoint_date, previous_date = cp_date\n",
        "                    if changepoint_date is not None:\n",
        "                        changepoint_dates_clean.append(changepoint_date)\n",
        "                    if previous_date is not None:\n",
        "                        changepoint_dates_clean.append(previous_date)\n",
        "                elif isinstance(cp_date, date):\n",
        "                    # New format: just the changepoint date\n",
        "                    # Add the changepoint date\n",
        "                    changepoint_dates_clean.append(cp_date)\n",
        "                    \n",
        "                    # Find previous data point in the dataframe (not just subtract 1 day)\n",
        "                    previous_date = None\n",
        "                    if df is not None and 'ds' in df.columns:\n",
        "                        # Prepare sorted dataframe for finding previous data points\n",
        "                        df_sorted = df.copy()\n",
        "                        df_sorted['ds'] = pd.to_datetime(df_sorted['ds'])\n",
        "                        df_sorted = df_sorted.sort_values('ds').reset_index(drop=True)\n",
        "                        # Convert 'ds' to date for comparison\n",
        "                        df_sorted['ds_date'] = df_sorted['ds'].dt.date\n",
        "                        \n",
        "                        # Find the changepoint date in the dataframe\n",
        "                        cp_matches = df_sorted[df_sorted['ds_date'] == cp_date]\n",
        "                        if len(cp_matches) > 0:\n",
        "                            # Get the index of the first match\n",
        "                            cp_idx = cp_matches.index[0]\n",
        "                            # Get the previous row if it exists\n",
        "                            if cp_idx > 0:\n",
        "                                previous_date = df_sorted.iloc[cp_idx - 1]['ds_date']\n",
        "                    \n",
        "                    # If we found a previous date, add it; otherwise skip (don't add arbitrary date)\n",
        "                    if previous_date is not None:\n",
        "                        changepoint_dates_clean.append(previous_date)\n",
        "            if changepoint_dates_clean:\n",
        "                # Sort and deduplicate dates (keep as date objects)\n",
        "                changepoint_dates_clean = sorted(set(changepoint_dates_clean))\n",
        "                prophet_params['changepoints'] = changepoint_dates_clean\n",
        "        # Adjust changepoint_prior_scale based on stability and changepoint_count\n",
        "        stability = features.get('stability', np.nan)\n",
        "        changepoint_count = features.get('changepoint_count', 0)\n",
        "        \n",
        "        if not np.isnan(stability):\n",
        "            if stability > 0.7 and changepoint_count <= 2:\n",
        "                # High stability, low changepoints -> smoother trend\n",
        "                prophet_params['changepoint_prior_scale'] = 0.3\n",
        "            elif stability > 0.5 and changepoint_count <= 3:\n",
        "                # Moderate stability -> default flexibility\n",
        "                prophet_params['changepoint_prior_scale'] = 0.15\n",
        "            else:\n",
        "                # Lower stability or more changepoints -> more flexible\n",
        "                prophet_params['changepoint_prior_scale'] = 0.1\n",
        "        else:\n",
        "            prophet_params['changepoint_prior_scale'] = 0.05\n",
        "        \n",
        "        # Adjust seasonality_prior_scale based on autocorrelation\n",
        "        acf1 = features.get('acf1', np.nan)\n",
        "        autocorr_persistence = features.get('autocorr_persistence', np.nan)\n",
        "        \n",
        "        if not np.isnan(acf1) and acf1 > 0.5:\n",
        "            # Strong autocorrelation -> emphasize seasonality\n",
        "            prophet_params['seasonality_prior_scale'] = 6.0\n",
        "        elif not np.isnan(autocorr_persistence) and autocorr_persistence > 10:\n",
        "            # High persistence -> moderate seasonality emphasis\n",
        "            prophet_params['seasonality_prior_scale'] = 10.0\n",
        "        else:\n",
        "            # Default seasonality strength\n",
        "            prophet_params['seasonality_prior_scale'] = 10.0\n",
        "\n",
        "        prophet_params['interval_width'] = 0.95\n",
        "    \n",
        "    elif classification == 'Low Predictability':\n",
        "        # Trend-only model (no seasonality)\n",
        "        prophet_params['changepoint_prior_scale'] = 0.1  # Slightly more flexible for trend-only\n",
        "        prophet_params['seasonality_prior_scale'] = 1.0  # Low (not used, but set for consistency)\n",
        "    \n",
        "    return prophet_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Process Series and Generate Forecasts\n",
        "\n",
        "Iterate over each time series, extract features, classify, and generate forecasts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each series: extract features, classify, and forecast\n",
        "forecasts_by_series = []\n",
        "classification_counts = {'Predictable': 0, 'Low Predictability': 0, 'Not Suitable': 0, 'Error': 0}\n",
        "\n",
        "# When not dry run: create DB connection and run record before the loop (for per-metric metadata and run stats)\n",
        "if not DRY_RUN:\n",
        "    engine, conn = create_database_connection(\n",
        "        environment=VM_JOBS_ENVIRONMENT if VM_JOBS_ENVIRONMENT else None,\n",
        "        config_path=VM_JOBS_CONFIG_PATH if VM_JOBS_CONFIG_PATH else None,\n",
        "    )\n",
        "    tsfel_params = {\n",
        "        'seasonality_lag_weekly': SEASONALITY_LAG_WEEKLY,\n",
        "        'seasonality_lag_monthly': SEASONALITY_LAG_MONTHLY,\n",
        "        'seasonality_lag_quarterly': SEASONALITY_LAG_QUARTERLY,\n",
        "        'acf_threshold': ACF_THRESHOLD,\n",
        "        'regularize_freq': 'D',\n",
        "        'regularize_method': 'ffill',\n",
        "        'tsfel_domains': ['statistical', 'temporal'],\n",
        "        'tsfel_sampling_frequency': 1.0,\n",
        "        'min_history_points': MIN_HISTORY_POINTS,\n",
        "        'outlier_ratio_threshold': 0.2,\n",
        "        'changepoint_count_threshold': 5,\n",
        "        'forecast_floor': FORECAST_FLOOR if FORECAST_FLOOR is not None else None,\n",
        "    }\n",
        "    representative_params = {\n",
        "        'yearly_seasonality': False,\n",
        "        'weekly_seasonality': False,\n",
        "        'daily_seasonality': False,\n",
        "        'seasonality_mode': 'additive',\n",
        "        'changepoint_prior_scale': 0.05,\n",
        "    }\n",
        "    model_config = {'prophet_params': representative_params, 'tsfel_params': tsfel_params}\n",
        "    run_id = create_forecast_run_record(\n",
        "        conn=conn,\n",
        "        job_id=\"metrics_forecast_notebooks\",\n",
        "        selection_value=SELECTOR,\n",
        "        model_type=\"tsfel_prophet\",\n",
        "        model_config=model_config,\n",
        "        model_fit_config={},\n",
        "        history_days=HISTORY_DAYS,\n",
        "        forecast_horizon_days=FORECAST_HORIZON_DAYS,\n",
        "        min_history_points=MIN_HISTORY_POINTS,\n",
        "        config_source=\"notebook\",\n",
        "    )\n",
        "    loop_start = datetime.now(timezone.utc)\n",
        "    print(f\"Created forecast run record: run_id={run_id}\")\n",
        "    if not check_forecast_metadata_table_exists(conn):\n",
        "        raise RuntimeError(\n",
        "            \"Table public.vm_metrics_forecast_metadata does not exist. \"\n",
        "            \"Run database/vm_metrics_forecast_metadata.sql to create it.\"\n",
        "        )\n",
        "else:\n",
        "    conn = None\n",
        "    run_id = None\n",
        "    loop_start = None\n",
        "\n",
        "print(f\"Processing {len(query_result)} time series...\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "for series_idx, item in enumerate(query_result):\n",
        "    metric = item.get('metric', {})\n",
        "    metric_name = metric.get('__name__')\n",
        "    if not metric_name:\n",
        "        continue\n",
        "    \n",
        "    labels = {k: v for k, v in metric.items() if k != '__name__'}\n",
        "    values = item.get('values', [])\n",
        "    samples = [(datetime.fromtimestamp(float(ts), tz=timezone.utc), float(value)) for ts, value in values]\n",
        "    \n",
        "    if not samples:\n",
        "        continue\n",
        "    \n",
        "    series_info = {'metric_name': metric_name, 'labels': labels}\n",
        "    \n",
        "    print(f\"\\nProcessing {series_idx + 1}/{len(query_result)}: {metric_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Prepare data\n",
        "        df = pd.DataFrame(samples, columns=['ds', 'y'])\n",
        "        df['ds'] = pd.to_datetime(df['ds'], utc=True).dt.tz_localize(None)\n",
        "        df = df.sort_values('ds').reset_index(drop=True)\n",
        "        \n",
        "        # Check minimum data points\n",
        "        if len(df) < MIN_HISTORY_POINTS:\n",
        "            print(f\"    Skipping: insufficient data ({len(df)} < {MIN_HISTORY_POINTS})\")\n",
        "            classification_counts['Not Suitable'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Check for invalid values\n",
        "        if df['y'].isna().all() or np.isinf(df['y']).any():\n",
        "            print(f\"    Skipping: invalid values\")\n",
        "            classification_counts['Not Suitable'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Extract TSFEL features\n",
        "        print(f\"  Extracting TSFEL features...\")\n",
        "        try:\n",
        "            features = extract_tsfel_features(df)\n",
        "            if not features or 'mean' not in features:\n",
        "                print(f\"    Warning: TSFEL feature extraction returned no features\")\n",
        "                classification_counts['Not Suitable'] += 1\n",
        "                continue\n",
        "        except Exception as feat_error:\n",
        "            print(f\"    Warning: TSFEL feature extraction failed: {feat_error}\")\n",
        "            classification_counts['Not Suitable'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Extract detector information\n",
        "        outlier_ratio = features.get('outlier_ratio', 0.0)\n",
        "        changepoint_count = features.get('changepoint_count', 0.0)\n",
        "        data_points = features.get('data_points', 0)\n",
        "        outlier_count = int(outlier_ratio * data_points) if data_points > 0 else 0\n",
        "        \n",
        "        detector_info = {\n",
        "            'outlier_count': outlier_count,\n",
        "            'outlier_ratio': float(outlier_ratio) if not np.isnan(outlier_ratio) else 0.0,\n",
        "            'changepoint_count': int(changepoint_count) if not np.isnan(changepoint_count) else 0\n",
        "        }\n",
        "        \n",
        "        # Classify series\n",
        "        classification, reason = classify_with_tsfel(features, detector_info)\n",
        "        print(f\"  Classification: {classification} - {reason}\")\n",
        "        classification_counts[classification] = classification_counts.get(classification, 0) + 1\n",
        "        \n",
        "        # Skip if not suitable\n",
        "        if classification == 'Not Suitable':\n",
        "            continue\n",
        "        \n",
        "        # Generate Prophet parameters (pass df to find previous data points for changepoints)\n",
        "        prophet_params = generate_prophet_params(classification, features, df=df)\n",
        "        if prophet_params is None:\n",
        "            print(f\"    Skipping: no Prophet parameters generated\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"  Prophet params: {prophet_params}\")\n",
        "        \n",
        "        # Prepare training data\n",
        "        df_training = df.copy()\n",
        "        \n",
        "        # Add floor/cap columns if floor is specified\n",
        "        prophet_params_final = prophet_params.copy()\n",
        "        if FORECAST_FLOOR is not None:\n",
        "            prophet_params_final['growth'] = 'logistic'\n",
        "            prophet_params_final['floor'] = FORECAST_FLOOR\n",
        "            prophet_params_final['cap'] = df_training['y'].max() * 1.5 if df_training['y'].max() > FORECAST_FLOOR else FORECAST_FLOOR + 1\n",
        "        \n",
        "        # Convert to darts TimeSeries\n",
        "        series = TimeSeries.from_dataframe(df_training.set_index('ds'), fill_missing_dates=True, freq=\"D\")\n",
        "        \n",
        "        # Train Prophet model\n",
        "        model = DartsProphet(**prophet_params_final)\n",
        "        model.fit(series)\n",
        "        \n",
        "        # Generate forecast\n",
        "        forecast = model.predict(n=FORECAST_HORIZON_DAYS)\n",
        "        forecast_df = pd.DataFrame({\n",
        "            'ds': forecast.time_index,\n",
        "            'yhat': forecast.values().flatten()\n",
        "        })\n",
        "        \n",
        "        print(f\"   Forecast: {len(forecast_df)} predictions\")\n",
        "        \n",
        "        # Store forecast (include features and reason so we can save metadata in the save loop using job_idx/metric_id from save_forecasts_to_database)\n",
        "        # Always store extended tuple with df_training and detector_info for plotting\n",
        "        forecasts_by_series.append((series_info, forecast_df, df_training, classification, prophet_params, features, detector_info, reason))\n",
        "        \n",
        "    except Exception as exc:\n",
        "        print(f\"   Failed: {exc}\")\n",
        "        classification_counts['Error'] += 1\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Processing complete: {len(forecasts_by_series)}/{len(query_result)} series forecasted\")\n",
        "print(f\"\\nClassification summary:\")\n",
        "for cls, count in classification_counts.items():\n",
        "    if count > 0:\n",
        "        print(f\"  {cls}: {count}\")\n",
        "\n",
        "# Update run record with counts and completion status (when not dry run)\n",
        "if not DRY_RUN and conn is not None and run_id is not None:\n",
        "    try:\n",
        "        duration_seconds = (datetime.now(timezone.utc) - loop_start).total_seconds() if loop_start else None\n",
        "        update_forecast_run_record(\n",
        "            conn=conn,\n",
        "            run_id=run_id,\n",
        "            series_count=len(query_result),\n",
        "            success_count=len(forecasts_by_series),\n",
        "            failed_count=len(query_result) - len(forecasts_by_series),\n",
        "            status=\"completed\" if len(forecasts_by_series) == len(query_result) else \"partial\",\n",
        "            completed_at=datetime.now(timezone.utc),\n",
        "            duration_seconds=duration_seconds,\n",
        "        )\n",
        "    except Exception as update_exc:\n",
        "        print(f\"Warning: failed to update run record: {update_exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7a. Cross-Correlation Outlier Clustering\n",
        "\n",
        "Cluster time series by similarity of outlier timing patterns to surface metrics whose anomalies co-occur (e.g., shared incidents or deployments). Uses IQR-based outlier detection, cross-correlation of binary outlier vectors, and hierarchical clustering. Visualization shows historical data with outlier points highlightedno forecast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_outlier_dates(df):\n",
        "    \"\"\"IQR-based outlier dates. Returns set of dates (as date) where outliers occur.\"\"\"\n",
        "    values = df['y'].values\n",
        "    Q1, Q3 = np.percentile(values, [25, 75])\n",
        "    IQR = Q3 - Q1\n",
        "    if IQR == 0:\n",
        "        return set()\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    outlier_mask = (values < lower) | (values > upper)\n",
        "    return set(pd.to_datetime(df.loc[outlier_mask, 'ds']).dt.date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build binary outlier matrix and cluster series by cross-correlation of outlier timing\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "outlier_clusters = {}  # cluster_id -> list of (series_info, df_training, outlier_dates)\n",
        "cluster_labels = []   # cluster label per series index\n",
        "\n",
        "if len(forecasts_by_series) < 2:\n",
        "    print(\"Skipping outlier clustering: need at least 2 series\")\n",
        "else:\n",
        "    # Extract outlier dates and df_training for each series\n",
        "    series_data = []\n",
        "    for item in forecasts_by_series:\n",
        "        series_info, forecast_df, df_training, classification, prophet_params, features, detector_info, reason = item\n",
        "        outlier_dates = get_outlier_dates(df_training)\n",
        "        series_data.append((series_info, df_training, outlier_dates))\n",
        "\n",
        "    # Check if any series have outliers\n",
        "    total_outliers = sum(len(od) for _, _, od in series_data)\n",
        "    if total_outliers == 0:\n",
        "        print(\"Skipping outlier clustering: no outliers detected across any series\")\n",
        "    else:\n",
        "        # Build common date grid\n",
        "        all_dates = set()\n",
        "        for _, df, _ in series_data:\n",
        "            all_dates.update(pd.to_datetime(df['ds']).dt.date)\n",
        "        date_grid = sorted(all_dates)\n",
        "\n",
        "        # Build binary matrix M: (n_series, n_days)\n",
        "        M = np.zeros((len(series_data), len(date_grid)), dtype=float)\n",
        "        for i, (_, _, outlier_dates) in enumerate(series_data):\n",
        "            for j, d in enumerate(date_grid):\n",
        "                M[i, j] = 1.0 if d in outlier_dates else 0.0\n",
        "\n",
        "        # Correlation matrix (handle constant vectors)\n",
        "        try:\n",
        "            corr_matrix = np.corrcoef(M)\n",
        "            np.nan_to_num(corr_matrix, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        except Exception:\n",
        "            corr_matrix = np.eye(len(series_data))\n",
        "\n",
        "        # Distance = 1 - abs(correlation) for clustering\n",
        "        dist_matrix = 1 - np.abs(corr_matrix)\n",
        "        np.fill_diagonal(dist_matrix, 0)\n",
        "\n",
        "        # Hierarchical clustering\n",
        "        condensed_dist = squareform(dist_matrix, checks=False)\n",
        "        Z = linkage(condensed_dist, method='average')\n",
        "        n_clusters = min(5, max(1, len(series_data) // 2))\n",
        "        cluster_labels = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
        "\n",
        "        # Group by cluster\n",
        "        for idx, label in enumerate(cluster_labels):\n",
        "            if label not in outlier_clusters:\n",
        "                outlier_clusters[label] = []\n",
        "            outlier_clusters[label].append(series_data[idx])\n",
        "\n",
        "        print(f\"Outlier clustering: {len(series_data)} series -> {len(outlier_clusters)} clusters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: correlation heatmap and dendrogram (separate figures for readability)\n",
        "if outlier_clusters and len(series_data) >= 2:\n",
        "    from scipy.cluster.hierarchy import dendrogram\n",
        "    metric_labels = [s[0]['metric_name'] for s in series_data]\n",
        "    n_series = len(metric_labels)\n",
        "    # Heatmap: size scales with number of series for label readability\n",
        "    fig1, ax1 = plt.subplots(figsize=(max(10, n_series * 0.8), max(8, n_series * 0.6)))\n",
        "    sns.heatmap(corr_matrix, ax=ax1, cmap='RdYlBu_r', center=0, vmin=-1, vmax=1,\n",
        "                xticklabels=metric_labels, yticklabels=metric_labels)\n",
        "    ax1.set_title('Outlier timing correlation (series x series)')\n",
        "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Dendrogram: separate figure\n",
        "    fig2, ax2 = plt.subplots(figsize=(max(12, n_series * 0.5), 6))\n",
        "    dendrogram(Z, ax=ax2, labels=metric_labels, leaf_rotation=45)\n",
        "    ax2.set_title('Hierarchical clustering dendrogram')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print cluster summary table\n",
        "if outlier_clusters:\n",
        "    print(\"Cluster summary (outlier timing cross-correlation):\")\n",
        "    print(\"-\" * 60)\n",
        "    for cid, members in sorted(outlier_clusters.items()):\n",
        "        metric_names = [m[0]['metric_name'] for m in members]\n",
        "        outlier_counts = [len(m[2]) for m in members]\n",
        "        print(f\"  Cluster {cid}: {len(members)} series, \"\n",
        "              f\"{sum(outlier_counts)} total outliers\")\n",
        "        for name, cnt in zip(metric_names, outlier_counts):\n",
        "            print(f\"    - {name}: {cnt} outliers\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-cluster time series plots: historical data only, outlier points highlighted (no forecast)\n",
        "if outlier_clusters:\n",
        "    for cid, members in sorted(outlier_clusters.items()):\n",
        "        # Width accommodates legend for long metric names\n",
        "        fig, ax = plt.subplots(figsize=(16, 6))\n",
        "        for series_info, df_training, outlier_dates in members:\n",
        "            df = df_training.copy()\n",
        "            df['ds'] = pd.to_datetime(df['ds'])\n",
        "            # Z-score normalize for comparable overlay\n",
        "            y_mean, y_std = df['y'].mean(), df['y'].std()\n",
        "            y_norm = (df['y'] - y_mean) / y_std if y_std and y_std > 0 else df['y'] - y_mean\n",
        "            ax.plot(df['ds'], y_norm, '-', alpha=0.7, label=series_info['metric_name'])\n",
        "            # Highlight outlier points\n",
        "            outlier_mask = df['ds'].dt.date.isin(outlier_dates)\n",
        "            if outlier_mask.any():\n",
        "                ax.scatter(df.loc[outlier_mask, 'ds'], y_norm[outlier_mask],\n",
        "                          s=80, marker='o', edgecolors='red', facecolors='none', zorder=5)\n",
        "        ax.set_title(f'Cluster {cid}: {len(members)} series (z-score normalized, outliers in red)')\n",
        "        ax.set_xlabel('Date')\n",
        "        ax.set_ylabel('Z-score')\n",
        "        ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), fontsize=9, frameon=True)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Results\n",
        "\n",
        "**Note:** Plots are always generated for each series showing historical data and forecasts. The `dry_run` parameter only controls whether forecasts are saved to the database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot forecasts for each series (always; dry_run only controls database save)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Generating plots (history + forecast for each series)\")\n",
        "if DRY_RUN:\n",
        "    print(\"Dry run: no data will be saved to database\")\n",
        "else:\n",
        "    print(\"Forecasts will be saved to database\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for plot_idx, forecast_item in enumerate(forecasts_by_series):\n",
        "    # Unpack: (series_info, forecast_df, df_training, classification, prophet_params, features, detector_info, reason)\n",
        "    series_info, forecast_df, df_training, classification, prophet_params, features, detector_info, reason = forecast_item\n",
        "\n",
        "    plt.figure(figsize=(18, 10))\n",
        "\n",
        "    # Plot historical data\n",
        "    plt.plot(df_training['ds'], df_training['y'], \n",
        "            'ko-', label='Historical Data', linewidth=2, markersize=3, alpha=0.6)\n",
        "\n",
        "    # Plot forecast trend\n",
        "    plt.plot(forecast_df['ds'], forecast_df['yhat'], \n",
        "            'b--', label='Forecast (trend)', linewidth=2.5)\n",
        "\n",
        "    # Vertical line showing where forecast starts\n",
        "    last_history_date = df_training['ds'].max()\n",
        "    plt.axvline(x=last_history_date, color='red', linestyle=':', \n",
        "               linewidth=2, label='Forecast Start', alpha=0.7)\n",
        "\n",
        "    # Title and labels\n",
        "    title = f\"TSFEL-Prophet Forecast: {series_info['metric_name']} [{classification}]\"\n",
        "    if series_info['labels']:\n",
        "        title += f\" {series_info['labels']}\"\n",
        "    plt.title(title, fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Value', fontsize=12)\n",
        "    plt.legend(loc='best', fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add classification features text below the plot (similar to classification notebook)\n",
        "    if features:\n",
        "        decision_lines = []\n",
        "        \n",
        "        # Extract key features for decision display\n",
        "        acf1 = features.get('acf1', np.nan)\n",
        "        autocorr_persistence = features.get('autocorr_persistence', np.nan)\n",
        "        stability = features.get('stability', np.nan)\n",
        "        has_weekly = features.get('has_weekly_seasonality', False)\n",
        "        has_monthly = features.get('has_monthly_seasonality', False)\n",
        "        has_quarterly = features.get('has_quarterly_seasonality', False)\n",
        "        skewness = features.get('skewness', np.nan)\n",
        "        kurtosis = features.get('kurtosis', np.nan)\n",
        "        zero_crossing_rate = features.get('zero_crossing_rate', np.nan)\n",
        "        interquartile_range = features.get('interquartile_range', np.nan)\n",
        "        mean_val = features.get('mean', np.nan)\n",
        "        data_points = features.get('data_points', np.nan)\n",
        "        outlier_ratio = detector_info.get('outlier_ratio', 0.0)\n",
        "        changepoint_count = detector_info.get('changepoint_count', 0)\n",
        "        \n",
        "        # Classification decision factors\n",
        "        decision_lines.append(f\"Classification: {classification}\")\n",
        "        if reason:\n",
        "            decision_lines.append(f\"Reason: {reason}\")\n",
        "        \n",
        "        # Primary decision factors\n",
        "        primary_factors = []\n",
        "        if not np.isnan(acf1):\n",
        "            if acf1 > 0.5:\n",
        "                primary_factors.append(f\"ACF1={acf1:.2f} (strong)\")\n",
        "            elif acf1 > 0.2:\n",
        "                primary_factors.append(f\"ACF1={acf1:.2f} (moderate)\")\n",
        "            else:\n",
        "                primary_factors.append(f\"ACF1={acf1:.2f} (low)\")\n",
        "        \n",
        "        # Add autocorrelation persistence (TSFEL feature)\n",
        "        if not np.isnan(autocorr_persistence):\n",
        "            if autocorr_persistence > 10:\n",
        "                primary_factors.append(f\"Persistence=lag {autocorr_persistence:.0f} (high)\")\n",
        "            elif autocorr_persistence > 5:\n",
        "                primary_factors.append(f\"Persistence=lag {autocorr_persistence:.0f} (moderate)\")\n",
        "            else:\n",
        "                primary_factors.append(f\"Persistence=lag {autocorr_persistence:.0f} (low)\")\n",
        "        \n",
        "        if not np.isnan(stability):\n",
        "            if stability > 0.7:\n",
        "                primary_factors.append(f\"Stability={stability:.2f} (high)\")\n",
        "            elif stability > 0.5:\n",
        "                primary_factors.append(f\"Stability={stability:.2f} (moderate)\")\n",
        "            else:\n",
        "                primary_factors.append(f\"Stability={stability:.2f} (low)\")\n",
        "        \n",
        "        if has_weekly or has_monthly or has_quarterly:\n",
        "            seasonality_list = []\n",
        "            if has_weekly:\n",
        "                seasonality_list.append('weekly')\n",
        "            if has_monthly:\n",
        "                seasonality_list.append('monthly')\n",
        "            if has_quarterly:\n",
        "                seasonality_list.append('quarterly')\n",
        "            primary_factors.append(f\"Seasonality: {', '.join(seasonality_list)}\")\n",
        "        \n",
        "        if primary_factors:\n",
        "            decision_lines.append(\"Primary: \" + \" | \".join(primary_factors))\n",
        "        \n",
        "        # Negative indicators (noise/variability)\n",
        "        negative_indicators = []\n",
        "        \n",
        "        # Zero crossing rate\n",
        "        if not np.isnan(zero_crossing_rate) and not np.isnan(data_points) and data_points > 0:\n",
        "            zcr_normalized = zero_crossing_rate / data_points\n",
        "            if zcr_normalized > 0.3:\n",
        "                negative_indicators.append(f\"High noise (ZCR={zcr_normalized:.1%})\")\n",
        "            elif zcr_normalized > 0.1:\n",
        "                negative_indicators.append(f\"Moderate noise (ZCR={zcr_normalized:.1%})\")\n",
        "        \n",
        "        # Skewness\n",
        "        if not np.isnan(skewness):\n",
        "            if abs(skewness) > 2.0:\n",
        "                negative_indicators.append(f\"Extreme skew ({skewness:.2f})\")\n",
        "            elif abs(skewness) > 1.0:\n",
        "                negative_indicators.append(f\"Moderate skew ({skewness:.2f})\")\n",
        "        \n",
        "        # Kurtosis\n",
        "        if not np.isnan(kurtosis):\n",
        "            if kurtosis > 5.0:\n",
        "                negative_indicators.append(f\"Heavy tails (kurt={kurtosis:.2f})\")\n",
        "            elif kurtosis > 4.0:\n",
        "                negative_indicators.append(f\"Moderate tails (kurt={kurtosis:.2f})\")\n",
        "        \n",
        "        # IQR variability\n",
        "        if not np.isnan(interquartile_range) and not np.isnan(mean_val) and abs(mean_val) > 0:\n",
        "            iqr_coefficient = interquartile_range / abs(mean_val)\n",
        "            if iqr_coefficient > 1.0:\n",
        "                negative_indicators.append(f\"High variability (IQR/mean={iqr_coefficient:.2f})\")\n",
        "            elif iqr_coefficient > 0.5:\n",
        "                negative_indicators.append(f\"Moderate variability (IQR/mean={iqr_coefficient:.2f})\")\n",
        "        \n",
        "        # Data quality issues\n",
        "        if outlier_ratio > 0.2:\n",
        "            negative_indicators.append(f\"High outliers ({outlier_ratio:.1%})\")\n",
        "        elif outlier_ratio > 0.1:\n",
        "            negative_indicators.append(f\"Moderate outliers ({outlier_ratio:.1%})\")\n",
        "        \n",
        "        if changepoint_count > 5:\n",
        "            negative_indicators.append(f\"Many changepoints ({changepoint_count})\")\n",
        "        elif changepoint_count > 2:\n",
        "            negative_indicators.append(f\"Some changepoints ({changepoint_count})\")\n",
        "        \n",
        "        if negative_indicators:\n",
        "            decision_lines.append(\"Concerns: \" + \" | \".join(negative_indicators[:3]))  # Limit to 3 concerns\n",
        "        \n",
        "        # Format decision text and add below plot\n",
        "        if len(decision_lines) > 0:\n",
        "            decision_text = \"\\n\".join(decision_lines)\n",
        "            \n",
        "            # Add text below the plot\n",
        "            plt.figtext(0.5, 0.02, decision_text, ha='center', va='bottom', fontsize=9, \n",
        "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7),\n",
        "                       wrap=True)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.08, 1, 0.98])  # Make room for text at bottom\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"   Plotted forecast for {series_info['metric_name']} [{classification}]\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Plotted {len(forecasts_by_series)} series\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Forecasts to Database\n",
        "\n",
        "**Note:** This step is skipped in dry run mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save forecasts to database for each series (only if not dry run)\n",
        "if not DRY_RUN:\n",
        "    # Defensive: ensure database connection and run_id exist (created before the processing loop)\n",
        "    if \"conn\" not in globals() or conn is None:\n",
        "        raise RuntimeError(\n",
        "            \"Database connection not established (conn is missing/None). \"\n",
        "            \"Check VM_JOBS_ENVIRONMENT, VM_JOBS_CONFIG_PATH, and VM_JOBS_DB_PASSWORD.\"\n",
        "        )\n",
        "    if \"run_id\" not in globals() or run_id is None:\n",
        "        raise RuntimeError(\"Forecast run record not created (run_id is missing/None).\")\n",
        "\n",
        "    def _unpack_forecast_item(item):\n",
        "        \"\"\"Support both tuple shapes:\n",
        "        - non-dry-run: (series_info, forecast_df, classification, prophet_params, features, reason)\n",
        "        - dry-run: (series_info, forecast_df, df_training, classification, prophet_params, features, detector_info, reason)\n",
        "        Returns: series_info, forecast_df, classification, prophet_params, features, reason\n",
        "        \"\"\"\n",
        "        if not isinstance(item, (list, tuple)):\n",
        "            raise ValueError(f\"Unexpected forecast item type: {type(item)}\")\n",
        "        if len(item) == 6:\n",
        "            series_info, forecast_df, classification, prophet_params, features, reason = item\n",
        "            return series_info, forecast_df, classification, prophet_params, features, reason\n",
        "        if len(item) >= 8:\n",
        "            series_info, forecast_df, _df_training, classification, prophet_params, features, _detector_info, reason = item\n",
        "            return series_info, forecast_df, classification, prophet_params, features, reason\n",
        "        raise ValueError(f\"Unexpected forecast item tuple length: {len(item)}\")\n",
        "\n",
        "    # Forecast types - only trend (no intervals)\n",
        "    forecast_types = [{\"name\": \"trend\", \"field\": \"yhat\"}]\n",
        "\n",
        "    print(f\"\\nSaving forecasts for {len(forecasts_by_series)} series...\")\n",
        "\n",
        "    total_rows_inserted = 0\n",
        "    for forecast_item in forecasts_by_series:\n",
        "        series_info, forecast_df, classification, prophet_params, features, reason = _unpack_forecast_item(\n",
        "            forecast_item\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            rows_inserted, job_idx_used, metric_id_used = save_forecasts_to_database(\n",
        "                conn=conn,\n",
        "                metric_name=series_info['metric_name'],\n",
        "                labels=series_info['labels'],\n",
        "                forecast_df=forecast_df,\n",
        "                forecast_types=forecast_types,\n",
        "                run_id=run_id,\n",
        "            )\n",
        "            total_rows_inserted += rows_inserted\n",
        "            # Save forecast metadata using the same (job_idx, metric_id) we just used for forecast data\n",
        "            if job_idx_used is not None and metric_id_used is not None:\n",
        "                try:\n",
        "                    save_forecast_metadata_for_metric_by_id(\n",
        "                        conn=conn,\n",
        "                        run_id=run_id,\n",
        "                        job_idx=job_idx_used,\n",
        "                        metric_id=metric_id_used,\n",
        "                        tsfel_features=features,\n",
        "                        classification={\"category\": classification, \"reason\": reason},\n",
        "                        prophet_params=prophet_params,\n",
        "                    )\n",
        "                except Exception as meta_exc:\n",
        "                    print(f\"    Forecast metadata: {meta_exc}\")\n",
        "            print(f\"   {series_info['metric_name']} [{classification}]: {rows_inserted} rows saved\")\n",
        "        except Exception as exc:\n",
        "            print(f\"   {series_info['metric_name']}: Failed to save - {exc}\")\n",
        "\n",
        "    print(f\"\\nTotal: {total_rows_inserted} forecast rows saved to database\")\n",
        "else:\n",
        "    print(\"Dry run mode: Skipping database save operations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close database connection (only if not dry run)\n",
        "if not DRY_RUN:\n",
        "    conn.close()\n",
        "    engine.dispose()\n",
        "    print(\"Database connection closed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write execution results for the job (timeseries_processed, timeseries_failed)\n",
        "# When run via papermill, output_results_path is set and the job reads this file.\n",
        "# IMPORTANT: This must never fail the notebook (DB saves may have already happened).\n",
        "try:\n",
        "    import json\n",
        "\n",
        "    _query_result = globals().get(\"query_result\") or []\n",
        "    _forecasts_by_series = globals().get(\"forecasts_by_series\") or []\n",
        "\n",
        "    timeseries_processed = len(_forecasts_by_series)\n",
        "    timeseries_failed = max(len(_query_result) - len(_forecasts_by_series), 0)\n",
        "\n",
        "    if output_results_path:\n",
        "        with open(output_results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"timeseries_processed\": timeseries_processed,\n",
        "                    \"timeseries_failed\": timeseries_failed,\n",
        "                },\n",
        "                f,\n",
        "                indent=2,\n",
        "            )\n",
        "        print(\n",
        "            f\"Wrote results to {output_results_path}: \"\n",
        "            f\"{timeseries_processed} processed, {timeseries_failed} failed/skipped\"\n",
        "        )\n",
        "except Exception as _results_exc:\n",
        "    print(f\"Warning: failed to write results JSON: {_results_exc}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
